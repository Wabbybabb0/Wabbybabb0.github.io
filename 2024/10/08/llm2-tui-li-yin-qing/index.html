<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM2æ¡†æ¶æ­å»ºè¿‡ç¨‹, Wabbybabboçš„æ‘¸é±¼åœ£åœ°">
    <meta name="description" content="ä¸Šç­ï¼Ÿä¸‹ç­ï¼">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM2æ¡†æ¶æ­å»ºè¿‡ç¨‹ | Wabbybabboçš„æ‘¸é±¼åœ£åœ°</title>
    <link rel="icon" type="image/png" href="/grape200x200.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Wabbybabboçš„æ‘¸é±¼åœ£åœ°" type="application/atom+xml">
</head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/grape200x200.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Wabbybabboçš„æ‘¸é±¼åœ£åœ°</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-sharp fa-solid fa-fish" style="zoom: 0.6;"></i>
      
      <span>ä¸»é¡µ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-soild fa-tags" style="zoom: 0.6;"></i>
      
      <span>æ ‡ç­¾</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>åˆ†ç±»</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>æ¡£æ¡ˆ</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>å…³äº</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="" class="waves-effect waves-light">

      
      <i class="fas fa-list" style="zoom: 0.6;"></i>
      
      <span>åˆ†äº«</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="/musics">
          
          <i class="fas fa-music" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>éŸ³ä¹</span>
        </a>
      </li>
      
      <li>
        <a href="/movies">
          
          <i class="fas fa-film" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>ç”µå½±</span>
        </a>
      </li>
      
      <li>
        <a href="/books">
          
          <i class="fas fa-book" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>ä¹¦ç±</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="æœç´¢" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/grape200x200.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Wabbybabboçš„æ‘¸é±¼åœ£åœ°</div>
        <div class="logo-desc">
            
            ä¸Šç­ï¼Ÿä¸‹ç­ï¼
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-sharp fa-solid fa-fish"></i>
			
			ä¸»é¡µ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-soild fa-tags"></i>
			
			æ ‡ç­¾
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			åˆ†ç±»
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			æ¡£æ¡ˆ
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			å…³äº
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-list"></i>
			
			åˆ†äº«
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a href="/musics " style="margin-left:75px">
				  
				   <i class="fa fas fa-music" style="position: absolute;left:50px" ></i>
			      
		          <span>éŸ³ä¹</span>
                  </a>
                </li>
              
                <li>

                  <a href="/movies " style="margin-left:75px">
				  
				   <i class="fa fas fa-film" style="position: absolute;left:50px" ></i>
			      
		          <span>ç”µå½±</span>
                  </a>
                </li>
              
                <li>

                  <a href="/books " style="margin-left:75px">
				  
				   <i class="fa fas fa-book" style="position: absolute;left:50px" ></i>
			      
		          <span>ä¹¦ç±</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Wabbybabb0/Wabbybabb0.github.io" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Wabbybabb0/Wabbybabb0.github.io" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/11.png')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM2æ¡†æ¶æ­å»ºè¿‡ç¨‹</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- æ–‡ç« å†…å®¹è¯¦æƒ… -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                            <a href="/tags/CUDA/">
                                <span class="chip bg-color">CUDA</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/" class="post-category">
                                é«˜æ€§èƒ½è®¡ç®—
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>å‘å¸ƒæ—¥æœŸ:&nbsp;&nbsp;
                    2024-10-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>æ›´æ–°æ—¥æœŸ:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>æ–‡ç« å­—æ•°:&nbsp;&nbsp;
                    17.3k
                </div>
                

                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>é˜…è¯»æ¬¡æ•°:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- æ˜¯å¦åŠ è½½ä½¿ç”¨è‡ªå¸¦çš„ prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="Lesson1-æ•´ä½“æ¶æ„"><a href="#Lesson1-æ•´ä½“æ¶æ„" class="headerlink" title="Lesson1 æ•´ä½“æ¶æ„"></a>Lesson1 æ•´ä½“æ¶æ„</h1><p>Llama2ï¼šç”Ÿæˆå¼æ¨¡å‹ä»¥decoder-onlyä¸ºæ¶æ„<br>ç”±ä¸¤ä¸ªDecoderç»„æˆï¼š<br>â‘ Context Decoderï¼Œä½äºprompté˜¶æ®µï¼Œç”¨æ¥ç”Ÿæˆä¸€ä¸ªtokenï¼›å…¨é‡æ¨ç†ï¼šè¾“å…¥æ˜¯ä¸€ä¸ªå¥å­ï¼Œåªéœ€è¦ç”Ÿæˆç¬¬ä¸€ä¸ªtokenï¼›å…·æœ‰å¹¶è¡Œè®¡ç®—çš„ç‰¹ç‚¹</p>
<p>â‘¡Mask self Decoderï¼Œä½äºgenerateé˜¶æ®µï¼Œç”¨æ¥ç”Ÿæˆç¬¬äºŒä¸ªtokenï¼›å¢é‡æ¨ç†ï¼šè¾“å…¥æ˜¯ä¸€ä¸ªtokenï¼Œåœ¨gptä¸Šçš„è¡¨ç°ä¸ºæ¯æ¬¡åå‡ºä¸ºä¸€ä¸ªtokenï¼›æ¯æ¬¡è¾“å…¥çš„éƒ½æ˜¯ä¸Šä¸€ä¸ªè¾“å‡ºçš„token<br><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/c7b40d8526dd">Transformerç³»åˆ—ï¼šæ³¨æ„åŠ›æœºåˆ¶çš„ä¼˜åŒ–ï¼ŒMQAå’ŒGQAåŸç†ç®€è¿° - ç®€ä¹¦</a></p>
<h1 id="Lesson2-é¡¹ç›®æ­å»º-amp-embedding-kernel"><a href="#Lesson2-é¡¹ç›®æ­å»º-amp-embedding-kernel" class="headerlink" title="Lesson2 é¡¹ç›®æ­å»º&amp;embedding kernel"></a>Lesson2 é¡¹ç›®æ­å»º&amp;embedding kernel</h1><p>è®²è§£äº†ï¼š<br><code>src/utils/tensor.h</code><br><code>src/kernels/input_embedding.cu</code><br><code>src/kernels/input_embedding.h</code><br><code>tests/unittests/test_input_embedding.cu</code></p>
<pre class="line-numbers language-none"><code class="language-none">-src
|-kernels
|-|-input_embeding.cu
|-utils
|-|-tensor.h
|-weights<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>LLMengine/src/utils/tensor.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">Struct Tensor{
	Device location,
	DataType dtype,
	std::vector&lt;int&gt; shape;
	...
	virtual int size() const {
&nbsp; &nbsp; &nbsp; &nbsp; if (shape.size() == 0) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // TODO: add an reminder info
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return 0;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; return std::accumulate(shape.begin(), shape.end(), (int)1, std::multiplies&lt;int&gt;());
&nbsp; &nbsp; }
&nbsp; &nbsp; ...
&nbsp; &nbsp; template&lt;typename T&gt;
&nbsp; &nbsp; TensorWrapper&lt;T&gt;* as(){
&nbsp; &nbsp; &nbsp; &nbsp; return static_cast&lt;TensorWrapper&lt;T&gt;*&gt;(this); // ä¸‹è¡Œè½¬æ¢(æ˜¾å¼)ï¼Œå°†this(Tensorç±»å‹çš„å½“å‰å¯¹è±¡)è½¬æ¢ä¸ºTensorWrapper&lt;T&gt;ç±»å‹çš„æŒ‡é’ˆ
&nbsp; &nbsp; }
}

Class TensorWrap: public Tensor {
	T * data;
	...
}

Struct TensorMap{
	std::unordered_map(std::string, Tensor*&gt; tensor_map);
	...
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li><code>std::unorder_map</code>ï¼šæ˜¯ä¸€ä¸ªå…³è”å®¹å™¨ï¼Œç”¨äºå­˜å‚¨é”®å€¼å¯¹ï¼Œé”®æ˜¯è¯¥Tensorçš„åå­—ï¼Œå€¼æ˜¯æŒ‡å‘Tensorç±»å‹å˜é‡çš„æŒ‡é’ˆ</li>
<li>å…³äºä¸ºä»€ä¹ˆè¦åœ¨<code>TensorWrap</code>ä¸­å…ˆç»§æ‰¿çˆ¶ç±»<code>Tensor</code>å†å®ç°æ¨¡æ¿åŒ–<code>T* data</code>ï¼š<code>Tensor</code>è¦æ”¾åˆ°<code>TensorMap</code>ä¸­ï¼Œè€ŒC++ä½œä¸ºå¼ºç±»å‹è¯­è¨€ï¼Œä¸æ”¯æŒå­—å…¸å­˜æ”¾ä¸åŒç±»å‹çš„tensor(å› ä¸ºç±»å‹å®šä¹‰ä¸º<code>Tensor</code>çš„æŒ‡é’ˆï¼Œå¦‚æœåœ¨<code>Tensor</code>ä¸­åŠ å…¥äº†<code>T*</code>ä½œä¸ºæˆå‘˜ï¼Œå¯èƒ½ä¼šä¹±å¥—äº†)</li>
<li><code>std::accumulate(shape.begin(), shape.end(), (int)1, std::multiplies&lt;int&gt;());</code>ï¼šåšä¹˜ç§¯ï¼Œåˆå§‹ä¹˜çš„å€¼ä¸º1</li>
</ul>
<p>å¦‚æœ<code>.cpp</code>æ–‡ä»¶è°ƒç”¨å¸¦æœ‰cudaè¯­æ³•çš„å‡½æ•°ï¼Œåˆ™å…¶å®šä¹‰ä¸èƒ½å­˜åœ¨<code>.h</code>æ–‡ä»¶é‡Œï¼Œä¾‹å¦‚å«æœ‰<code>&lt;&lt;&lt; &gt;&gt;&gt;</code><br>    ä¾‹å­ï¼šåœ¨<code>src/kernel/input_embedding.cu</code>ä¸­</p>
<ul>
<li>å®šä¹‰äº†<code>launchInputEmbedding</code>ğŸ‘‡<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;
void launchInputEmbedding(TensorWrapper&lt;int&gt;* input_ids, &nbsp; &nbsp;// INT [token num]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; TensorWrapper&lt;T&gt;* output, &nbsp; &nbsp; &nbsp; // FP32 [token num, hidden_size] = [token num, 4096]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; EmbeddingWeight&lt;T&gt;* embed_table// FP32 [vocal_size, hidden_size]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ) {
&nbsp; &nbsp; // åˆ†é…çº¿ç¨‹å—ï¼Œæ ¸å‡½æ•°éœ€è¦çš„ç»´åº¦ä¿¡æ¯
&nbsp; &nbsp; const int blockSize = 256;
&nbsp; &nbsp; const int max_context_token_num = output-&gt;shape[0]; // token num
&nbsp; &nbsp; const int hidden_size = output-&gt;shape[1];
&nbsp; &nbsp; const int gridSize = 2048;
&nbsp; &nbsp; LLM_CHECK_WITH_INFO(max_context_token_num == input_ids-&gt;shape[0], "input ids 1st shape should equal to 1st shape of output");
&nbsp; &nbsp; embeddingFunctor&lt;T&gt;&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(input_ids-&gt;data,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;output-&gt;data,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;embed_table-&gt;data,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;max_context_token_num,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;hidden_size);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li>å®ä¾‹åŒ–<ul>
<li>æ˜¾å¼å®ä¾‹åŒ–æ˜¯å‘Šè¯‰ç¼–è¯‘å™¨ç”Ÿæˆä¸€ä¸ªæ¨¡æ¿å‡½æ•°çš„ç‰¹å®šå®ä¾‹ã€‚åœ¨æ¨¡æ¿å‡½æ•°å®šä¹‰ä¸­ï¼Œåªæ˜¯å®šä¹‰äº†ä¸€ä¸ªé€šç”¨çš„é€»è¾‘ï¼Œä½†æ²¡æœ‰çœŸæ­£ç”Ÿæˆä»£ç ã€‚<strong>åªæœ‰åœ¨æ¨¡æ¿å®ä¾‹åŒ–çš„æ—¶å€™ï¼Œç¼–è¯‘å™¨æ‰ä¼šæ ¹æ®å…·ä½“çš„æ•°æ®ç±»å‹æ¥ç”Ÿæˆç›¸åº”çš„å‡½æ•°ä»£ç ã€‚</strong></li>
<li>åŸå› ï¼š<ul>
<li>é¿å…ä»£ç è†¨èƒ€ï¼šå¦‚æœä¸æ˜¾å¼å®ä¾‹åŒ–ï¼Œé‚£ä¹ˆæ¯æ¬¡ä½¿ç”¨ä¸åŒç±»å‹è°ƒç”¨æ¨¡æ¿å‡½æ•°æ—¶ï¼Œç¼–è¯‘å™¨éƒ½ä¼šç”Ÿæˆæ–°çš„ä»£ç </li>
<li>CUDAç¼–è¯‘é™åˆ¶</li>
</ul>
</li>
<li>åˆ†åˆ«ç”Ÿæˆäº†ğŸ‘‡ä¸¤ç§ç±»å‹çš„å…·ä½“å®ä¾‹<code>T=float</code>ã€<code>T=half</code><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">// æ˜¾å¼å®ä¾‹åŒ–æ¨¡ç‰ˆå‡½æ•°ï¼Œç”±äºcudaçš„è¯­æ³•è§„åˆ™ï¼Œä¸èƒ½å­˜åœ¨.cppæ–‡ä»¶é‡Œï¼Œå› æ­¤åªèƒ½åœ¨æ­¤å®ä¾‹åŒ–
template void launchInputEmbedding(TensorWrapper&lt;int&gt;* input_ids, &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;TensorWrapper&lt;float&gt;* output, &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;EmbeddingWeight&lt;float&gt;* embed_table);
template void launchInputEmbedding(TensorWrapper&lt;int&gt;* input_ids, &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;TensorWrapper&lt;half&gt;* output, &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;EmbeddingWeight&lt;half&gt;* embed_table);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ul>
</li>
</ul>
<p><code>src/kernels/input_embedding.cu</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">__global__ void embeddingFunctor(const int* input_ids,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;T* output,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;const T* embed_table,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;const int max_context_token_num,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;const int hidden_size)
{
&nbsp; &nbsp; int index = blockIdx.x * blockDim.x + threadIdx.x;
&nbsp; &nbsp; while (index &lt; max_context_token_num * hidden_size) {
&nbsp; &nbsp; &nbsp; &nbsp; int id = input_ids[index / hidden_size];
&nbsp; &nbsp; &nbsp; &nbsp; output[index] = embed_table[id * hidden_size + index % hidden_size];
&nbsp; &nbsp; &nbsp; &nbsp; index += blockDim.x * gridDim.x;
&nbsp; &nbsp; }
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/6382f7e2cd4bf0b302b43f58c93ea08.jpg" alt="">indexçš„ç´¢å¼•å¯¹åº”çš„æ˜¯outputçš„è¾“å‡ºçš„æ¯ä¸ªä½ç½®ï¼Ÿ<p></p>
<p>è¿™ä¸ªkernelçš„ç”¨å¤„ï¼šå°†åŸæœ¬è¾“å…¥æ ¼å¼çš„[batch size, sequence length]å˜æˆ[batch size, sequence length, hidden size]</p>
<p>æœ‰<code>.cpp</code>æ–‡ä»¶ã€<code>.cc</code>æ–‡ä»¶ã€<code>.cu</code>æ–‡ä»¶çš„ç›®å½•ä¸‹éœ€è¦æ”¾<code>CMakeLists.txt</code>æ–‡ä»¶</p>
<h1 id="Lesson3-Calculate-padding-offset-kernel"><a href="#Lesson3-Calculate-padding-offset-kernel" class="headerlink" title="Lesson3 Calculate padding offset kernel"></a>Lesson3 Calculate padding offset kernel</h1><p>è®²è§£äº†ï¼š<br><code>src/kernels/cal_paddingoffset.cu</code><br><code>src/kernels/cal_paddingoffset.h</code><br><code>tests/unittests/test_cal_paddingoffset.cu</code></p>
<p><a target="_blank" rel="noopener" href="https://github.com/bytedance/effective_transformer">Padding Offsetæ€æƒ³æ¥æº</a><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/Pasted%20image%2020241011153928.png" alt="|425"></p>
<p><code>src/lkernels/cal_paddingoffset.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void launchCalPaddingoffset(TensorWrapper&lt;int&gt;* padding_offset,
							TensorWrapper&lt;int&gt;* cum_seqlens,
							TensorWrapper&lt;int&gt;* input_lengths
);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li>å‚æ•°ï¼š<br>  <code>padding_offset</code>ï¼š<code>[batch size, max q_seq length]</code>è®°å½•æ¯ä¸ªtokenåœ¨å…¶ä¹‹å‰çš„paddingä¸ªæ•°<br>  <code>cum_seqlens</code>ï¼š<code>[batch size + 1]</code>ç¬¬ä¸€ä¸ªå¥å­ç´¯ç§¯é•¿åº¦æ˜¯å®ƒæœ¬èº«ï¼Œç¬¬äºŒä¸ªå¥å­ç´¯ç§¯é•¿åº¦æ˜¯ç¬¬ä¸€å¥+ç¬¬äºŒå¥é•¿åº¦<br>  <code>input_lengths</code>ï¼š<code>[batch size]</code>æ¯ä¸ªå¥å­çš„è¾“å…¥é•¿åº¦ï¼Œæœ¬èº«çš„<br>  <code>launchCalPaddingoffset</code>å‡½æ•°çš„ç›®çš„æ˜¯è¾“å‡ºpaddingä¸ªæ•°å’Œç´¯ç§¯é•¿åº¦</li>
<li>ä¾‹å­ï¼š<br>  11100<br>  11000<br>  11111<br>  batch size = 3<br>  seqlen = [3, 2, 5]<br>  max_q_len = 5<br>  padding_offset = [0, 0, 0, 0, 0<pre><code>              2, 2, 2, 2, 2
              5, 5, 5, 5, 5]
</code></pre>  cum_seqlens = [0, 3, 5, 10]</li>
</ul>
<p>ç›¸æ¯”äº<code>Lesson2</code>ä¸­çš„æ¨¡æ¿åŒ–ï¼Œè¿™é‡Œä¸éœ€è¦æ¨¡æ¿åŒ–çš„åŸå› æ˜¯ï¼Œè¯¥å‡½æ•°çš„å‚æ•°éƒ½æ˜¯<code>int</code>ç±»å‹ï¼Œè€Œ<code>Lesson2</code>ä¸­çš„æ˜¯<code>T</code>ç±»å‹ï¼Œéœ€è¦å¯¹å…¶åš<code>FP16</code>å’Œ<code>FP32</code>çš„æ¨¡æ¿åŒ–</p>
<p><code>src/kernels/cal_paddingoffset.cu</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">__global__ void CalPaddingoffset(int* &nbsp; &nbsp; &nbsp; &nbsp; padding_offset,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;int* &nbsp; &nbsp; &nbsp; &nbsp; cum_seqlens,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;const int* &nbsp; input_lengths, //actual input lens
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;const int &nbsp; &nbsp;batch_size,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;const int &nbsp; &nbsp;max_q_len) {
&nbsp; &nbsp; // è‡ªå·±æ‰“çš„24-10-11
&nbsp; &nbsp; int cum_offset = 0;
&nbsp; &nbsp; int ind = 0;
&nbsp; &nbsp; int total_seqlen = 0;
&nbsp; &nbsp; for(int b = 0; b &lt; batch_size; b++) { // bå¯¹åº”æ¯ä¸ªbatchä¸­çš„ç¬¬b+1ä¸ªseq
&nbsp; &nbsp; &nbsp; &nbsp; int seqlen = input_lengths[b]; &nbsp; &nbsp;// è·å–æ¯ä¸ªå¥å­é•¿åº¦
&nbsp; &nbsp; &nbsp; &nbsp; cum_seqlens[b] = total_seqlen; &nbsp; &nbsp;// (1)å°†ç´¯ç§¯çš„seqlenå­˜å…¥åˆ°æ¯ä¸ªå¥å­ä¸­ï¼Œcum_seqlens[0] = 0, ..., cum_seqlens[0] = æœ€åä¸€ä¸ªå¥å­çš„å¥å­ç´¯ç§¯é•¿åº¦
&nbsp; &nbsp; &nbsp; &nbsp; for( int i =0; i &lt; seqlen; i++) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; padding_offset[ind] = cum_offset; // (2)å°†ç´¯ç§¯çš„offsetå­˜å…¥åˆ°æ¯ä¸ªtokenä¸­ï¼Œpadding_offsetçš„ä¸‹æ ‡åº”è¯¥æ˜¯ä¸€ä¸ªç´¯ç§¯çš„å€¼ï¼Œæ‰€ä»¥åº”è¯¥åœ¨forçš„å¤–éƒ¨å®šä¹‰indç„¶åå–å…¶ä¸ºä¸‹æ ‡
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ind++;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; cum_offset += max_q_len - seqlen; &nbsp; &nbsp; // è·å–æ¯ä¸ªå¥å­ç´¯ç§¯çš„offset
&nbsp; &nbsp; &nbsp; &nbsp; total_seqlen += seqlen; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // è·å–æ¯ä¸ªå¥å­ç´¯ç§¯çš„å¥å­é•¿åº¦
&nbsp; &nbsp; }
&nbsp; &nbsp; cum_seqlens[batch_size] = total_seqlen;
&nbsp; &nbsp; }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<p><code>kernel</code>å†™å®Œä¹‹åè¿˜éœ€è¦å†™<code>CMake</code>æ–‡ä»¶<br><code>test/unittest/CMakelist.txt</code>ï¼šå°†testç¼–è¯‘ä¸ºå¯æ‰§è¡Œæ–‡ä»¶<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">add_executable(cal_paddingoffset // â€»
	test_input_embedding.cu
)
target_link_libraries(
	cal_paddingoffset PUBLIC    //è¿™è¦å’Œâ€»å¤„çš„åç§°å¯¹åº”
	-lcudart
	-lcudadevrt
	paddingoffset               // è¿™é‡Œå¯ä»¥è‡ªå·±èµ·
)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br><code>src/kernels/CMakelist.txt</code>(æ³¨æ„å’Œä¸Šé¢çš„åç§°çš„å¯¹åº”)<br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">add_library(paddingoffset STATIC cal_paddingoffset.cu)
set_property(TARGET paddingoffset PROPERTY CUDA_SEPARABLE_COMPILATION   ON)
set_property(TARGET paddingoffset PROPERTY POSITION_INDEPENDENT_CODE ON)
set_property(TARGET paddingoffset PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLE ON)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<h1 id="Lesson4-RMS-norm"><a href="#Lesson4-RMS-norm" class="headerlink" title="Lesson4 RMS norm"></a>Lesson4 RMS norm</h1><p>è®²è§£äº†ï¼š<br><code>src/kernels/rmsnorm_kernel.cu</code><br><code>src/kernels/rmsnorm_kernel.h</code><br><code>tests/unittests/test_rmsnorm.cu</code><br><code>src/utils/vectorize_utils.h</code><br><code>src/weights/llama/norm_weights.h</code></p>
<p><code>src/utils/vectorize_utils.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;
struct Vec{
&nbsp; &nbsp; using Type = T;
&nbsp; &nbsp; static constexpr int size = 0;
};
// é™¤æ­¤ä¹‹å¤–è¿˜å®šä¹‰äº†float4(size=4)ï¼Œhalf2(size=2)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br><code>static</code>ï¼šè¡¨ç¤ºè¯¥æˆå‘˜<code>size</code>å±äºç±»è€Œä¸æ˜¯æŸä¸ªå®ä¾‹(å¯¹è±¡)<br><code>constexpr</code>ï¼šå®šä¹‰ä¸€ä¸ªé™æ€çš„ç±»æˆå‘˜ï¼Œå¹¶ä¸”è¯¥æˆå‘˜æ˜¯ä¸€ä¸ªç¼–è¯‘æ—¶å¸¸é‡ï¼Œåœ¨ç¼–è¯‘æ—¶å°±ç¡®å®š<br><code>float4</code>å’Œ<code>half2</code>åˆ†åˆ«æ˜¯åŒ…å«4ä¸ª<code>float</code>åˆ†é‡çš„å‘é‡å’ŒåŒ…å«2ä¸ª<code>half</code>åˆ†é‡çš„å‘é‡<br>ä½œç”¨æ˜¯å­˜å‚¨é€šç”¨çš„å‘é‡åŒ–æ•°æ®ç»“æ„<p></p>
<p><code>src/weights/llama/norm_weights.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
struct LayerNormWeight {  
    T* gamma; 
};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<h2 id="4-1src-kernel-rmsnorm-kernel-cu"><a href="#4-1src-kernel-rmsnorm-kernel-cu" class="headerlink" title="4.1src/kernel/rmsnorm_kernel.cu"></a>4.1<code>src/kernel/rmsnorm_kernel.cu</code></h2><p>(1)<code>warpReduceSum</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
__device__ T warpReduceSum(T val){  
    for(int i = 32 / 2; i &gt; 0; i &gt;&gt;= 1){  
        val += __shfl_xor_sync(0xffffffff, val, i);  
    }    
    return val; // æœ€åè¿™ä¸ªwarpçš„ç»“æœä¿å­˜åœ¨ç¬¬ä¸€ä¸ªç¬¬ä¸€ä¸ªçº¿ç¨‹(threadIdx.x=0)
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>å°†ä¸€ä¸ªwarpä¸­çš„æ•°æ®åŠ èµ·æ¥<p></p>
<p>(2)<code>blockReduceSum</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
__device__ T blockReduceSum(T val){  
    int tid = threadIdx.x;  
    it wid = tid / 32;  
    int laneid = tid % 32;  
    int warpnum = (blockDim.x + 32 - 1) / 32;  
    val = warpReduceSum&lt;T&gt;(val);     // valæ˜¯æ¯ä¸ªwarpçš„æ€»å’Œçš„å€¼
    static __shared__ T warpsum[64]; // ä¸èƒ½å†™warpnumï¼Œå› ä¸ºç”³è¯·çš„æ˜¯é™æ€çš„ï¼Œéœ€è¦ä¼ å…¥ç¼–è¯‘æœŸå¸¸é‡64
    if(landid == 0) // å¦‚æœæ˜¯wrapçš„ç¬¬ä¸€ä¸ªçº¿ç¨‹(å­˜æœ‰è¯¥wrapçš„ç»“æœ)
    { 
	    warpsum[wid] = val; // å°†æ¯ä¸ªwarpçš„æ±‚å’Œæ”¾å…¥warpsumä¸­
    }    
    __syncthreads(); // å¤„ç†å®Œå…±äº«å†…å­˜çš„è¯»å†™åè¦åŠ ä¸Š`__syncthreads();!!!
    T sum = tid &lt; warpnum ? warpsum[wid] : (T)0; 
    // å¤„ç†å‰warpnumä¸ªwarpsum[wid]ï¼Œå¹¶ä¸”ç¡®ä¿ä½¿ç”¨çº¿ç¨‹idä¸º0~warpnum-1æ¥å¤„ç†
    sum = warpReduceSum&lt;T&gt;(sum);  
    return sum;
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>å°†ä¸€ä¸ªblockçš„æ•°æ®åŠ èµ·æ¥<br>å‚æ•°ï¼š<br>    <code>tid</code>ï¼šå…¨å±€çš„thread idx (0~?)<br>    <code>wid</code>ï¼šwrap idxï¼Œæ¯32ä¸ªthreadsä¸ºä¸€ä¸ªwrap (0~?)<br>    <code>laneid</code>ï¼šwrapä¸­çš„threadçš„ç¼–å·(0~31)<br>    <code>warpnum</code>ï¼šç”¨åˆ°çš„warpçš„ä¸ªæ•°ï¼Œæœ€å°ä¸º1ï¼Œæ‰€ä»¥è¿™é‡Œéœ€è¦å‘ä¸Šå–æ•´<br>    <code>warpsum</code>ï¼šå¤§å°ä¸º64çš„ç±»å‹ä¸ºTçš„æ•°ç»„ï¼Œå­˜æ”¾æ¯ä¸ªwarpçš„æ€»å’Œ<p></p>
<p>(3)<code>RMSNorm</code><br>è®¡ç®—å…¬å¼ï¼š$\dfrac{x_iÃ—g_i}{\sqrt{\sum^iE(x_i^2)+eps}}$<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template &lt;typename T&gt;  
__global__ void RMSNorm(T* decoder_in,
                        T* decoder_residual,  
                        T* scale, //[q_hidden_units], RMSNorm weights  
                        float eps, //RMSNorm eps  
                        int num_tokens,  
                        int hidden_units) {  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>å‚æ•°ï¼š<br>    <code>decoder_in</code>ï¼šæ˜¯è¾“å…¥åŒæ—¶ä¹Ÿæ˜¯è¾“å‡ºä½ç½®ï¼Œ<code>[num tokens, q_hidden_units]</code><br>    <code>decoder_residual</code>ï¼šæš‚æ—¶ä¸çŸ¥é“è¿™ä¸ªçš„ç”¨å¤„<br>    <code>scale</code>ï¼šå¯å­¦ä¹ çš„å‚æ•°(æƒé‡)ï¼Œ<code>[q_hidden_units]</code><br>    <code>eps</code>ï¼šå¾ˆå°çš„æ­£æ•°<br>    <code>num_tokens</code>ï¼štokençš„ä¸ªæ•°<br>    <code>hidden_units</code>ï¼šéšè—å±‚çš„å•å…ƒçš„æ•°é‡<br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int vec_size = Vec&lt;T&gt;::size;
using Vec_t = typename Vec&lt;T&gt;::Type;  
Vec_t *dout = reinterpret_cast&lt;Vec_t*&gt;(decoder_in + blockIdx.x * hidden_units); // æ¯ä¸ªçº¿ç¨‹éœ€è¦è¯»çš„æ•°æ®çš„åç§»; blockçš„æ•°é‡æ˜¯tokençš„æ•°é‡  
Vec_t *rsd = reinterpret_cast&lt;Vec_t*&gt;(decoder_residual * blockIdx.x * hidden_units);  
float thread_sum = 0.0f;  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>å‚æ•°ï¼š<br>    <code>vec_size</code>ï¼šè¯»å–vectorçš„å¤§å°ï¼Œæ¯”å¦‚<code>float4</code>çš„å‘é‡ä¸ªæ•°ä¸º4ï¼Œ<code>half2</code>çš„å‘é‡ä¸ªæ•°ä¸º2<br>    <code>Vec_t</code>ï¼šè¯»å–ç±»å‹å¹¶å­˜åˆ°<code>Vec_t</code>ä¸­<br>        ï¼å‰ä¸€å¥æ²¡æœ‰ç”¨<code>typename</code>è€Œåä¸€å¥ç”¨äº†çš„åŸå› æ˜¯ï¼š<br>        â‘ å‰è€…å±äºéä¾èµ–å‹ï¼Œ<code>size</code>çš„å€¼åœ¨ç¼–è¯‘æ—¶å¯ä»¥ç¡®å®šï¼Œä¸<code>T</code>dçš„å…·ä½“ç±»å‹æ— å…³ï¼›<br>        â‘¡åè€…æ—¶ä¾èµ–ç±»å‹ï¼Œ<code>Type</code>æ˜¯ä¸€ä¸ªç±»å‹åˆ«åï¼Œå–å†³äº<code>T</code>ï¼Œå› æ­¤éœ€è¦<code>typename</code>å…³é”®å­—æ¥å‘Šè¯‰ç¼–è¯‘å™¨ä»–æ˜¯ä¸€ä¸ªç±»å‹<br>    <code>dout</code>ï¼šæ ¹æ®çº¿ç¨‹æŒ‡å‘æ¯ä¸€ä¸ªä»¥è¾“å…¥å‘é‡ä¸ºèµ·å§‹çš„<code>block</code>çš„å¼€å¤´ï¼Œæ¯ä¸€ä¸ª<code>block</code>å¯¹åº”ä¸€ä¸ª<code>token</code>ï¼Œæ¯ä¸ª<code>block</code>ä¹‹é—´ç›¸å·®å¤§å°ä¸º<code>hidden units</code>çš„é—´éš”<br>    <code>rsd</code>ï¼šåŒ<code>dout</code><br>    <code>thread_sum</code>ï¼šç”¨äºæ±‚å’Œ<br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">for (int idx = threadIdx.x; idx &lt; hidden_units / vec_size; idx += blockDim.x) {  
        Vec_t vec = dout[idx];
        rsd[idx] = vec;  
        thread_sum += vec.x * vec.x;  
        thread_sum += vec.y * vec.y;  
        thread_sum += vec.z * vec.z;  
        thread_sum += vec.w * vec.w;  
    }    
thread_sum = blockReduceSum&lt;float&gt;(thread_sum);  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>ç”¨äºæ±‚$\sum^iE(x_i)$ï¼Œæ¯ä¸ªblockå¾—åˆ°ä¸€ä¸ªæ€»å’Œ<br>å‚æ•°ï¼š<br>    <code>vec</code>ï¼šå°†<code>dout[idx]</code>çš„æ•°æ®å­˜åˆ°vecä¸­<br>    <img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/492e42f3b277fbb6c44c818fa908be8.jpg" alt="|225"><br>    <code>thread_sum</code>ï¼šæ¯ä¸ªçº¿ç¨‹éƒ½æœ‰ä¸€ä¸ªç§æœ‰çš„å‰¯æœ¬<br>æ³¨æ„ï¼š<br>    <code>idx</code>çš„èŒƒå›´æ˜¯ä»<code>threadIdx.x</code>å¼€å§‹çš„ï¼ŒèŒƒå›´æ˜¯0~<code>blockDim.x-1</code><br>    å› æ­¤æ¯ä¸ªforå¾ªç¯å®é™…åªå¤„ç†äº†ä¸€ä¸ªblockçš„æ±‚å’Œï¼Œ<code>idx+=blockDimx.x</code>ä½¿å¾—å¯ä»¥å¯¹ä¸‹ä¸€ä¸ªblockè¿›è¡Œæ±‚å’Œ<br>    æ‰€ä»¥è¯´è¿™é‡Œçš„æ±‚å’Œæ˜¯blockå±‚é¢çš„ï¼Œä¹Ÿæ˜¯æ¯ä¸ªtokenå±‚é¢çš„<br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">__shared__ float inv_mean;  
if (threadIdx.x == 0) {  
    inv_mean = rdqrtf(thread_sum / hidden_units + eps);  
}    
__syncthreads(); // share memory inv_meanå†™å…¥å®Œæˆåè¦åŠ ä¸Šè¿™å¥è¯  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>ç”¨äºè®¡ç®—å¹³å‡å€¼$inv_mean\dfrac{1}{\sqrt{\sum^iE(x_i^2)+eps}}$<br>    <code>inv_mean</code>ï¼šå› ä¸ºå‡å€¼æ˜¯<code>block</code>å±‚é¢çš„ï¼Œæ‰€ä»¥æœ€å¥½æŠŠå®ƒè®¾ä¸ºshare memory<br>    share memoryå†™å…¥å®Œæˆåè¦åŠ ä¸Š<code>__syncthreads();</code><br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">Vec_t *s = reinterpret_cast&lt;Vec_t *&gt;(scale);  
    for (int idx = threadIdx.x; idx &lt; hidden_units / vec_size; idx += blockDim.x) {  
        Vec_t vec = dout[idx];  
        dout[idx].x = vec.x * inv_mean * s[idx].x; // å› ä¸ºè¾“å…¥è¾“å‡ºéƒ½æ˜¯decoder_inï¼Œæ‰€ä»¥éœ€è¦å®å®åœ¨åœ¨åœ°è¿›dout[idx]è¿™ä¸ªæŒ‡é’ˆæŒ‡å‘çš„bufferï¼Œç­‰å·å·¦è¾¹ä¸èƒ½ç”¨vec  
        dout[idx].y = vec.y * inv_mean * s[idx].y; // å› ä¸ºvec sizeæ˜¯4ï¼Œæ‰€ä»¥ç´¯åŠ 4æ¬¡  
        dout[idx].z = vec.z * inv_mean * s[idx].z;  
        dout[idx].w = vec.w * inv_mean * s[idx].w;  
    }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>ç”¨äºè®¡ç®—$inv_mean Ã— x_i Ã—g_i$<br>æ³¨æ„ï¼šéœ€è¦æŠŠç»“æœå†™å›<code>dout</code>ä¸­<p></p>
<p>(4)<code>launchRMSNorm</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
void launchRMSNorm( TensorWrapper&lt;T&gt;* decoder_out,
                    TensorWrapper&lt;T&gt;* decoder_residual,
                    LayerNormWeight&lt;T&gt;&amp; attn_norm_weight,
                    bool is_last 
                    ){  
    int num_tokens = decoder_out-&gt;shape[0];  
    int hidden_units = decoder_out-&gt;shape[1];  
    int vec_size = Vec&lt;T&gt;::size;  
    int num_threads = hidden_units / 4;
    T* rsd = decoder_residual-&gt;data;  
    dim3 grid(num_tokens);   // num_tokensä¸ªblock
    dim3 block(num_threads); // hidden_units / 4ä¸ªblock
    RMSNorm&lt;T&gt;&lt;&lt;&lt;grid, block&gt;&gt;&gt;(decoder_out-&gt;data,  
                            rsd,  
                            attn_norm_weight.gamma, // scale
                            eps,  
                            num_tokens,  
                            hidden_units);
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<h1 id="Lesson5-Casual-Mask"><a href="#Lesson5-Casual-Mask" class="headerlink" title="Lesson5 Casual Mask"></a>Lesson5 Casual Mask</h1><p>è®²è§£äº†ï¼š<br><code>src/kernels/build_casual_mask.cu</code><br><code>src/kernels/build_casual_mask.h</code><br><code>tests/unittests/test_casual_mask.cu</code></p>
<p><code>src/kernel/build_casual_mask.cu</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
__global__ void BuildCausalMasksConsideringContextPastKV(T* mask,
                                                const int* q_lens,
                                                const int* k_lens,
                                                int max_q_len, 
                                                int max_k_len){
    int tid = threadIdx.x;  
    int qlen = q_lens[blockIdx.x];
    int klen = k_lens[blockIdx.x];
    mask += blockIdx.x * max_k_len * max_q_len; // æ¯ä¸ªblockåªæœ‰256ä¸ªçº¿ç¨‹ï¼Œç›¸åº”çš„ï¼Œmaskä¹Ÿéœ€è¦æœ‰åç§»é‡ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªmaskä¸Šï¼Œä¸blockçš„ç§»åŠ¨åŒæ­¥  
    while(tid &lt; max_k_len * max_q_len){  
        int q = tid / max_k_len; // ç›®å‰å¤„äºå“ªä¸€è¡Œ  
        int k = tid % max_k_len; // ç›®å‰å¤„äºå“ªä¸€åˆ—  
        bool is_one = q &lt; qlen &amp;&amp; k &lt; klen &amp;&amp; k &lt;= q + (klen - qlen);  
        mask[tid] =  static_cast&lt;T&gt;(is_one);  
        tid += blockDim.x; 
    }
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>å‚æ•°ï¼š<br>    <code>mask</code>ï¼š<code>[batch_size, max_q_len, max_k_len]</code>æ¯ä¸ª<code>mask</code>æ˜¯ä¸€ä¸ªçŸ©é˜µï¼Œç”¨äºè¡¨ç¤ºå“ªäº›tokenå¯¹äºç›®å‰å¯¹è¯æ˜¯å¯è§çš„(ç½®1)å’Œä¸å¯è§çš„(ç½®0)<br>    <code>q_lens</code>ï¼š<code>[batch_size]</code>ï¼Œä½œä¸ºinput lensï¼Œæˆ‘çš„ç†è§£æ˜¯å½“å‰å¯¹è¯çš„è¾“å…¥<br>    <code>k_lens</code>ï¼š<code>[batch_size]</code>ï¼Œä½œä¸ºcontext lensï¼Œæˆ‘çš„ç†è§£æ˜¯ç»“åˆä¸€å®šç¨‹åº¦çš„ä¸Šä¸‹æ–‡çš„è¾“å…¥<br>    <code>max_q_len</code>&amp;<code>max_k_len</code>ï¼šåˆ†åˆ«æ˜¯<code>q_lens</code>å’Œ<code>k_lens</code>ä¸­æœ€å¤§çš„<br>ç†è§£ï¼š<br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int qlen = q_lens[blockIdx.x];
int klen = k_lens[blockIdx.x];<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><br>ğŸ‘†æ¯ä¸ª<code>block</code>å¯¹åº”ä¸€ä¸ªå¯¹è¯ï¼Œ<code>batch_size</code> = å¯¹è¯ä¸ªæ•°ã€‚è¿™é‡Œæ˜¯åˆ†åˆ«å–æ¯ä¸ªå¯¹è¯çš„<code>qlen</code>å’Œ<code>klen</code><br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">mask += blockIdx.x * max_k_len * max_q_len;
   while(tid &lt; max_k_len * max_q_len){  
       int q = tid / max_k_len; // ç›®å‰å¤„äºå“ªä¸€è¡Œ  
       int k = tid % max_k_len; // ç›®å‰å¤„äºå“ªä¸€åˆ—  
       bool is_one = q &lt; qlen &amp;&amp; k &lt; klen &amp;&amp; k &lt;= q + (klen - qlen);  
       mask[tid] = static_cast&lt;T&gt;(is_one);  
       tid += blockDim.x; 
   }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li>ä¸ºäº†ç¡®ä¿<code>mask</code>é‡Œçš„æ¯ä¸ªæ•°éƒ½èƒ½è¢«å¤„ç†åˆ°ï¼ˆç¬¬ä¸‰å¥å¾ˆé‡è¦ï¼‰<ul>
<li><code>mask[tid] = static_cast&lt;T&gt;(is_one);</code>ï¼š<code>block</code>ä¸­çš„ä¸€ä¸ªçº¿ç¨‹å¯¹åº”<code>mask</code>é‡Œçš„ä¸€ä¸ªæ•°ï¼Œä½†æ˜¯<code>blockDim.x=256</code>ï¼Œæ‰€ä»¥éœ€è¦åŠ ä¸Šç¬¬ä¸‰å¥è¯</li>
<li>å¾ªç¯æ¡ä»¶<code>tid &lt; max_k_len * max_q_len</code>ï¼šç¡®ä¿æ¯ä¸ªæ•°éƒ½æœ‰å¯¹åº”çº¿ç¨‹å¤„ç†</li>
<li><code>mask += blockIdx.x * max_k_len * max_q_len;</code>ï¼š</li>
<li><code>mask</code>çš„å¤§å°ï¼<code>block</code>çš„çº¿ç¨‹æ•°çš„æƒ…å†µğŸ‘‡</li>
</ul>
</li>
</ul>
<p>é‚£ä¹ˆå°±æ˜¯ä¸€ä¸ª<code>block</code>å¤„ç†ä¸€ä¸ª<code>mask</code>ï¼Œå¦‚æœ<code>block</code>å¤§å°å°äº<code>mask</code>çš„è¯ï¼Œå°±ç»§ç»­ç”¨è¯¥<code>block</code>çš„çº¿ç¨‹å¤„ç†<code>mask</code>å‰©ä½™çš„æ•°</p>
<h1 id="Lesson6-Linear"><a href="#Lesson6-Linear" class="headerlink" title="Lesson6 Linear"></a>Lesson6 Linear</h1><p>è®²è§£äº†ï¼š<br><code>src/kernels/cublas_utils.h</code>ï¼šå®šä¹‰cublasç±»<br><code>src/kernels/cublas_utils.cc</code>ï¼šå®ç°cublasç±»<br><code>src/kernels/linear.cu</code><br><code>src/kernels/linear.h</code><br><code>tests/unittests/test_linear.cu</code></p>
<h2 id="6-1-cublasç±»çš„å£°æ˜ä¸å®šä¹‰"><a href="#6-1-cublasç±»çš„å£°æ˜ä¸å®šä¹‰" class="headerlink" title="6.1 cublasç±»çš„å£°æ˜ä¸å®šä¹‰"></a>6.1 cublasç±»çš„å£°æ˜ä¸å®šä¹‰</h2><p><code>src/kernels/cublas_utils.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">class cublasWrapper {  
private:  
    cublasHandle_t cublas_handle;
    cudaDataType_t Atype;
    cudaDataType_t Btype;
    cudaDataType_t Ctype;
    cublasComputeType_t computeType;
public:  
    cublasWrapper(cublasHandle_t cublas_handle);  
    ~cublasWrapper();
    void setFP32GemmConfig();
    void setFP16GemmConfig();
    void Gemm(cublasOperation_t transa,  
              cublasOperation_t transb,  
              const int         m,  
              const int         n,  
              const int         k,  
              const void*       A,  
              const int         lda,  
              const void*       B,  
              const int         ldb,  
              void*             C,  
              const int         ldc,  
              float             alpha,  
              float             beta);  
        // for qk*v and q*k    
    void stridedBatchedGemm(cublasOperation_t transa,  
	                        cublasOperation_t transb,  
							const int         m,  
							const int         n,  
							const int         k,  
							const void*       A,  
							const int         lda,  
							const int64_t     strideA,  
							const void*       B,  
							const int         ldb,  
							const int64_t     strideB,  
							void*             C,  
							const int         ldc,  
							const int64_t     strideC,  
							const int         batchCount,  
							float             f_alpha,  
							float             f_beta);  
};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>ğŸ‘†å£°æ˜<code>cublasWrapper</code>ç±»ï¼Œ<code>batchedGemm</code>ç›¸å¯¹äº<code>Gemm</code>å¤šäº†æ­¥é•¿<code>stride</code>å’Œ<code>batchCount</code><p></p>
<p>å®šä¹‰éƒ¨åˆ†ï¼š<br>â‘ æ„é€ å‡½æ•°<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">cublasWrapper::cublasWrapper(cublasHandle_t cublas_handle,  
                             cublasLtHandle_t cublaslt_handle):  
    cublas_handle(cublas_handle),  
    cublaslt_handle(cublaslt_handle){  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li><code>cublasHandle_t</code>æ˜¯cublasåº“ä¸­çš„ä¸€ä¸ªç±»å‹ï¼Œä¸å¥æŸ„æœ‰å…³</li>
<li>ä¼ å…¥<code>cublas_handle</code>è¿”å›åˆ°ç±»ä¸­çš„<code>cublas_handle_</code></li>
<li><code>cublasHandle_t</code>å’Œ<code>cublasLtHandle_t</code><ul>
<li><code>cublasHanle_t</code>ï¼šç”¨äºä¸€èˆ¬çš„çº¿æ€§ä»£æ•°è¿ç®—ï¼ˆå¦‚å‘é‡å’ŒçŸ©é˜µæ“ä½œï¼‰</li>
<li><code>cublasLtHandle_t</code>ï¼šç”¨äºæ›´é«˜çº§çš„çŸ©é˜µè¿ç®—ï¼Œç‰¹åˆ«æ˜¯è‡ªå®šä¹‰å’Œä¼˜åŒ–çŸ©é˜µä¹˜æ³•ï¼ˆGEMMï¼‰ï¼Œåœ¨éœ€è¦å¤æ‚é…ç½®æˆ–å¤šç§æ•°æ®ç±»å‹æ—¶æœ‰ç”¨å¤„</li>
</ul>
</li>
</ul>
<p>â‘¡å•ç²¾åº¦ä¸åŠç²¾åº¦çš„é…ç½®<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void cublasWrapper::setFP32GemmConfig()  
{  
    Atype       = CUDA_R_32F;  
    Btype       = CUDA_R_32F;  
    Ctype       = CUDA_R_32F;  
    computeType = CUBLAS_COMPUTE_32F; // 
}  
  
void cublasWrapper::setFP16GemmConfig()  
{  
    Atype       = CUDA_R_16F;  
    Btype       = CUDA_R_16F;  
    Ctype       = CUDA_R_16F;  
    computeType = CUBLAS_COMPUTE_16F;  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li>å¯¹äº<code>computeType</code>ï¼Œå½“cuda version&lt;11.0æ—¶ç”¨CUDA_R_32Fï¼Œcuda version&gt;11.0æ—¶ä½¿ç”¨CUBLAS_COMPUTE_32Fï¼ŒåŠç²¾åº¦çš„åŒç†</li>
</ul>
<p>â‘¢ä¸º<code>alpha</code>å’Œ<code>beta</code>èµ‹å€¼<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">const void* alpha = is_fp16_computeType ? reinterpret_cast&lt;void*&gt;(&amp;(h_alpha)) : reinterpret_cast&lt;void*&gt;(&amp;f_alpha);  
const void* beta  = is_fp16_computeType ? reinterpret_cast&lt;void*&gt;(&amp;(h_beta)) : reinterpret_cast&lt;void*&gt;(&amp;f_beta);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p></p>
<ul>
<li>å¦‚æœ<code>is_fp16_computeTyp</code>ä¸º1ï¼Œåˆ™ä¼ å…¥åŠç²¾åº¦çš„<code>alpha</code>ç»™<code>alpha</code>ï¼Œ<code>beta</code>åŒç†</li>
</ul>
<p>â‘£å…³äºbatchedGemmä¸GemmåŒç†</p>
<h2 id="6-2-Gemm"><a href="#6-2-Gemm" class="headerlink" title="6.2 Gemm"></a>6.2 Gemm</h2><p><code>src/kernels/linear.cu</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
void launchLinearGemm(TensorWrapper&lt;T&gt;* input,  
                      BaseWeight&lt;T&gt;&amp; weight,   
					  TensorWrapper&lt;T&gt;* output,  
                      cublasWrapper* cublas_wrapper,  
                      bool trans_a = false,  
                      bool trans_b = false){
                      {  
Bk = input-&gt;shape.size() == 3 ? input-&gt;shape[1] * input-&gt;shape[2] : input-&gt;shpe[1];  
Cm = output-&gt;shape.size() == 3 ? output-&gt;shape[1] * output-&gt;shape[2] : output-&gt;shpe[1];  
  
int lda = Am;  
int ldb = Bk;  
int ldc = Cm;  

cublasOperation_t transA = trans_b ? CUBLAS_OP_T : CUBLAS_OP_N; 
cublasOperation_t transB = trans_a ? CUBLAS_OP_T : CUBLAS_OP_N;

// å¯èƒ½ä¼šå‡ºç°è¾“å…¥ä¸º[bs, 1, hiddenunits] * [hiddenunits, hiddenunits]ï¼Œæ‰€ä»¥éœ€è¦æ£€æŸ¥è¾“å…¥çš„ç»´åº¦
if(!trans_b &amp;&amp; !trans_a){   
    LLM_CHECK_WITH_INFO(Ak == Bk, "2nd dim of input MUST = 1st dim of weight!");  
}  
  
cublas_wrapper-&gt;Gemm(transA,  
                     transB,  
                     trans_b ? Ak : Am,           // m
                     Cn,                          // n
                     Bk,                          // k
                     weight.data,  
                     lda,  
                     input-&gt;data,  
                     ldb,  
                     output-&gt;data,  
                     ldc,  
                     1.0f,  
                     0.0f);}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>â‘  å…³äºAã€Bã€C<br>(1)ä¸€èˆ¬çš„gemm<br>Aï¼š<code>input shape = [seqlen, hidden_units]</code><br>Bï¼š<code>weight shape = [hidden_units, hidden_units]</code><br><code>A * B = C with trans_b = false</code><p></p>
<p>å¯¹äºqkvlinearï¼Œæ˜¯æŒ‡å°†ä¸‰æ¬¡çŸ©é˜µä¹˜æ³•èåˆåˆ°ä¸€æ¬¡<br><code>input=[seqlen, hidden_units]</code><br><code>weight shape = [hidden_units, 3Ã—hidden_units]</code></p>
<p>(2)å‡ºç°åœ¨samplingçš„<code>LMHead</code><br>Aï¼š<code>input shape = [batch_size, hidden_units]</code><br>Bï¼š<code>weight_shape = [vocabulary_size, hidden_units]</code><br><code>A * B = C with transb = true</code></p>
<p>â‘¡é‡ç‚¹ä¸éš¾ç‚¹ï¼š</p>
<ul>
<li><code>torch.nn.linear</code>çš„è®¡ç®—å…¬å¼æ˜¯$y=xÃ—w^T$ï¼Œä¿®æ”¹ä¹‹å‰æ˜¯$y=xÃ—w$ï¼Œå› æ­¤<code>trans_b=True</code></li>
<li>cublas APIæ¥å—çš„è¾“å…¥ä»¥åŠè¾“å‡ºçš„å†…å­˜æ’å¸ƒå…¨éƒ¨éƒ½é»˜è®¤ä¸º<strong>åˆ—ä¸»åº(column-major)</strong><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/Pasted%20image%2020241019161646.png" alt=""></li>
<li>å› æ­¤ï¼Œæˆ‘ä»¬çš„æ€è·¯æ˜¯<ul>
<li>ä»åŸæœ¬çš„$y=xÃ—w$ï¼Œå› ä¸º<code>nn</code>çš„è®¡ç®—æ–¹å¼</li>
<li>åŠ ä¸Š<code>trans_b=True</code>åå¯ä»¥å®ç°$y=xÃ—w^T$ï¼Œå› ä¸ºåˆ—ä¸»åº(ä»è¡Œä¸»åºåˆ°åˆ—ä¸»åºéœ€è¦å°†ä¸¤è¾¹åŒæ—¶è½¬ç½®)<ul>
<li>è¿™é‡Œçš„<code>trans_b</code>å¯¹åº”çš„æ˜¯åŸæœ¬æˆ‘ä»¬ç†è§£çš„$y=xÃ—w$å…¬å¼ï¼Œ<code>trans_b</code>å¯¹åº”$w$</li>
<li>å˜æˆcolumn majorä¹‹å<code>A</code>å¯¹åº”$w^T$ï¼Œæ‰€ä»¥æ˜¯ç”¨<code>trans_b</code>å†³å®š<code>trans_A</code></li>
</ul>
</li>
<li>å°†$y=xÃ—w^T$å˜æˆ$y^T=wÃ—x^T$åå¯ä»¥å®ç°åˆ—åºåˆ—çš„è¦æ±‚ï¼Œé‚£ä¹ˆå¯¹åº”$y=xÃ—w$å°±åº”è¯¥å˜æˆ$y^T=w^TÃ—x^T$</li>
<li>å³ä»åŸå§‹çš„$y=xÃ—w$å˜æˆæˆ‘ä»¬éœ€è¦çš„å…¬å¼ï¼Œåªéœ€è¦<ul>
<li>æ·»åŠ <code>trans_b=True</code></li>
<li>å…¬å¼$y^T=w^TÃ—x^T$</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int Am = weight.shape[1];
int Ak = weight.shape[0];  
int Bk = input-&gt;shape[1];  
int Bn = input-&gt;shape[0];  
int Cm = output-&gt;shape[1];  
int Cn = output-&gt;shape[0];<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="6-3-StrideBatchGemm"><a href="#6-3-StrideBatchGemm" class="headerlink" title="6.3 StrideBatchGemm"></a>6.3 StrideBatchGemm</h2><p>â‘ å…³äºinput1å’Œinput2<br>$qÃ—k$<br>input1ï¼š<code>q shape = [batch_size, head_nums, seqlen(=len_q), hidden_units]</code><br>input2ï¼š<code>k shape = [batch_size, head_nums, seqlen(=len_k), hidden_units]</code><br><code>A * B = C with trans_b = true</code><br>$qkÃ—v$<br>input1ï¼š<code>qk shape = [batch_size, head_nums, seqlen(=len_q), seqlen(=len_k)]</code><br>input2ï¼š<code>v shape = [batch_size, head_nums, seqlen(=len_k), hidden_units]</code><br><code>A * B = C with transb = false</code><br>å®é™…ä¸Šåœ¨<code>src/kernels/linear.cu</code>ä¸­å¤„ç†è¿‡ç¨‹ä¸Gemmå·®ä¸å¤š</p>
<p>â‘¡StrideBatchGemmå’ŒBatchGemmç›¸æ¯”<br>å‡å¦‚<code>A=[1,2,3,4]</code></p>
<ul>
<li>StrideBatchå¤šä¸€ä¸ª<code>Stride</code>å˜é‡ï¼Œç”¨äºä½œåœ°å€åç§»å–å‡ºè¦ç›¸ä¹˜çš„å€¼ï¼Œåç§»é‡ç­‰äº<code>A[i]</code>å’Œ<code>A[i+1]</code>ä¹‹é—´çš„è·ç¦»<ul>
<li>=3*4</li>
</ul>
</li>
<li>ä¸¤ä¸ªéƒ½æœ‰<code>batchCount</code>å˜é‡<ul>
<li>å¯¹äºStrideBatchæ˜¯æ¯ä¸ªæ‰¹æ¬¡ä¸­éœ€è¦ç›¸ä¹˜çš„çŸ©é˜µä¸ªæ•° = 1*2</li>
<li>BatchGemmæ˜¯Aã€Bã€Cä¸­æŒ‡é’ˆä¸ªæ•°ï¼ŒåŠçŸ©é˜µä¸ªæ•°<h2 id="6-4-å…¶ä»–"><a href="#6-4-å…¶ä»–" class="headerlink" title="6.4 å…¶ä»–"></a>6.4 å…¶ä»–</h2><code>cublasHanle_t</code>ç”¨äºå®šä¹‰ä¸€ä¸ªå¥æŸ„ï¼Œç”¨äºç®¡ç†å’Œé…ç½® <code>cuBLAS</code> åº“ä¸­çš„æ‰€æœ‰å‡½æ•°è°ƒç”¨ï¼Œç±»ä¼¼ä¸€ä¸ªæ§åˆ¶å™¨(å¼€/å…³)</li>
</ul>
</li>
</ul>
<p>åˆå§‹åŒ–åˆ—è¡¨ä¾‹å­ ï¼š<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">class Person {
public:
	////ä¼ ç»Ÿæ–¹å¼åˆå§‹åŒ–
	//Person(int a, int b, int c) {
	//	m_A = a;
	//	m_B = b;
	//	m_C = c;
	//}

	//åˆå§‹åŒ–åˆ—è¡¨æ–¹å¼åˆå§‹åŒ–
	Person(int a, int b, int c) :m_A(a), m_B(b), m_C(c) {}
	void PrintPerson() {
		cout &lt;&lt; "mA:" &lt;&lt; m_A &lt;&lt; endl;
		cout &lt;&lt; "mB:" &lt;&lt; m_B &lt;&lt; endl;
		cout &lt;&lt; "mC:" &lt;&lt; m_C &lt;&lt; endl;
...
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<p><code>RAII</code>æœºåˆ¶å¯ä»¥è‡ªåŠ¨ææ„æ‰ä¸€äº›ç±»æˆå‘˜å˜é‡</p>
<p>huggingfaceçš„7b chatä¸­linearçš„weightå…¨æ˜¯è½¬ç½®åçš„ï¼Œæ¯”å¦‚<code>gate</code>çš„æƒé‡åº”è¯¥æ˜¯<code>[q_hidden_units, inter_size]</code>ï¼Œä½†æ˜¯åœ¨huggingfaceé‡Œæ˜¯<code>[inter_size, q_hidden_units]</code>ï¼Œæ‰€ä»¥<code>launchLinearGemm</code>çš„<code>trans_b</code>å¯¹äºæ‰€æœ‰linear weightsæ¥è¯´éƒ½æ˜¯<code>true</code></p>
<h1 id="Lesson7-Debug-ä¸€"><a href="#Lesson7-Debug-ä¸€" class="headerlink" title="Lesson7 Debug(ä¸€)"></a>Lesson7 Debug(ä¸€)</h1><p><code>src/kernels/rmsnorm_kernel.cu</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">dout[idx].x = __float2half(vec.x * inv_mean) * s[idx].x;
dout[idx].y = __float2half(vec.y * inv_mean) * s[idx].y;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><br>å‡ºç°å¦‚ä¸‹æŠ¥é”™ï¼š<br><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">error: <span class="token function">more</span> than one operator <span class="token string">"*"</span> matches these operands:
built-in operator <span class="token string">"arithmetic * arithmetic"</span>
<span class="token keyword">function</span> <span class="token string">"operator*(const __half &amp;, const __half &amp;)"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br>åœ¨ç¼–è¯‘å™¨æ‰§è¡Œä¹˜æ³•è¿ç®—æ—¶ï¼Œå‘ç°æœ‰å¤šä¸ªç¬¦åˆæ¡ä»¶çš„*æ“ä½œç¬¦ä½†æ˜¯ä¸ç¡®å®šåº”è¯¥ä½¿ç”¨å“ªä¸€ä¸ª<p></p>
<ul>
<li><code>built-in operator "arithmetic * arithmetic"</code>ï¼šè¿™æ˜¯CUDAæ”¯æŒçš„åŸºæœ¬ç®—æœ¯ç±»å‹ä¹‹é—´çš„ä¹˜æ³•æ“ä½œï¼ˆå¦‚æ•´æ•°æˆ–æµ®ç‚¹æ•°ï¼‰ã€‚</li>
<li><code>function "operator*(const __half &amp;, const __half &amp;)"</code>ï¼šè¿™æ˜¯CUDAä¸­é’ˆå¯¹<code>__half</code>ç±»å‹ï¼ˆå³åŠç²¾åº¦æµ®ç‚¹æ•°ï¼‰æä¾›çš„ä¹˜æ³•æ“ä½œç¬¦ã€‚<br>è§£å†³æ–¹æ³•ï¼š<br>å°†ä»£ç æ”¹ä¸º<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">dout[idx].x = s[idx].x * __float2half(__half2float(vec.x) * inv_mean);
dout[idx].y = s[idx].y * __float2half(__half2float(vec.y) * inv_mean);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
</li>
</ul>
<p>ç¼–è¯‘é¡ºåºä»<code>kernels</code>åˆ°<code>tests</code>åŸå› ï¼š</p>
<ul>
<li>ç¼–è¯‘<code>tests</code>æ—¶éœ€è¦è°ƒç”¨åˆ°<code>src/kernels</code>çš„cudaå‡½æ•°æˆ–è€…launchå‡½æ•°ï¼Œæ‰€ä»¥éœ€è¦å…ˆç¼–è¯‘<code>kernels</code>æ–‡ä»¶ä¸‹çš„</li>
<li>åœ¨æ ¹ç›®å½•ä¸‹çš„<code>CMakeList.txt</code>ä¸­æœ‰å…ˆåé¡ºåº<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">add_subdirectory(src)
add_subdirectory(tests)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
</li>
</ul>
<h1 id="Lesson8-RoPE"><a href="#Lesson8-RoPE" class="headerlink" title="Lesson8 RoPE"></a>Lesson8 RoPE</h1><p><a target="_blank" rel="noopener" href="https://www.53ai.com/news/qianyanjishu/1291.html">ä¸€æ–‡çœ‹æ‡‚ LLaMA ä¸­çš„æ—‹è½¬å¼ä½ç½®ç¼–ç ï¼ˆRotary Position Embeddingï¼‰</a><br>è®²è§£äº†ï¼š<br><code>src/kernels/qkv_bias_and_RoPE.cu</code><br><code>src/kernels/qkv_bias_and_RoPE.h</code><br><code>src/models/llama/llama_params.h</code><br><code>tests/unittests/test_bias_and_rope.cu</code><br><code>src/utils/vectorize_utils.h</code></p>
<p>æœ¬èŠ‚èåˆç®—å­çš„ä½œç”¨</p>
<ul>
<li>å°†<code>qkv bias</code>åŠ åˆ°<code>QKV</code>ä¸Šï¼Œ<code>QKV = [num tokens, qkv head num, head size]</code><ul>
<li><code>qkv head num</code> = <code>q head num</code> + <code>k head num</code> + <code>v head num</code></li>
<li><code>k head num</code> = <code>v head num</code></li>
</ul>
</li>
<li>paddingåï¼Œ<code>QKV</code>ä¼šè¢«åˆ†å‰²æˆä¸‰ä¸ªçŸ©é˜µ<code>q</code>ã€<code>k</code>ã€<code>v</code>ï¼Œ<ul>
<li>shape(q)=<code>[bs, q head num, max q len, head size]</code></li>
<li>shape(k/v)=<code>[bs, kv head num, max q len, head size]</code></li>
</ul>
</li>
<li>rope &amp; attention</li>
<li>å†™å›æ˜¾å­˜(gmem)</li>
</ul>
<p>è¾“å…¥ï¼š<br>    <code>QKV</code> shape=<code>[num tokens, qkv head num, head size]</code><br>    <code>qkv bias</code> shape = <code>[qkv head num, head size]</code><br>è¾“å‡ºï¼š<br>    <code>q</code>ï¼š<code>[bs, q head num, max q len, head size]</code><br>    <code>k</code>ï¼š<code>[bs, kv head num ,max q len, head size]</code><br>    <code>v</code>ï¼š<code>[bs, kv head num, max q len, head size]</code><br>    è¿™é‡Œçš„<code>max q len</code>å°±æ˜¯<code>seqlen</code><br>ä¸‹ä¸€èŠ‚ä¼šè®²åˆ°<code>repeat kv</code></p>
<h2 id="8-1-src-kernels-qkv-bias-and-RoPE-cu"><a href="#8-1-src-kernels-qkv-bias-and-RoPE-cu" class="headerlink" title="8.1 src/kernels/qkv_bias_and_RoPE.cu"></a>8.1 <code>src/kernels/qkv_bias_and_RoPE.cu</code></h2><p>llamaä½¿ç”¨çš„æ˜¯QGA(Grouped-Query Attention)ï¼Œé‡‡ç”¨çš„æ˜¯ä¸€ç»„Q(Nä¸ª)å…±äº«åŒä¸€ä¸ªKV</p>
<p><code>QKV</code>ç¬¬ä¸€ä¸ªç»´åº¦æ˜¯<code>token_num</code>ï¼Œå› æ­¤ç½‘æ ¼çš„ç¬¬ä¸€ä¸ªç»´åº¦xä¹Ÿæ˜¯<code>token_num</code>ï¼Œç½‘æ ¼çš„ç¬¬äºŒä¸ªç»´åº¦yæ˜¯<code>head_num</code>(<code>q head num</code>)</p>
<p><code>qkv</code>ç±»å‹æ˜¯<code>BaseWeight&lt;T&gt;</code>ï¼Œåœ¨<code>src/weights/base_weights.h</code>ä¸­<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;
struct BaseWeight{
	std::vector&lt;int&gt; shape;
	T* data;
	WeightType type;
	T* bias; // qkvéœ€è¦è¿™ä¸€é¡¹
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<p>â‘ <code>GetRoPRfreq()</code>æ˜¯ç”¨æ¥æ±‚$Î¸$å’Œ$m$çš„<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">inline __device__ float2 GetRoPEfreq(int zid, int rot_embed_dim, float base, float t_step) {  
    float inv_freq = t_step / powf(base, zid / (float)rot_embed_dim); // æ±‚mÎ¸  
    return{cos(inv_freq), sin(inv_freq)};  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><br>å…¬å¼ï¼š$Î˜=\{Î¸_i=10000^{-2(i-1)/d},i\in[1,2,â€¦,d/2]\}$<br>å…¥å‚ï¼š<p></p>
<ul>
<li><code>zid</code>ï¼š<code>2(i-1)</code></li>
<li><code>rot_embed_dim</code>ï¼š<code>d</code>ï¼Œè¯åµŒå…¥å‘é‡çš„ç»´åº¦</li>
<li><code>base</code>ï¼šå…¬å¼ä¸­çš„10000</li>
<li><code>t_step</code>ï¼štime stepï¼Œæ˜¯è¦æ±‚çš„<code>m</code>ï¼Œ<code>m</code>è¡¨ç¤ºç¬¬<code>m</code>ä¸ªtoken<br>å˜é‡ï¼š</li>
<li><code>inv_freq</code>ï¼šå°±æ˜¯$mÎ¸_i$<ul>
<li>$mÎ¸_i=m\ Ã·\ 10000^{2(i-1)/d}$</li>
<li>$10000^{2(i-1)/d}$ = <code>powf(base,zid / (float)d)</code><ul>
<li><code>base=10000</code></li>
<li><code>zid=2(i-1)</code></li>
<li>å› ä¸ºä¼ è¿›æ¥çš„<code>rot_embed_dim</code>æ˜¯<code>int</code>å‹çš„ï¼Œæ‰€ä»¥åŠ äº†ä¸ª<code>float</code></li>
</ul>
</li>
</ul>
</li>
<li>è¿”å›çš„æ˜¯$cos(mÎ¸_i)$å’Œ$sin(mÎ¸_i)$</li>
</ul>
<p>â‘¡<code>GetRoPEres()</code>æ˜¯ç”¨æ¥å¾—åˆ°RoPEåçš„ç»“æœçš„<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">inline __device__ float2 GetRoPEres(float data, float data_rotate, const float2 coef){  
    float2 rot_v;
    rot_v.x = coef.x * data - coef.y * data_rotate;
    rot_v.y = coef.x * data_rotate + coef.y * data;
    retern rot_v;  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>å…¥å‚ï¼š<p></p>
<ul>
<li><code>data</code>ï¼š<code>head_size</code>ä¸­çš„å‰åŠçš„æ•°æ®</li>
<li><code>data_rotate</code>ï¼š<code>head_size</code>ä¸­ååŠçš„æ•°æ®</li>
<li><code>coef</code>ï¼šé€šè¿‡<code>GetRoPRfreq()</code>å¾—åˆ°çš„$cos(mÎ¸_i)$å’Œ$sin(mÎ¸_i)$<br>å˜é‡ï¼š</li>
<li>(ä¸¾ä¾‹)<code>rot_v.x</code>=$cos(mÎ¸_0)\ <em>\ x_0\ -\ sin(mÎ¸_0)\ </em>\ x_{64}$</li>
<li>(ä¸¾ä¾‹)<code>rot_v.y</code>=$cos(mÎ¸_0)\ <em>\ x_{64}\ +\ sin(mÎ¸_0)\ </em>\ x_0$</li>
<li>ä¸Šé¢ä¸¤ä¸ªä¸ºä¸€ç»„<code>rot_v</code>ï¼Œä¸€ç»„æŒ‡çš„æ˜¯ä»–ä»¬å…±äº«$cos(mÎ¸_0)$å’Œ$sin(mÎ¸_0)$<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/d224a5131c3f2a304c66f0ccf912e90.jpg" alt=""></li>
</ul>
<p>â‘¢<code>add_fusedQKV_bias_transpose_kernel()</code><br>å®é™…ä¸Šå¹¶æ²¡æœ‰åŠ ä¸Šbiasåç½®é¡¹<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template &lt;typename T&gt;  
__global__ void add_fusedQKV_bias_transpose_kernel(
	T *q_buf,  
	T *k_buf,  
	T *v_buf,  
	T *QKV,  
	const int *padding_offset, // created before qkv linear  
	const int *history_length,  
	const int *input_length, // actual length of each seq  
	const int batch_size,  
	const int seq_len, // max_seq_len to pad to    
	const int head_num,  
	const int kv_head_num,  
	const int head_size,  
	const int rotary_embedding_dim,  
	float rotary_embedding_base // default 10000 in llama  
	)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<p>1)é…ç½®blockã€threadå’Œpadding<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int token_id = blockIdx.x;
int head_id = blockIdx.y; 
int tid = threadIdx.x;
int token_padding_offset = padding_offset[token_id];<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><br><code>token_id</code>å’Œ<code>head_id</code>ç”¨äºè·å¾—æ•°æ®åç§»é‡<br><code>token_padding_offset</code>æ˜¯è¯¥<code>token</code>ä¹‹å‰çš„paddingä¸ªæ•°<p></p>
<p>2)ä¸ºå†™åˆ°æ˜¾å­˜é‡Œé¢åšå‡†å¤‡<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int dst_token_id = token_id + token_padding_offset;
int batch_id = dst_token_id / seq_len;
int local_token_id = dst_token_id % seq_len;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br><code>dst_token_id</code>ï¼šå¯ä»¥ç†è§£ä¸ºå½“å‰<code>token_id</code>åœ¨å…¨éƒ¨<code>token</code>ä¸­çš„ä½ç½®<br>    <code>token_id</code>æ˜¯å½“å‰tokenåœ¨ä¸è€ƒè™‘paddingæ—¶çš„tokenä½ç½®<br>    <code>token_padding_offset</code>æ˜¯å½“å‰tokenä¹‹å‰çš„paddingä¸ªæ•°<br><code>batch_id</code>ï¼šå½“å‰tokenæ‰€åœ¨ä½ç½®çš„å¯¹åº”çš„å¥å­id<br><code>local_token_id</code>ï¼šå½“å‰tokenåœ¨å½“å‰å¥å­çš„ä½ç½®(0~seq_len-1)<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/8b1b3ff66b49b6249741cc3c0f00791.jpg" alt="|275"><br>ä¸ºäº†å†™åˆ°æ˜¾å­˜é‡Œæ‰åšçš„padding<p></p>
<p>3)åŸºäº(ä½œä¸ºè¾“å…¥)QKV bufferçš„ä¸‰ä¸ªç»´åº¦(<code>num tokens, qkv head num, head size</code>)è·å–qã€kã€v<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int qkv_head_num = head_num + 2 * kv_head_num; 
int q_id = token_id * qkv_head_num * head_size + head_id * head_size + tid;  
int k_id = token_id * qkv_head_num * head_size + head_id * head_size + tid + head_num * head_size;
int v_id = token_id * qkv_head_num * head_size + head_id * head_size + tid + head_num * head_size + kv_head_num * head_size;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><br><code>qkv_head_num</code>ï¼šå…¶ä¸­<code>head_num</code>æ˜¯<code>q_head_num</code><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/8d4904053dda536ed4635322867c4fa.jpg" alt="|425"><p></p>
<p>4)è®¡ç®—RoPE<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">const int cur_seq_history_len = history_length[batch_id];
const int context_length = cur_seq_history_len + input_length[batch_id]  
const int timestep = cur_seq_history_len + local_token_id; 
if(tid &gt;= rotary_embedding_dim / 2){ 
    return;  
}  
float2 cos_sin = GetRoPEfreq(tid * 2, rotary_embedding_dim, rotary_embedding_base, timestep);  
float2 q_rotate = GetRoPEres(QKV[q_id], QKV[q_id + head_size / 2], cos_sin);  
float2 k_rotate = GetRoPEres(QKV[k_id], QKV[k_id + head_size / 2], cos_sin);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>è¿™é‡Œçš„é•¿åº¦éƒ½ä»¥tokenä¸ºå•ä½<br><code>cur_seq_history_len</code>ï¼šå½“å‰åºåˆ—çš„å†å²çš„åºåˆ—é•¿åº¦æ€»å’Œ<br><code>context_length</code>ï¼šå½“å‰åºåˆ—é•¿åº¦+å†å²çš„åºåˆ—é•¿åº¦<br><code>timestep</code>ï¼šå†å²åºåˆ—é•¿åº¦+å½“å‰seqä¸­çš„tokenï¼Œå¾—åˆ°å½“å‰tokenåœ¨æ•´ä¸ªåºåˆ—ä¸­çš„ä½ç½®<p></p>
<p>llamaçš„æ—‹è½¬ç¼–ç æ˜¯å°†head sizeåˆ‡åˆ†æˆä¸¤åŠï¼Œå·¦ä¸€åŠä¸å³ä¸€åŠå¯¹åº”åšRoPEï¼Œæ‰€ä»¥å½“<code>tid &gt;= rotary_embedding_dim/2</code>æ—¶å°±å¯ä»¥åœæ­¢åšRoPEè®¡ç®—ï¼Œ<code>rotary_embedding_dim</code>æ˜¯è¯åµŒå…¥å‘é‡çš„ç»´åº¦ï¼Œè¿™é‡ŒæŒ‡çš„åº”è¯¥æ˜¯tokençš„ç»´åº¦</p>
<p>åœ¨<code>q_rotate</code>å’Œ<code>k_rotate</code>çš„è®¡ç®—è¿‡ç¨‹ä¸­ä¹Ÿèƒ½è¯å®<code>data</code>å’Œ<code>data_rotate</code>å¯¹åº”çš„æ˜¯çº¿ç¨‹ï¼Œæ‰€ä»¥åœ¨ä¸Šé¢çš„ifè¯­å¥ä¸­åªéœ€è¦ä¸€åŠçš„çº¿ç¨‹å³å¯</p>
<p>5)å†™å›gmem<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int dst_q_id = batch_id * seq_len * head_num * head_size +  
               head_id * seq_len * head_size +  
               local_token_id * head_size + tid;  
int dst_kv_id = batch_id * seq_len * kv_head_num * head_size +  
               head_id * seq_len * head_size +  
               local_token_id * head_size + tid;  
q_buf[dst_q_id] = q_rotate.x;  
q_buf[dst_q_id + head_size / 2] = q_rotate.y;  
if(head_id &lt; kv_head_num){  
    // å¯¹äºMQAå’ŒGQA  
    k_buf[dst_kv_id] = k_rotate.x;  
    k_buf[dst_kv_id + head_size / 2] = k_rotate.y;  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>ä¸‹é¢ç»™å‡ºäº†<code>dst_q_id</code>çš„ä¾‹å­<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/f602724b51ab74e9a401426a567845f.jpg" alt="|675"><p></p>
<p>â‘£<code>rope_kernel_for_self_decoder()</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
__global__ void rope_kernel_for_self_decoder(T* q,  
                    T* k,  
                    const int batch_size,  
                    const int head_num,  
                    const int kv_head_num,  
                    const int head_size,  
                    const int step,  
                    int   rotary_embedding_dim,  
                    float rotary_embedding_base)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>è¿™é‡Œä¸»è¦é’ˆå¯¹self decoder<p></p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int tid = threadIdx.x;  
int q_head_id = blockIdx.x;  
int q_batch_id = blockIdx.y;  
int kv_head_id = q_head_id / (head_num / kv_head_num); // å°†kv_head_idçš„æ•°é‡è†¨èƒ€åˆ°q_head_idçš„æ•°é‡  
int kv_batch_id = q_batch_id;  
  
int batch_stride = head_num * head_size; // seq len=1  
int kv_batch_stride = kv_head_num * head_size;
int head_stride = head_size;  
int q_offset = q_batch_id * batch_stride + q_head_id * head_stride + tid;  
int k_offset = kv_batch_id * kv_batch_stride + kv_head_id * head_stride + tid;  
if(tid &gt;= rotary_embedding_dim / 2){  
    return;  
}  
float2 cos_sin = GetRoPEfreq(tid * 2, rotary_embedding_dim, rotary_embedding_base, step - 1); // è¿™é‡Œé€šè¿‡ä¸hfç›¸æ¯”å‘ç°è¦-1
float2 q_rotate = GetRoPEres(q[q_offset], q[q_offset + head_size / 2], cos_sin);  
float2 k_rotate = GetRoPEres(k[k_offset], k[k_offset + head_size / 2], cos_sin);  
  
q[q_offset] = q_rotate.x;  
q[q_offset + head_size / 2] = q_rotate.y;  
k[k_offset] = k_rotate.x;  
k[k_offset + head_size / 2] = k_rotate.y;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>æœ€å<code>k[k_offset]</code>ä¸éœ€è¦åˆ¤æ–­<code>head_idx&lt;kv_head_num</code>æ˜¯å› ä¸º<code>int kv_head_id = q_head_id / (head_num / kv_head_num);</code>è¿™é‡Œçš„å¯¹åº”å…³ç³»ä¸ä¼šä»¤k headè¶Šå‡ºè¾¹ç•Œ<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/7b5bd7706fad306560cabfdbc75cb21.jpg" alt="|475"></p>
<h2 id="8-2-å…¶ä»–"><a href="#8-2-å…¶ä»–" class="headerlink" title="8.2 å…¶ä»–"></a>8.2 å…¶ä»–</h2><p><code>using Vec_t = Vec&lt;t&gt;::type;</code>å’Œ<code>using Vec_t = typename Vec&lt;t&gt;::type;</code>çš„åŒºåˆ«</p>
<ul>
<li>ä½¿ç”¨<code>typename</code>å…³é”®å­—ç”¨æ¥æ˜ç¡®å‘Šè¯‰ç¼–è¯‘å™¨<code>Vec&lt;t&gt;::type</code>æ˜¯ä¸€ä¸ªç±»å‹è€Œä¸æ˜¯ä¸€ä¸ª(é™æ€)æˆå‘˜</li>
<li>ä¸ä½¿ç”¨<code>typename</code>çš„å‰ææ˜¯ç¼–è¯‘å™¨å·²ç»ç¡®å®šäº†<code>Vec&lt;t&gt;::type</code>æ˜¯ä¸€ä¸ªç±»å‹ï¼Œä¸éœ€è¦<code>typename</code>åšæç¤º<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">// éœ€è¦typenameåšæç¤º
template&lt;typename T&gt;
struct Vec{
	using Type = T;
}

// ä¸éœ€è¦typenameåšæç¤º
struct Vec{
	using Type = int;
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ul>
<p><code>const_cast</code>ä¸»è¦ç”¨äºç§»é™¤(æˆ–æ·»åŠ )å¯¹è±¡çš„<code>const</code>é™å®šç¬¦ï¼Œå¯ä»¥ä¿®æ”¹é‚£äº›è¢«å£°æ˜ä¸º<code>const</code>çš„å˜é‡</p>
<h1 id="Lesson9-concat-past-kv-cache"><a href="#Lesson9-concat-past-kv-cache" class="headerlink" title="Lesson9 concat past kv cache"></a>Lesson9 concat past kv cache</h1><p>è®²è§£äº†ï¼š<br><code>src/kernels/concat_past_kv.cu</code><br><code>src/kernels/concat_past_kv.h</code><br><code>tests/unittests/test_concat_kv.cu</code></p>
<p>llamaä¸­<code>max_q_len</code>(å³<code>seq_len</code>)æ˜¯8192ï¼Œæ˜¯å…³æ³¨å¯¹è±¡ï¼›kå’Œvå†™åˆ°<code>max_q_len</code>éœ€è¦æ ¹æ®<code>history_len</code>æ‰¾åˆ°ç›¸åº”çš„ä½ç½®<br><code>kv cache shape = [num layers, bs, kv_head_num, max_seq_len, head_size]</code><br>â†“å…¶ä¸­ï¼Œmax_seq_lençš„ä½ç½®æ˜¯å†™åˆ°<br><code>[seqlen[history_len:history_len + max_q_len]]</code></p>
<p>è¿™ä¸€èŠ‚å†…å®¹ä¸å¤šï¼Œä½†æ˜¯æŠ˜ç£¨äº†æˆ‘æŒºé•¿æ—¶é—´çš„T.T<br>    ä¸»è¦æ˜¯<code>max_q_len</code>ã€<code>max_seq_len</code>ã€<code>history_len</code>ã€<code>cur_query_len</code>è¿™å‡ ä¸ªå˜é‡æ²¡å¼„æ˜ç™½(å¯èƒ½æ˜¯è§†é¢‘é»˜è®¤æˆ‘ä¼šå§å“ˆå“ˆ)</p>
<ul>
<li><code>max_q_len</code>ï¼šåšå®Œæ—‹è½¬ä¹‹åçš„kã€vçš„å¯¹åº”çš„æ¯ä¸ªbatchçš„é•¿åº¦ï¼Œå³tokençš„ä¸ªæ•°</li>
<li><code>max_seq_len</code>ï¼šè€ƒè™‘ä¸Šä¸‹æ–‡çš„æ¯ä¸ªbatchçš„é•¿åº¦ï¼Œå³tokençš„é•¿åº¦ï¼Œä»€ä¹ˆå«è€ƒè™‘ä¸Šä¸‹æ–‡å‘¢ï¼Œå°±æ˜¯å…¥å‚çš„æ—¶å€™ä¼šè¾“å…¥<code>history_len</code>çš„å°±æ˜¯ä¸Šæ–‡é•¿åº¦ï¼Œ<code>max_seq_len</code>ä½œä¸ºè¯¥batchçš„æœ€é•¿çš„é•¿åº¦</li>
<li><code>history_len</code>ï¼šè¿™ä¸ªbatchä¸­çš„ä¸Šæ–‡é•¿åº¦ï¼Œå³tokençš„é•¿åº¦</li>
<li><code>cur_query_len</code>ï¼šéœ€è¦è¿›è¡ŒæŸ¥è¯¢çš„é•¿åº¦(æ–°ç”Ÿæˆçš„tokençš„é•¿åº¦)<br><code>history_len + cur_query_len &lt;= max_seq_len</code></li>
</ul>
<p>éš¾ç‚¹å°±æ˜¯å†™å…¥çš„ä½ç½®çš„åç§»<code>dst_offset</code>ï¼Œå®é™…ä¸Šè¿™ä¸€èŠ‚ä¹Ÿæ˜¯è¦è§£å†³çš„é—®é¢˜å°±æ˜¯kv cacheçš„å†™å…¥ä½ç½®ï¼Œç»“åˆä»£ç çœ‹ä¸‹å›¾å°±å¥½äº†<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/11adf571dc54da75266cc135ab66c1f.jpg" alt=""><br>ğŸ‘†å½“layer=1çš„æƒ…å†µ<br>ğŸ‘‡è¿™é‡Œåªæ”¾keyçš„ï¼Œvalueçš„å’Œä»–å·®ä¸å¤š<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template &lt;typename T&gt;  
__global__ void append_key_cache(T *k_dst, // [num layers, bs, kv head num, max_q_len, head size]  
                                 const size_t layer_offset,  
                                 const T *k_src, // [bs, kv_head num, max_q_len, head size]  
                                 const int kv_head_num,  
                                 const int head_size,  
                                 const int *cur_query_length,  
                                 const int *history_length,// [batch_size]  
                                 const int max_q_len,  
                                 const int max_seq_len){  
    // æ ¹æ®è¿™é‡Œçš„dim3 grid(max_q_len, batch_size, kv_head_num);æ¥å†™ä¸‹é¢çš„ä¸‰è¡Œ  
    int batch_id = blockIdx.y;  
    int head_id = blockIdx.z;  
    int token_id = blockIdx.x;  
    int tid = threadIdx.x;  
    T* k_cache_dst = k_dst + layer_offset; // å°†kå†™åˆ°å½“å‰çš„layerä½ç½®ï¼Œç®—æ˜¯ä¸€ä¸ªå®šä½ï¼›k_dstæ˜¯æ‰€æœ‰kçš„èµ·å§‹ä½ç½®  
    int cumsum_seq_len = history_length[batch_id]; // å½“å‰batchåœ¨å½“å‰layerä¸­ç´¯ç§¯çš„å¥å­é•¿åº¦  

    int cur_seq_len = cur_query_length[batch_id];  
    if(token_id &lt; cur_seq_len){  
        // [bs, kv_head_num, max_q_len, head size] =&gt; [bs, kv_head_num, max_seq_len[cumsum_seq_len:cumsum_seq_len + max_q_len], head_size]  
        // åœ¨k_srcä¸Šçš„åç§»  
        int src_offset = batch_id * kv_head_num * max_q_len * head_size 
				       + head_id * max_q_len * head_size  
                       + token_id * head_size + tid;  
        // éœ€è¦å†™å…¥çš„ä½ç½®çš„åç§»  
        int dst_offset = batch_id * kv_head_num * max_seq_len*head_size 
				       + head_id * max_seq_len * head_size 
				       + (cumsum_seq_len + token_id) * head_size + tid;  
        k_cache_dst[dst_offset] = k_src[src_offset]; // k_srcæ˜¯å½“å‰layerçš„ï¼Œdst_offsetéœ€è¦åŠ ä¸Š  
    }  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<h1 id="Lesson10-RepeatKV-for-MQA-amp-GQA-kernel"><a href="#Lesson10-RepeatKV-for-MQA-amp-GQA-kernel" class="headerlink" title="Lesson10 RepeatKV for MQA&amp;GQA kernel"></a>Lesson10 RepeatKV for MQA&amp;GQA kernel</h1><p>è®²è§£äº†ï¼š<br><code>src/kernels/repeat_kv.cu</code><br><code>src/kernels/repeat_kv.h</code><br><code>test/unittests/test_repeat_kv.cu</code></p>
<p>å†™è¿™ä¸ªkernelçš„åŠ¨æœºï¼šå°†MHAè½¬æ¢ä¸ºMQAï¼Œç›®çš„æ˜¯å¹³è¡¡æ¨ç†é€Ÿåº¦å’ŒMHAæ‰€èƒ½è¾¾åˆ°çš„ç²¾åº¦ï¼›å› ä¸ºkå’Œvçš„æ•°é‡ä¸å¤´æ•°é‡æˆæ­£æ¯”ï¼Œæ‰€ä»¥è¦å‡å°å¤´çš„æ•°é‡å’Œsizeä»¥å‡å°å¸¦å®½å‹åŠ›ï¼ŒåŒæ—¶å› ä¸ºåé¢è¦åšQKgemmï¼Œå› æ­¤è¦çŸ©é˜µå¯¹é½</p>
<p>å°ºå¯¸å˜åŒ–ï¼š<br><code>[batch size, kv head num, max seq len, head size]=&gt;</code><br><code>[batch size, q head num, max k len, head size]</code></p>
<p><code>q_head_per_kv = head_num / kv_head_num</code>ï¼Œå³æ¯ä¸€ç»„k headæˆ–v headå¯¹åº”å¤šå°‘ç»„q headå…±ç”¨</p>
<p><code>dim3 grid((max_k_len * head_size + blockSize - 1) / blockSize, batch_size, head_num);</code><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/6a7c0ad2ea6497df0a680878fb4c32d.jpg" alt=""></p>
<p><code>src/kernels/repeat_kv.cu</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template &lt;typename T&gt;  
__global__ void repeat_value_cache(T *v_dst,  
                                   const T *v_src,  
                                   const size_t layer_offset,  
                                   const int head_num,  
                                   const int q_head_per_kv,  
                                   const int head_size,  
                                   const int *context_length,  
                                   const int max_k_len,  
                                   const int max_seq_len){  
    const int batch_id = blockIdx.y;  
    const int head_id = blockIdx.z;  
    const int gtid = blockIdx.x * blockDim.x + threadId.x;  
    const auto val_src = v_src + layer_offset;  
    const T* val_dst = v_dst;  
    const int seq_len = context_length[batch_id];  
    const int v_head_size_id = gtid % head_size; 
    const int v_seq_len_id = gtid / head_size;  

    if(v_seq_len_id &lt; seq_len){  
        const int src_id = batch_id * (head_num / q_head_per_kvï¼‰*
					       head_size * max_seq_len +  
                           head_id / q_head_per_kv * head_size * 
                           max_seq_len +  
                           v_seq_len_id * head_size +  
                           v_head_size_id;  
  
        const int dst_id = batch_id * head_num * head_size * max_k_len + 
                           head_id * head_size * max_seq_len +  
                           v_seq_len_id * head_size +  
                           v_head_size_id;  
        val_dst[dst_id] = val_src[src_id];  
    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/ef1ab04ddb1556e1cd39bef5cf2ac47.jpg" alt=""><br>å®é™…ä¸Šå°±æ˜¯æŒ‰ç…§<code>q head</code>çš„å¤§å°é‡æ–°æ’å¸ƒäº†<code>k head</code>æˆ–<code>v head</code>ï¼Œä½¿ä»–ä»¬ä¸€ä¸€å¯¹åº”ã€‚(å›¾ä¸­ç»¿è‰²éƒ¨åˆ†ä¸ºå¯¹åº”å…³ç³»ï¼Œæ¯<code>q_head_num/kv_head_num</code>ç»„<code>q hea</code>då…±ç”¨ä¸€ç»„<code>k head</code>æˆ–<code>v head</code>)<p></p>
<p>æ€»è§‰å¾—è¿™é‡Œçš„<code>max_k_len</code>æœ‰ç‚¹è¯¯å¯¼äººâ€¦åº”è¯¥ä¸æ˜¯<code>kv head num * max seq len = q head num * max k len</code>ï¼Œåªæ˜¯å•çº¯çš„æ‰©å±•äº†</p>
<h1 id="Lesson11-Fused-mask-amp-softmax"><a href="#Lesson11-Fused-mask-amp-softmax" class="headerlink" title="Lesson11  Fused mask&amp;softmax"></a>Lesson11  Fused mask&amp;softmax</h1><p>è®²è§£äº†ï¼š<br><code>src/kernels/attn_softmax_kernel.cu</code><br><code>src/kernels/attn_softmax_kernel.h</code></p>
<p><code>SumOp</code>å’Œ<code>MaxOp</code>çš„å®šä¹‰<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template &lt;typename T&gt;  
struct SumOp  
{  
    __device__ __forceinline__ T operator()(const T &amp;a, const T &amp;b) const { return a + b; }  
};  
  
template &lt;typename T&gt;  
struct MaxOp  
{  
    __device__ __forceinline__ T operator()(const T &amp;a, const T &amp;b) const { return max(a, b); }  
};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>ğŸ‘†è¿™æ ·å†™çš„ç›®çš„æ˜¯æ¨¡æ¿åŒ–<p></p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template &lt;template &lt;typename&gt; class ReductionOp, typename T&gt;  
__inline__ __device__ T warpReduce(T val)  
{  
    for (int mask = 32 / 2; mask &gt; 0; mask /= 2)  
    {        
	    val = ReductionOp&lt;T&gt;()(val, __shfl_xor_sync(0xffffffff, val, mask));  
    }    
    return val;  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>ğŸ‘†ä½¿ç”¨æ¨¡æ¿æ¨¡æ¿å‚æ•°<code>ReductionOp</code>ï¼Œåœ¨è°ƒç”¨<code>warpReduce</code>æ—¶ä¼ å…¥ä¸åŒçš„æ“ä½œç±»å‹<code>SumOp</code>å’Œ<code>MaxOp</code></p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">LLM-CHECK_WITH_INFO(k_length % 2 == 0, "K_len should be divided by 2 under half type!");<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>fp32ç±»å‹ä¸‹ä»¥float4åŠ›åº¦è¯»å†™ï¼ˆè¿˜æœªå®ç°ï¼‰ï¼Œfp16ç±»å‹ä¸‹ä»¥half2è¯»å†™ï¼Œè¿™é‡Œæ˜¯åªå¯¹fp16åšå‘é‡åŒ–ä½¿å…¶<code>vec_size=2</code>ï¼Œè€Œfp32å‘é‡åŒ–å<code>vec_size=1</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#define LAUNCH_SOFTMAX(dtype, vec_size)                                \ 
    if (block.x &gt; 2048 &amp;&amp; block.x &lt;= 4096)                             \ 
    {                                                                  \ 
        constexpr int NUMS_PER_THREAD_PER_ROW = 4;                     \ 
        block.x /= 4 * vec_size;                                       \ 
        block.x = (block.x + 32 - 1) / 32 * 32;                        \ 
        assert(block.x &lt; 1024);                                        \ 
        ScaleMaskAndSoftmax_##dtype&lt;dtype, NUMS_PER_THREAD_PER_ROW&gt;    \&lt;&lt;&lt;grid, block&gt;&gt;&gt;((dtype *)attn_score-&gt;data, \                                             (dtype *)qk-&gt;data,         \  
	              (dtype *)mask-&gt;data,       \                                             batch_size,                \                                             head_nums,                 \                                             q_length,                  \                                             k_length,                  \               
	              scale);                    \  
    }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li><code>NUMS_PER_THREAD_PER_ROW</code>ä½œä¸ºç¼–è¯‘å™¨å¸¸é‡</li>
<li>å¦‚æœå½“å‰è¾“å…¥çš„shapeæ¯”è¾ƒå¤§ï¼Œæ¯ä¸ªçº¿ç¨‹åªè®¿é—®4ä¸ªvecï¼Œå³<code>.x</code>ã€<code>.y</code>ã€<code>.z</code>ã€<code>.w</code>è¿™ç§ï¼Œæ‰€ä»¥<code>block.x</code>è¢«åˆ†ä¸º<code>4*vec_size</code>ä»½<ul>
<li>å…¶ä¸­ï¼Œ<code>vec_size</code>å¯¹äº<code>half</code>æ¥è¯´å–2ï¼Œå¯¹äº<code>float</code>æ¥è¯´å–1</li>
</ul>
</li>
<li>åŒæ—¶blockä¸ªæ•°ä»éœ€å¯¹é½32ï¼Œå‘ä¸Šå–æ•´</li>
<li>æ•´ä½“çœ‹æ¥å°±æ˜¯ç”¨è¾ƒå°‘çš„çº¿ç¨‹å¤„ç†æ•°æ®ï¼Œå¦‚æœè¾“å…¥shapeå¤ªå¤§å°±é‡‡ç”¨è¾“å…¥å‘é‡åŒ–ï¼ˆç›®å‰åªå®ç°äº†fp16ï¼‰å¹¶ä¸”å‡å°‘çº¿ç¨‹ä½¿ç”¨</li>
</ul>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template &lt;typename T, int NUMS_PER_THREAD_PER_ROW&gt;  
__global__ void ScaleMaskAndSoftmax_float(T *attn_score,  
                                          T *qk,  
                                          T *mask,  
                                          int batch_size,  
                                          int head_nums,  
                                          int q_len,  
                                          int k_len,  
                                          float scale){  
    int batch_id = blockIdx.y;
    int head_id = blockIdx.z; 
    if(threadIdx.x &gt;= k_len){  
        return;  
    }    
    __shared__ float s_max, inv_sum;  
    for(int row_start = 0; row_start &lt; q_len; row_start++){  
        int qk_offset = 0;  
        T qk_data = static_cast&lt;T&gt;(0);  
        T mask_data = static_cast&lt;T&gt;(0);  
        T data[NUMS_PER_THREAD_PER_ROW];  
        T thread_max = FIL_MIN;  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>åœ¨launchä¸­<ul>
<li>grid=<code>[q_length, batch_size, head_nums]</code></li>
<li>block=<code>[k_length(ä»¥32çš„å€æ•°å‘ä¸Šå–æ•´)]</code></li>
</ul>
</li>
<li>å¼€å§‹å¤„ç†æ‰€æœ‰è¡Œ</li>
</ul>
<p>ä»¥ä¸‹å…¨éƒ½åœ¨ä¸Šä¸€å±‚çš„<code>for</code>çš„å†…éƒ¨ï¼Œä¸ºä¾¿äºçœ‹ä»£ç å› æ­¤å¿½ç•¥éƒ¨åˆ†ç¼©è¿›<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">for (int col_start = 0; col_start &lt; NUMS_PER_THREAD_PER_ROW; col_start++){ 
	// æ¯ä¸ªçº¿ç¨‹åªéœ€è¦å¤„ç†NUMS_PER_THREAD_PER_ROWä¸ªæ•°æ®  
	qk_offset = batch_id * head_nums * q_len * k_len + 
			    head_id * q_len * k_len + row_start * k_len + 
			    col_start * blockDim.x + threadIdx.x;  
	qk_data = qk[qk_offset];  
	mask_offset = batch_id * q_len * k_len + head_id * q_len * k_len 
				  + row_start * k_len + col_start * blockDim.x 
				  + threadIdx.x;  
    mask_data = mask[mask_offset];  
  
    data[col_start] = scale * qk_data + (1 - mask_data) * -1e4f;  
    thread_max = fmax(data[col_start], thread_max); // ä¸€ä¸ªçº¿ç¨‹å¯¹å¤šä¸ªå…ƒç´ åšå¤„ç†ä¹‹åï¼Œå¤šä¸ªå…ƒç´ çš„æœ€å¤§å€¼  
}  
T max_val = blockReduce&lt;MaxOp, T&gt;(thread_max); // ä¸€è¡Œçš„æœ€å¤§å€¼  
// blockçš„æœ€å¤§å€¼å­˜åœ¨idä¸º0çš„çº¿ç¨‹ä¸­  
if(threadIdx.x == 0){  
    s_max = max_val;  
}        
__syncthreads();  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li>åˆ—è¢«åˆ†ä¸º<code>NUMS_PER_THREAD_PER_ROW</code>ä¸ªæ•°æ®ç”±åŒä¸€ä¸ªçº¿ç¨‹å¤„ç†</li>
<li>æ¯éå†ä¸€æ¬¡<code>col_start</code>å°±ä¼šæœ‰ç›¸åº”çš„çº¿ç¨‹å¹¶è¡Œï¼Œä¹‹åå†ç”¨<code>blockReduce</code>è¿›è¡Œæœ€åçš„è§„çº¦</li>
<li><code>mask_data</code>å’Œ<code>qk_data</code>ä¸åŒçš„åœ°æ–¹æ˜¯æ²¡æœ‰<code>head_nums</code>ï¼Œå…¶ä»–éƒ½ä¸€è‡´<ul>
<li>å¦‚æœ<code>mask_data=1</code>ï¼Œè¯´æ˜ä¸éœ€è¦è¢«maskï¼Œåä¹‹éœ€è¦è¢«mask(åŠ ä¸Š$-10^4$ï¼Œè¿™ä½¿å¾—åœ¨softmaxæ—¶å¾—åˆ°çš„å€¼éå¸¸çš„å°)<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/8616eaf5c62443e35305f7cfd27a345.jpg" alt=""><br>è€ƒè™‘åˆ°æ•°å€¼èŒƒå›´çš„æº¢å‡ºé—®é¢˜ï¼Œä¸€èˆ¬ä¼šåœ¨æŒ‡æ•°éƒ¨åˆ†å‡å»<code>D=max(zi)</code><br>softmaxçš„å…¬å¼ä¸ºï¼š$D=max(z_i),softmax(z_i)=\dfrac{e^{z_i-D}}{\sum^C_{c=1}e^{z_c-D}}$<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">T thread_sum = 0.0f;  
for(int col_start = 0; col_start &lt; NUMS_PER_THREAD_PER_ROW; col_start++){  
    data[col_start] = expf(data[col_start] - s_max);  
    thread_sum += data[col_start];  
}        
T sum_val = blockReduce&lt;SumOp, T&gt;(thread_sum);  
if(threadIdx.x == 0){  
    inv_sum = 1 / (sum_val + 1e-6);  
}       
__syncthreads();  
for(int col_start = 0; col_start &lt; NUMS_PER_THREAD_PER_ROWl;col_start++) {  
	qk_offset = batch_id * head_nums * q_len * k_len + head_id * q_len * 
				k_len + row_start * k_len + col_start * blockDim.x + 
				threadIdx.x;  
attn_score[qk_offset] = (data[col_start] * inv_sum);  
}  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ul>
</li>
</ul>
<p>å¯¹äºfp16ï¼Œä¸åŒçš„åœ°æ–¹åœ¨äºå‘é‡åŒ–å¤„ç†<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">//scalar_cast_vec: å°†å¸¸é‡è½¬æ¢ä¸º2ä¸ªæˆ–4ä¸ªå‘é‡  
Vec_t ONE = scalar_cast_vec&lt;Vec_t&gt;(__float2half(1.0f));  
Vec_t NEG_INF = scalar_cast_vec&lt;Vec_t&gt;(__float2half(-10000.0f));  
Vec_t scale_vec = scalar_cast_vec&lt;Vec_t&gt;(__float2half(scale));<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><br>æ ¹æ®<code>src/utils/vectorze_utils.h</code>ï¼šhalf-&gt;half2 ï¼Œfloat-&gt;float4<p></p>
<p>åœ¨<code>src/utils/vectorize_utils.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T_OUT, typename T_IN&gt;  
inline __decvice__ T_OUT scalar_cast_vec(T_IN val){  
    return val;  
}  
// halfè½¬ä¸ºhalf2  
template&lt;&gt;  
inline __device__ half2 scaler_cast_vec&lt;half2, half&gt;(half val){  
    return __half2half2(val);  
}  
// floatè½¬ä¸ºfloat2  
template&lt;&gt;  
inline __device__ float2 scaler_cast_vec&lt;float2, float&gt;(float val){  
	return __make_float2(val, val);  
}  
// floatè½¬ä¸ºfloat4  
template&lt;&gt;  
inline __device__ float4 scaler_cast_vec&lt;float4, float&gt;(float val){  
    return __make_float4(val, val, val, val);  
}  
// floatè½¬ä¸ºhalf2  
template&lt;&gt;  
inline __device__ float2 scaler_cast_vec&lt;half2, float&gt;(float val){  
    return __float2half2_rn(val);  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>è¿˜æœ‰ä¸€éƒ¨åˆ†ç›´æ¥ç”¨åº“ä¸­half2å‡½æ•°è¿›è¡Œè®¡ç®—å¤„ç†<p></p>
<h1 id="Lesson12-Fused-transpose-amp-remove-padding"><a href="#Lesson12-Fused-transpose-amp-remove-padding" class="headerlink" title="Lesson12 Fused transpose&amp;remove padding"></a>Lesson12 Fused transpose&amp;remove padding</h1><p>è®²è§£äº†ï¼š<br><code>src/kernels/fused_transpose_and_remv_pad.cu</code><br><code>src/kernels/fused_transpose_and_remv_pad.h</code></p>
<p><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/1ebaf4bf0c8c8e05cdc563c83f71a77.jpg" alt=""><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template &lt;typename T&gt;  
__global__ void fused_transpose_reshape_remv_pad(T *src,  
                                                 T *dst,  
                                                 const int num_tokens,  
                                                 const int batch_size,  
                                                 const int seq_len,  
                                                 const int head_num,  
                                                 const int head_size,  
                                                 const int *padding_offset /*for remove padding*/)  
{  
    int token_id = blockIdx.x; // è¿™é‡Œçš„token_idæ˜¯æŒ‡paddingä¹‹å‰çš„æ¯ä¸ªtokençš„id  
    int batch_id = token_id + padding_offset[token_id] / seq_len; // è¿™é‡Œçš„batch_idæ˜¯æŒ‡paddingä¹‹åæ¯ä¸ªtokenå¯¹åº”çš„batchçš„id  
    int seq_id = token_id + padding_offset[token_id] % seq_len;   // æ¯ä¸ªtokenåœ¨å¥å­ä¸­çš„ç¼–å·ï¼ŒèŒƒå›´æ˜¯0~seq_len-1  
    // transposeå‰åçš„offset  
    int src_offset = batch_id * head_num * seq_len * head_size + seq_id * head_size; // transposeå‰çš„åç§»ä½ç½®ï¼Œå…·ä½“åˆ°head_sizeçš„åç§»ï¼Œè¿™é‡ŒæŠŠhead_id * seq_len * head_sizeå»æ‰äº†ï¼Œä¼šåœ¨forå¾ªç¯è¡¥ä¸Š  
    int dst_offset = token_id * head_num * head_size; // è¿™é‡Œçš„åç§»åªå…·ä½“åˆ°token  
  
    for(int i = threadIdx.x; i &lt; head_num * head_size; i+=blockDim.x){ // å› ä¸ºæ¯ä¸ªblockå¤„ç†ä¸€ä¸ªtokenï¼Œæ‰€ä»¥i+=blockDim.x  
        int head_id = i / head_size;  
        int head_size_id = i % head_size;  
        dst[dst_offset + i] = src[src_offset + i * seq_len * head_size + head_size_id];  
    }
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>ä»£ç æ¯”è¾ƒå®¹æ˜“ç†è§£ï¼Œä¸æ‡‚çš„çœ‹æ³¨é‡Šå³å¯<p></p>
<h1 id="Lesson13-Fused-addResidualNorm"><a href="#Lesson13-Fused-addResidualNorm" class="headerlink" title="Lesson13 Fused addResidualNorm"></a>Lesson13 Fused addResidualNorm</h1><p>è®²è§£äº†ï¼š<br><code>src/fused_addresidual_norm.cu</code><br><code>src/fused_addresidual_norm.h</code></p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
__global__ void FusedAddBiasResidualRMSNorm( // residual.shape = [num tokens, hidden_units]  
                    T* residual,    // [num tokens, hidden_units]  
                    T* decoder_in,  // [num tokens, hidden_units]  
                    /*optional*/const T* bias,  // [hidden_units]  
                    const T* scale, // [hidden_units], RMSNorm weights  
                    float eps,      // RMSNorm eps  
                    int num_tokens,   
                    int hidden_units){  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>rmsnorm(decoder_in + residual + bias)</p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">// grid:[num_tokens] block:[num_threads]    int vec_size = Vec&lt;T&gt;::size;  
    using Vec_t = typename Vec&lt;T&gt;::Type;  
    int batch_id = blockIdx.x; // ä¸€ä¸ªblockè¡¨ç¤ºä¸€ä¸ªbatch  
    int tid = threadIdx.x;  
    Vec_t *de_out = reinterpret_cast&lt;Vec_t*&gt;(decoder_in + batch_id * hidden_units); 
    Vec_t *rsd = reinterpret_cast&lt;Vec_t*&gt;(residual + batch_id * hidden_units);  
    Vec_t *bia;  
    if(bias != nullptr){  
        bia = reinterpret_cast&lt;Vec_t*&gt;(bias);  
    }    Vec_t tmp;  
    T thread_sum = static_cast&lt;T&gt;(0.0f);  
    for (int i = threadIdx.x; i &lt; hidden_units / vec_size; i += blockDim.x) {  
        if(residual != nullptr){  
            // ä¸‹é¢å¯¹åº”HFä¸­çš„hidden_states = residual + hidden_states  
            de_out[i].x += rsd[i].x;  
            de_out[i].y += rsd[i].y;  
            de_out[i].z += rsd[i].z;  
            de_out[i].w += rsd[i].w;  
            // ä¸‹é¢å¯¹åº”residul = hidden_states            
            rsd[i].x = de_out[i].x;  
            rsd[i].y = de_out[i].y;  
            rsd[i].z = de_out[i].z;  
            rsd[i].w = de_out[i].w;  

        }        
        if(bias != nullptr){  
            de_out[i].x += bia[i].x;  
            de_out[i].y += bia[i].y;  
            de_out[i].z += bia[i].z;  
            de_out[i].w += bia[i].w;  
        }  
        thread_sum += de_out[i].x * de_out[i].x;  
        thread_sum += de_out[i].y * de_out[i].y;  
        thread_sum += de_out[i].z * de_out[i].z;  
        thread_sum += de_out[i].w * de_out[i].w;  
    }  
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><code>Vec_t *de_out = reinterpret_cast&lt;Vec_t*&gt;(decoder_in + batch_id * hidden_units)</code>ï¼šæ¯ä¸ªblockè¡¨ç¤ºä¸€ä¸ªtokenï¼Œæ¯ä¸ªtokençš„å¤§å°ä¸ºhidden_unitsï¼Œè¿™é‡Œè¡¨ç¤ºäº†å½“å‰tokençš„åç§»é‡ </li>
<li>åœ¨HFä¸­çš„é¡ºåº<br><code>hidden_states = residual + hidden_states</code>å¯¹åº”<code>de_out[i].x += rsd[i].x;</code><br><code>residul = hidden_states</code>å¯¹åº”<code>rsd[i].x = de_out[i].x;</code><br><code>hidden_states = self.post_attention_layernorm(hidden_states)</code>å¯¹åº”<code>de_out[idx].x = de_out[idx].x * inv_mean * s[idx].x;</code></li>
<li>æ ¹æ®å…¬å¼$\dfrac{x_iÃ—g_i}{\sqrt{\sum^iE(x_i^2)+eps}}$<ul>
<li>$x_i$å¯¹åº”åŠ äº†<code>residual</code>çš„<code>de_out[i]</code></li>
<li>$g_i$å¯¹åº”<code>s[idx]</code></li>
</ul>
</li>
</ul>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">    // æ±‚åˆ†æ¯ï¼Œä»¥1/xxxè¡¨ç¤º
    T block_sum = blockReduceSum&lt;float&gt;(thread_sum);  
    __shared__ float inv_mean;  
    if (threadIdx.x == 0) {  
        inv_mean = rsqrtf(block_sum / hidden_units + eps);  
    }    __syncthreads(); 
  
    // rmsnorm  
    Vec_t *s;  
    if(scale != nullptr) {  
        s = reinterpret_cast&lt;Vec_t *&gt;(scale);  
    }    
    for (int idx = threadIdx.x; idx &lt; hidden_units / vec_size; idx += blockDim.x) {  
        de_out[idx].x = de_out[idx].x * inv_mean * s[idx].x; 
        de_out[idx].y = de_out[idx].y * inv_mean * s[idx].y; 
        de_out[idx].z = de_out[idx].z * inv_mean * s[idx].z;  
        de_out[idx].w = de_out[idx].w * inv_mean * s[idx].w;  
    }
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h1 id="Lesson-14-Gate-Linear-amp-Up-Linear"><a href="#Lesson-14-Gate-Linear-amp-Up-Linear" class="headerlink" title="Lesson 14 Gate Linear&amp;Up Linear"></a>Lesson 14 Gate Linear&amp;Up Linear</h1><p>è®²è§£äº†ï¼š<br><code>src/kernels/linear</code></p>
<p>è¾“å…¥ï¼š<br>    ä¸ºcontext decoderæ—¶ï¼Œ<code>[batch_size, q hidden units]</code>ï¼›<br>    ä¸ºself decoderæ—¶ï¼Œ<code>[token nums, q hidden units]</code><br>Gate&amp;Upæƒé‡ï¼š<code>[q hidden units, 2 * inter size]</code><br>è¾“å‡ºï¼š<code>[batch_size(æˆ–token nums), 2 * inter size] = [bs/tn, 2, inter size]</code>ï¼Œå®é™…ä¸Šè¾“å‡ºæ˜¯ä¸‰ç»´</p>
<h1 id="Lesson-15-SwiGLU"><a href="#Lesson-15-SwiGLU" class="headerlink" title="Lesson 15 SwiGLU"></a>Lesson 15 SwiGLU</h1><p>è®²è§£äº†ï¼š<br><code>src/kernels/act_kernel.h</code><br><code>src/kernels/act_kernel.cu</code></p>
<p><code>SiLU(Sigmoid Linear Unit)</code>ï¼Œç›¸å¯¹äºReLUï¼ŒSiLUåœ¨å‡½æ•°æ¥è¿‘0æ—¶å…·æœ‰æ›´å¹³æ»‘çš„æ›²çº¿<br>$y=x*sigmoid(\beta x)=\dfrac{1}{1+e^{-\beta x}}$ï¼Œå½“$\beta=1$æ—¶å°±æ˜¯SiLU<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/Pasted%20image%2020241105112111.png" alt=""><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;
__device__ __forceinline__ T silu(const T&amp; in){
	return in / (1.0f * expf(-in));
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<p><code>grid:[batch_size=input-&gt;shape[0]]</code><br><code>block:[256]</code></p>
<p><code>Gate Linear</code>å’Œ<code>Up Linear</code>çš„è¾“å‡º(å¯¹äºcontext decoderè€Œè¨€)<code>[bs, 2, inter size]</code>å¯ä»¥è§†ä¸ºä¸¤ä¸ªå¤§å°ä¸º<code>[bs, inter size]</code>çš„éƒ¨åˆ†ï¼Œç¬¬ä¸€éƒ¨åˆ†åš<code>SiLU</code>ï¼Œå¾—åˆ°çš„ç»“æœä¸ç¬¬äºŒéƒ¨åˆ†åš<code>mul</code>æœ€ç»ˆå¾—åˆ°æœ€åçš„ç»“æœ<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
__global__ void silu_and_mul_kernel(
					T* out, // shape: [bs, intermedia size]  
					const T* input,  // shape: [bs, 2, intermedia size]  
	                const int intermedia_size) {  
    const int batch_idx = blockIdx.x;  
    for(int idx = threadIdx.x; idx &lt; intermedia_size; idx +=blockDim.x){ 
        const T x = input[batch_idx * 2 * intermedia_size + idx];// ç¬¬ä¸€ä¸ª 
        const T y = input[batch_idx * 2 * intermedia_size + intermedia_size + idx]; // ç¬¬äºŒä¸ª  
        out[batch_idx * intermedia_size + idx] = silu&lt;T&gt;(x) * y;  
    }
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<h1 id="Lesson16-Fused-SelfDecoderAttention-kernel"><a href="#Lesson16-Fused-SelfDecoderAttention-kernel" class="headerlink" title="Lesson16 Fused SelfDecoderAttention kernel"></a>Lesson16 Fused SelfDecoderAttention kernel</h1><p>è®²è§£äº†ï¼š<br><code>src/fused_decoder_self_attention.cu</code></p>
<p>èåˆéƒ¨åˆ†ï¼š<code>concat kv</code>+<code>repeat kv</code>+<code>qk gemv</code>+<code>softmax</code>+<code>qk*v gemv</code></p>
<ul>
<li>å¦‚ä½•fuseï¼šæ•°æ®åœ¨å¯„å­˜å™¨(å¦‚<code>q</code>ã€<code>k</code>å’Œ<code>v</code>)å’Œæ˜¾å­˜(å¦‚<code>q_buf</code>ã€<code>k_buf</code>å’Œ<code>v_buf</code>)éƒ½å‡ºç°ï¼Œå› æ­¤éœ€è¦å¤ç”¨åœ¨å¯„å­˜å™¨å’Œå…±äº«å†…å­˜ä¸­çš„æ•°æ®ï¼Œå› ä¸ºè®¿é—®æ˜¾å­˜ä¼šè€—æ—¶ï¼Œå¹¶ä¸”å¸¦å®½å¾ˆä½</li>
<li>ä½¿ç”¨åŠ¨æ€å…±äº«å†…å­˜</li>
<li><code>Q*k Gemv</code>ï¼š<ul>
<li><code>q.shape=[batch size, head num, 1, head size]</code><ul>
<li>è¿™é‡Œçš„<code>1</code>è¡¨ç¤ºæ¯æ¬¡é’ˆå¯¹ä¸€ä¸ªç‰¹å®šä½ç½®(å½“å‰token)è®¡ç®—attention</li>
</ul>
</li>
<li><code>k.shape=[batch size, head num, step, head size]</code>ï¼Œ<ul>
<li>è¿™é‡Œä¸æ˜¯<code>kv head num</code>ï¼Œæ˜¯å› ä¸ºåœ¨<code>repeat kv</code>è¿™ä¸€æ­¥ä¸­å·²ç»æŠŠqå’Œkçš„å¤´å¯¹é½äº†</li>
<li>è¿™é‡Œçš„<code>step</code>è¡¨ç¤ºæ¯ä¸ªå¥å­åŒ…å«<code>step</code>ä¸ª<code>token</code>ï¼Œæ¯ä¸ª<code>token</code>çš„keyéƒ½ä¸å½“å‰æŸ¥è¯¢å‘é‡<code>q</code>åšç‚¹ç§¯</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>é‡æ¸©ï¼š</p>
<ul>
<li>qkvçŸ©é˜µçš„shape<ul>
<li>q<code>[batch size, q head num, 1, head size]</code></li>
<li>k<code>[batch size, kv head num, step(/seqlen), head size]</code></li>
<li>v<code>[batch size, kv head num, step(/seqlen), head size]</code></li>
</ul>
</li>
</ul>
<p><code>launchDecoderMaskedMHA()</code></p>
<ul>
<li><code>qkv_buf</code>ï¼š<code>[batch size, qkv head num, head size]</code>ï¼Œé»˜è®¤<code>head_num</code>æ˜¯qçš„headï¼Œqkvã€kvçš„headä¼šåŠ ä¸Šç›¸åº”çš„å‰ç¼€</li>
<li>ç”¨<code>getVal</code>çš„å‰ææ˜¯æ•°æ®å¿…é¡»åœ¨CPUä¸Š(<code>LLM_CHECK(location == CPU)</code>)</li>
<li>gridï¼š<code>[head_num, batch_size]</code></li>
<li>blockï¼š<code>[head_size]</code></li>
</ul>
<p>å…¥å‚ï¼š<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
void launchDecoderMaskedMHA(TensorWrapper&lt;T&gt;* qkv_buf,
                            BaseWeight&lt;T&gt;&amp; qkv, 
                            TensorWrapper&lt;int&gt;* layer_id,  
                            TensorWrapper&lt;T&gt;* k_cache,  
                            TensorWrapper&lt;T&gt;* v_cache,  
                            TensorWrapper&lt;bool&gt;* finished, 
                            TensorWrapper&lt;int&gt;* step, 
                            TensorWrapper&lt;T&gt;* mha_output,  
                            LLaMAAttentionStaticParams&amp; static_params){ <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li><code>qkv_buf=qkv_linear=[bs, q_hidden_units] * [qhiddenunits, hiddenunits] = [bs, qkv_head_num, head_size]</code><ul>
<li><code>qhiddenunits</code>ï¼šå°†è¾“å…¥çš„åµŒå…¥å‘é‡(embedding vector)çš„å‘é‡é•¿åº¦ï¼Œ</li>
<li><code>hiddenunits</code>ï¼š<code>=[qkv_head_num,qiddenunist]=[qkv_head_num,head_size]</code><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/22bbf7d7781de208731d147b46c3b5e%201.jpg" alt="|550"></li>
</ul>
</li>
<li>kvçš„cache<ul>
<li>k_cache<code>[num layers, bs, kv head num, max seq len or step, head size]</code></li>
<li>v_cache<code>[num layers, bs, kv head num, max seq len or step, head size]</code><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">    const int qkv_head_num = qkv_buf-&gt;shape[1];  
    const int kv_head_num = k_cache-&gt;shape[2];  
    const int max_seq_len = k_cache-&gt;shape[3];   
int head_num = qkv_head_num - 2 * kv_head_num;  
    const int head_size = qkv_buf-&gt;shape[2];  
    const int cur_step = step-&gt;getVal();
    const int layer = layer_id-&gt;getVal();  
    const int layer_offset = layer * max_seq_len * batch_size * kv_head_num * head_size;  
    size_t smem_size_bytes = head_size * sizeof(T) + cur_step * sizeof(float);  
    T* qkv_data = qkv_buf-&gt;data;  
    T* q = qkv_data;
    T* k = qkv_data + head_num * head_size;  
    T* v = qkv_data + (head_num + kv_head_num) * head_size;  
  
    int   rotary_embedding_dim = static_params.rotary_embedding_dim;  
    float rotary_embedding_base = static_params.rotary_embedding_base;  
    int   max_position_embeddings = static_params.max_position_embeddings;  
    bool  use_dynamic_ntk = static_params.use_dynamic_ntk;  
    dim3 grid(head_num, batch_size);  
    dim3 block(head_size); //vec size = 4 for fp32  
    masked_MHA_kernel&lt;T&gt;&lt;&lt;&lt;grid, block, smem_size_bytes&gt;&gt;&gt;(  
										q,  
										k,  
										v,  
										// /*(T*)*/qkv.bias,  
										k_cache-&gt;data + layer_offset,  
										v_cache-&gt;data + layer_offset,  
										mha_output-&gt;data,  
										batch_size,  
										head_num,  
										kv_head_num,  
										max_seq_len,  
										head_size,  
										cur_step,  
										rotary_embedding_dim,  
										rotary_embedding_base);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
</ul>
</li>
<li><code>qã€kã€v</code>ï¼š<code>qkv_buf=[bs, qkv_head_num, head_size]</code>ï¼Œ<code>qã€kã€v</code>åˆ†åˆ«åŠ ä¸Šç›¸åº”åç§»é‡</li>
<li><code>k_cacheã€v_cache</code>ï¼šå®šä½åˆ°æŸä¸€ä¸ª<code>layer</code>ä¸Šï¼Œä¸è€ƒè™‘<code>layer</code>æ—¶çš„<code>shape</code>ä¸º<code>[bs, kv head num, max seq len or step, head size]</code></li>
<li><code>mha_output-&gt;data</code>ï¼šä½œä¸ºè¾“å‡ºåœ°å€</li>
<li><code>cur_step</code>ï¼šå½“å‰æ—¶é—´æ­¥ï¼Œå½“å‰ç”Ÿæˆåˆ°ç¬¬å‡ ä¸ªtoken</li>
<li><code>rotary_embedding_dimã€rotary_embedding_base</code>ï¼šRoPEç”¨</li>
</ul>
<p><code>masked_MHA_kernel()</code><br>å…¥å‚ï¼š<br></p><pre class="line-numbers language-c" data-language="c"><code class="language-c">template<span class="token operator">&lt;</span>typename T<span class="token operator">&gt;</span>  
__global__ <span class="token keyword">void</span> <span class="token function">masked_MHA_kernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> T<span class="token operator">*</span> q<span class="token punctuation">,</span>  
                    <span class="token keyword">const</span> T<span class="token operator">*</span> k<span class="token punctuation">,</span>  
                    <span class="token keyword">const</span> T<span class="token operator">*</span> v<span class="token punctuation">,</span>  
                    T<span class="token operator">*</span> qkv_bias<span class="token punctuation">,</span>  
                    T<span class="token operator">*</span> k_cache<span class="token punctuation">,</span>  
                    T<span class="token operator">*</span> v_cache<span class="token punctuation">,</span>  
                    T<span class="token operator">*</span> mha_output<span class="token punctuation">,</span>  
                    <span class="token keyword">const</span> <span class="token keyword">int</span> batch_size<span class="token punctuation">,</span>  
                    <span class="token keyword">const</span> <span class="token keyword">int</span> head_num<span class="token punctuation">,</span>  
                    <span class="token keyword">const</span> <span class="token keyword">int</span> kv_head_num<span class="token punctuation">,</span>  
                    <span class="token keyword">const</span> <span class="token keyword">int</span> max_seq_len<span class="token punctuation">,</span>  
                    <span class="token keyword">const</span> <span class="token keyword">int</span> head_size<span class="token punctuation">,</span>  
                    <span class="token keyword">const</span> <span class="token keyword">int</span> step<span class="token punctuation">,</span>  
                    <span class="token keyword">int</span>   rotary_embedding_dim<span class="token punctuation">,</span>  
                    <span class="token keyword">float</span> rotary_embedding_base<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token comment">// rsqrt(dh)  </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li><code>k_offset</code>å’Œ<code>cache_offset</code>åŒºåˆ«ï¼š<ul>
<li><code>k_offset</code>æ˜¯qkv linearæä¾›ç»™kçš„ï¼Œ(å› ä¸ºæ˜¯self_attentionæ‰€ä»¥)ä¸€ä¸ªbatchåªæœ‰ä¸€ä¸ªtoken</li>
<li><code>cache_offset</code>æ˜¯kv cacheæä¾›ç»™kçš„ï¼Œæœ‰<code>max seq len</code>ï¼Œä¸€ä¸ªbatchæœ€å¤šæœ‰max seq lenä¸ªtoken(æœ‰è¿™ä¹ˆå¤šæ˜¯å› ä¸ºæ–°ç”Ÿæˆçš„tokençš„kã€vä¹ŸåŠ ä¸Šå»äº†)</li>
</ul>
</li>
<li>ä»¥<code>tid * vec_size &lt; head_size</code>ä½œä¸ºæ˜¯å¦è¶…å‡ºè¾¹ç•Œçš„åˆ¤æ–­<ul>
<li><code>head_size</code>ä¸€èˆ¬æ˜¯4ã€8ã€16çš„å€æ•°ï¼Œæ‰€ä»¥å½“<code>vec_size</code>ä¸º2æˆ–4æ—¶ä¹Ÿèƒ½æ­£å¸¸åˆ¤æ–­</li>
<li>(æŠ›å¼€å€æ•°é—®é¢˜ä¼šè§‰å¾—ä¸èƒ½æ­£å¸¸åˆ¤æ–­çš„åŸå› æ˜¯ï¼š<code>head_size=7</code>ï¼Œå½“<code>tid(=1)*vec_size(=4)</code>æ—¶ï¼Œ<code>4&lt;7</code>æ­¤æ—¶åˆ¤æ–­æœªè¶…å‡ºè¾¹ç•Œï¼Œä½†æ˜¯ä¸€å…±æœ‰<code>2Ã—4=8</code>å·²ç»è¶…å‡ºè¾¹ç•Œäº†)</li>
</ul>
</li>
<li>è¾“å‡ºï¼š`mha_output.shape=[batch_size, q_head_num, 1, head_size]</li>
</ul>
<p>â‘ ConcatPastKVCache<br><code>input=[bs, kv head num, seqlen, head size]</code><br><code>output=[bs, kv head num, max_seq_len, head size]</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int tid = threadIdx.x;  
   int q_head_id = blockIdx.x;  
   int q_batch_id = blockIdx.y;  
   int kv_head_id = q_head_id / (head_num / kv_head_num);  
   int kv_batch_id = q_batch_id;  
   
   int batch_stride = head_num * head_size;  
   int kv_batch_stride = kv_head_num * head_size;  
   int head_stride = head_size;  
 
   int q_offset = q_batch_id * batch_size + q_head_id * head_stride + tid;  
   // k_offsetæ˜¯qkv linearæä¾›ç»™kçš„  
   int k_offset = kv_batch_id * kv_batch_stride + kv_head_id * head_stride + tid;
   // cache_offsetæ˜¯kv cacheæä¾›ç»™kçš„  
   int cache_offset = kv_batch_id*kv_head_num*max_seq_len*head_size 
				 + kv_head_id * max_seq_len * head_size 
				 + tid * vec_size;//æ²¡æœ‰seq lençš„ç»´åº¦æ˜¯å› ä¸ºseq lenå§‹ç»ˆä¸º1  
   int step_stride = head_size;  
 
   float scale = rsqrt((float)head_size);  
 
   int vec_size = Vec&lt;T&gt;::size;  
   int q_offset_vec = q_batch_id * batch_size + q_head_id * head_stride + tid * vec_size;  
   int k_offset_vec = kv_batch_id * kv_batch_stride + kv_head_id * head_stride + tid * vec_size;  
   using Vec_t = typename Vec&lt;T&gt;::Type; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<p>â‘¡å£°æ˜åŠ¨æ€å…±äº«å†…å­˜å˜é‡<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">const T* q_mem = q;  
const T* k_mem = k;  
const T* v_mem = v;  
if(tid * vec_size &lt; head_size){  
    qvec = *reinterpret_cast&lt;Vec_t*&gt;(const_cast&lt;T*&gt;(&amp;q_mem[q_offset_vec]));  
    kvec = *reinterpret_cast&lt;Vec_t*&gt;(const_cast&lt;T*&gt;(&amp;k_mem[k_offset_vec]));  
    vvec = *reinterpret_cast&lt;Vec_t*&gt;(const_cast&lt;T*&gt;(&amp;v_mem[v_offset_vec]));  
}  
extern __shared__ char sqk[]; // å£°æ˜åŠ¨æ€å…±äº«å†…å­˜å˜é‡  
// shared memoryçš„åˆ†é…  
// å­˜åˆ°shared memoryä¸­çš„æ•°æ®çš„ç‰¹ç‚¹æ˜¯ä½å»¶è¿Ÿã€é«˜å¤ç”¨  
// åœ¨è¿™é‡Œå¯¹qç”¨shared memoryè¿›è¡Œå­˜å‚¨æ˜¯å› ä¸ºä¹‹åæœ‰ä¸ªä¼˜åŒ–ï¼Œä½¿ç”¨ä¸€ä¸ªblockå–å¤šè¡Œkè¿›è¡Œqk gemmï¼Œæ­¤æ—¶qçš„å¤ç”¨é¢‘ç‡å˜é«˜ï¼Œä¸éœ€è¦é‡å¤åŠ è½½q  
T* sq_scalar = reinterpret_cast&lt;T*&gt;(sqk);  
float* logits = reinterpret_cast&lt;float*&gt;(sq_scalar + head_size);  
Vec_t *sq = reinterpret_cast&lt;Vec_t*&gt;(sq_scalar);  
  
if(tid * vec_size &lt; head_size){  
    sq[tid] = qvec;  
}    __syncthreads();  
float zero = 0.0f;  
Vec_t zero_f4 = scalar_cast_vec&lt;Vec_t, T&gt;(zero); // å°†floatè½¬ä¸ºfloat4  
float4 scale_f4 = scalar_cast_vec&lt;float4, float&gt;(scale);  
  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">// q*k gemv  
for(int iter = 0; iter &lt; step; iter++){ //ä¸€ä¸ªblockå¾ªç¯è®¡ç®—stepè¡Œ  
    Vec_t kvec_qk = tid * vec_size &lt; head_size ? *reinterpret_cast&lt;Vec_t*&gt;(&amp;k_cache[iter * step_stride + cache_offset]) : zero_f4; // è¿™é‡Œä¹˜iterç›¸å½“äºä¹˜max seq lenã€‚æˆ‘çš„ç†è§£æ˜¯cache_offsetæ˜¯å¯¹äºtokenè€Œè¨€çš„ï¼Œiter*cache_offsetçš„åç§»ä½¿å®šä½åˆ°å½“å‰step(å½“å‰token)  
  
    if(iter == step - 1 &amp;&amp; tid * vec_size &lt; head_size){ // stepçš„æœ€åä¸€ä¸ªä½ç½®å­˜å‚¨RoPEè¾“å‡ºçš„k  
        *reinterpret_cast&lt;Vec_t*&gt;(&amp;k_cache[iter * step_stride + cache_offset]) = kvec;  
        kvec_qk = kvec; // è¿™é‡Œçš„kvec_qkæ˜¯ç”¨æ¥åšè®¡ç®—çš„ï¼Œä¸‹é¢çš„vvec_qkcåŒç†  
    }  
  
    Vec_t qk = zero_f4;  
    qk.x = tid * vec_size &lt; head_size ? sq[tid].x * kvec_qk.x * scale_f4.x : zero;  
    qk.y = tid * vec_size &lt; head_size ? sq[tid].y * kvec_qk.y * scale_f4.y : zero;  
    qk.z = tid * vec_size &lt; head_size ? sq[tid].z * kvec_qk.z * scale_f4.z : zero;  
    qk.w = tid * vec_size &lt; head_size ? sq[tid].w * kvec_qk.w * scale_f4.w : zero;  
  
    T qk_acc = qk.x + qk.y + qk.z + qk.w; // ä¸€ä¸ªçº¿ç¨‹æœ‰4ä¸ªå€¼ï¼Œå…ˆåœ¨çº¿ç¨‹å±€éƒ¨æŠŠè¿™å››ä¸ªå€¼åŠ èµ·æ¥ï¼Œå†ç”¨blockReduceSum  
    T attn_score = blockReduceSum&lt;T&gt;(qk_acc);  
    if(tid == 0){  
        logits[iter] = attn_score; // logitsæ˜¯stepÃ—1å¤§å°çš„æ•°ç»„  
    }  
    __syncthreads();  
}  
// softmax    T local_logits = tid &lt; step ? (T)logits[tid] : 0;  
__shared__ float row_max, fenmu;  
T block_max = blockReduceMax&lt;T&gt;(local_logits);  
if(tid == 0){  
    row_max = block_max;  
}    __syncthreads();  
T fenzi = tid &lt; step ? expf(logits[tid] - row_max) : 0; // e(x_i - x-max) / sigma(e(x_i, x_max));  
T block_fenmu = blockReduceSum&lt;T&gt;(fenzi);  
if(tid == 0){  
    fenmu = block_fenmu + 1e-6;  
}    __syncthreads();  
if(tid &lt; step){  
    logits[tid] = (T)(fenzi / fenmu);  
}    __syncthreads();  
  
// éšå¼çš„repeat kvï¼Œéƒ½æ˜¯å‘é‡åŒ–ç±»å‹  
if(tid * vec_size &lt; head_size){  
    Vec_t O = scalar_cast_vec&lt;Vec_t, T&gt;(0.0f); // ä¸­é—´å¯„å­˜å™¨  
    for(int iter = 0; iter &lt; step; iter++){  
        Vec_t vvec_qkv = *reinterpret_cast&lt;Vec_t*&gt;(&amp;v_cache[iter * step_stride + cache_offset]);  
  
        if(iter == step - 1){ // stepçš„æœ€åä¸€ä¸ªä½ç½®å­˜å‚¨RoPEè¾“å‡ºçš„k  
            *reinterpret_cast&lt;Vec_t*&gt;(&amp;v_cache[iter * step_stride + cache_offset]) = vvec;  
            vvec_qkv = vvec;  
        }            __syncthreads();  
        O.x += vvec_qkv.x * logits[iter]; // vçš„ä¸€æ•´è¡ŒÃ—qkçš„ä¸€ä¸ª  
        O.y += vvec_qkv.y * logits[iter]; // vçš„ä¸€æ•´è¡ŒÃ—qkçš„ä¸€ä¸ª  
        O.z += vvec_qkv.z * logits[iter]; // vçš„ä¸€æ•´è¡ŒÃ—qkçš„ä¸€ä¸ª  
        O.w += vvec_qkv.w * logits[iter]; // vçš„ä¸€æ•´è¡ŒÃ—qkçš„ä¸€ä¸ª  
    }  
    *reinterpret_cast&lt;Vec_t*&gt;(&amp;mha_output[q_offset]) = O; // [batch size, q head num, 1, head size]  
}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h1 id="Lesson17-topK"><a href="#Lesson17-topK" class="headerlink" title="Lesson17 topK"></a>Lesson17 topK</h1><p>è®²è§£äº†ï¼š<br><code>src/kernels/topK.cu</code><br><code>src/kernels/topK.h</code></p>
<p>è¾“å…¥ï¼š<code>[bs, beam_width, vocab size]</code><br>è¾“å‡ºï¼š<code>[bs, beam_width, K]</code></p>
<p>topKä¸­çš„Kæ˜¯ä»ä¸€ç»„å€™é€‰ä¸­é€‰å–å¾—åˆ†æœ€é«˜çš„å‰Kä¸ªå€¼<br>beam_widthæ˜¯æŒ‡ä¿ç•™çš„å€™é€‰è·¯å¾„æ•°<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/Pasted%20image%2020241117225342.png" alt="|450"></p>
<p>ç›®çš„ï¼šæ¯ä¸ªvocabéœ€è¦é€‰æ‹©Kä¸ªå€¼ä½œä¸ºtopK<br>åšæ³•ï¼šç”±äº<code>vocab_size</code>æ¯”è¾ƒå¤§ï¼Œå› æ­¤åˆ†æˆä¸¤æ¬¡topK</p>
<ul>
<li>ç¬¬ä¸€æ¬¡ï¼š<code>[bs, beamwidth, vocab size] =&gt; [bs, beamwidth, BlockPerBeam, K]</code><ul>
<li>å°†vocabåˆ†ä¸º<code>BlockPerBeam</code>æ®µï¼Œæ¯æ®µåštopKé€‰å‡ºå‰<code>K</code>ä¸ªæœ€å¤§çš„å€¼</li>
<li>ç¬¬ä¸€æ¬¡topKåæ¯ä¸ªvocabè¿˜æœ‰<code>BlockPerBeam * K</code>ä¸ªå€¼</li>
<li>gridï¼š<code>[min(batch_size * BlockPerBeam, maxBlockNums)]</code></li>
<li>blockï¼š<code>[256]</code></li>
</ul>
</li>
<li>ç¬¬äºŒæ¬¡ï¼š<code>[bs, beamwidth, BlockPerBeam, K] =&gt; [bs, beamwidth, K]</code><ul>
<li>å°†vocabå‰©ä¸‹çš„<code>BlockPerBeam * K</code>ä¸ªå€¼ç›´æ¥åštopKå¾—åˆ°Kä¸ªå€¼</li>
<li>gridï¼š<code>[min(batch_size, maxBlockNums)]</code></li>
<li>blockï¼š<code>[256]</code></li>
</ul>
</li>
</ul>
<p>â‘ topKçš„åšæ³•<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T, int K&gt;  
struct topK{  
    // ä¸‹é¢è¿™ä¸¤è¡Œçš„è®¿é—®æƒé™æ˜¯publicï¼Œå› ä¸ºé»˜è®¤å°±æ˜¯publicæ‰€ä»¥ä¸ç”¨æ˜¾å¼åœ°å†™å‡ºæ¥  
    T val[K];  
    int id[L];  
	// åˆå§‹åŒ–topKä¸­idå…¨ä¸º-1ï¼Œvalå…¨ä¸ºæœ€å°å€¼
    __device__ void init(){  
        for(int i = 0; i &lt; K; i++){  
            id[i] = -1; 
            val[i] = FLT_MIN;  
        }    
    }    
    // å¦‚æœå½“å‰è¾“å…¥çš„æ•°å­—æ¯”æœ€åä¸€ä¸ªæ•°å­—å¤§ï¼Œåˆ™æ‘’å¼ƒæœ€åä¸€ä¸ªæ•°å­—ï¼Œå°†è¾“å…¥çš„æ•°å­—æ’è¿›æ¥
    void insertHeap(T data, int data_id){  
		if(id[K-1] == -1 || val[K-1] &lt; data){  
			id[K-1] = data_id;  
			val[K-1] = data;  
		}        
        // åªéœ€è¦å¯¹å½“å‰è¾“å…¥è¿›æ¥çš„åšå†’æ³¡æ’åºï¼Œå› ä¸ºæ¯è¿›æ¥ä¸€ä¸ªéƒ½åšä¸€æ¬¡å†’æ³¡æ’åº
        for(int i = K-2; i &gt;= 0; i--){  
            if(val[i + 1] &gt; val[i]){  
                T tmp = val[i];  
                val[i] = val[i + 1];  
                val[i + 1] = tmp;  
                int tmp_id = id[i];  
                id[i] = id[i + 1];  
                id[i + 1] = tmp_id;  
            }        
        }    
    }
};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<p>â‘¡å°†ä¸¤ä¸ªtopKåšä¸€æ¬¡reduceè¾“å‡ºä¸ºä¸€ä¸ªtopK<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T, int K&gt;  
__device__ topK&lt;T, K&gt; reduce_functor(const topK&lt;T, K&gt;&amp; a, const topK&lt;T, K&gt;&amp; b) {  
    topK&lt;T, K&gt; res = a;  
    for(int i = 0; i &lt; K; i++){  
        res.insertHeap(b.val[i], b.id[i]);  
    }    
    return res;  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<p>â‘¢ç¬¬ä¸€æ¬¡topK<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T, int K, int blockSize, int BlockPerBeam&gt;  
__global__ void topK_kernel_round1(const T* probs, 
								   const int vocab_size,   
								   int* topK_ids, 
								   T* topK_vals){  
    int tid = threadIdx.x;  
    int bid = blockIdx.x;  
    int row_id = bid / BlockPerBeam;     // å“ªä¸€æ‰¹vocab/å“ªä¸€ä¸ªbatchä¸­  
    int block_lane = bid % BlockPerBeam; // åŒä¸€æ‰¹vocabä¸­çš„å“ªä¸€ä¸ªæ®µ  
    topK&lt;T, K&gt; thread_topK; // ä¸ºæ¯ä¸€ä¸ªçº¿ç¨‹åˆ†é…ä¸€ä¸ªtopKå¯„å­˜å™¨  
    thread_topK.init();  
    // ä¸‹é¢åšthreadå±‚æ¬¡çš„reduce  
    for(int data_id = tid + block_lane * blockSize; data_id &lt; vocab_size; data_id += BlockPerBeam * blockSize){  
        int data_offset = data_id + row_id * vocab_size;  
        T data = probs[data_offset];  
        thread_topK.insertHeap(data, data_offset);  
    }    
    
    typedef cub::BlockReduce&lt;topK&lt;T, K&gt;, blockSize&gt; blockreduce; 
    __shared__ typename blockreduce::TempStorage tmp_storage;    
    topK&lt;T, K&gt; block_topk = blockreduce(tmp_storage).Reduce(thread_topK, reduce_functor&lt;T, K&gt;);  
  
    if(tid == 0){  
        for(int k_offset = 0; k_offset &lt; K; k_offset++){  
            int dst_offset = row_id * BlockPerBeam * K + 
				             block_lane * K + 
				             k_offset;  
            topK_vals[dst_offset] = block_topk.val[k_offset];  
            topK_ids[dst_offset] = block_topk.id;  
        }    
    }
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>å…¥å‚ï¼š<p></p>
<ul>
<li><code>probs</code>ï¼šè¾“å…¥çš„æ¦‚ç‡å€¼<code>[bs, beamwidth, vocab size]</code></li>
<li><code>topK_ids</code>å’Œ<code>topK_vals</code>ï¼šä½œä¸ºè¾“å‡º</li>
</ul>
<p>åœ¨æœªéœ€è¦<code>data+=BlockPerBeam*blockSize</code>æ—¶ï¼Œ</p>
<ul>
<li>æ¯ä¸ªbatchä¸­ï¼Œ<code>block_lane=0~7</code>ï¼Œ<code>tid=0~255</code></li>
<li>åœ¨ä¸åŒbatchä¸­ï¼Œ<code>row_id</code>ä¸åŒ</li>
<li><code>data_id+=BlockPerBeam*blockSize</code>å¯ä»¥ç†è§£ä¸ºå½“<code>data_id</code>æ˜¯0~2047å¹¶ä¸”<code>data_id</code>ä»æœªè¶…å‡º<code>vocab_size</code>æ—¶ï¼Œåœ¨ä¸å˜åŠ¨<code>tid</code>å’Œ<code>bid</code>å‰æä¸‹ï¼Œçº¿ç¨‹å¹¶è¡Œæ‰§è¡Œ<code>data+_id</code>åŠ ä¸Šæ­¥é•¿ä¸º<code>BlockPerBeam*blockSize</code>å¾—åˆ°çš„æ–°çš„<code>data_id</code>çš„è¡Œä¸ºã€‚ç›´åˆ°<code>data_id</code>è¶…è¿‡<code>vocab_size</code>ä¸ºæ­¢</li>
<li><code>data_id</code>å¯ä»¥ç†è§£ä¸ºåœ¨æŸä¸€vocabä¸­çš„åç§»é‡ï¼ŒåŠ ä¸Š<code>row_id</code>å…³äºbatchçš„åç§»å¾—åˆ°æœ€ç»ˆçš„åç§»é‡<code>data_offset</code></li>
<li><code>thread_topK</code>ï¼šæ˜¯æ¯ä¸ªçº¿ç¨‹éƒ½æœ‰è‡ªå·±çš„topK<ul>
<li><code>bid=0, tid=0</code>ï¼šè´Ÿè´£<code>data_id</code>ä¸º0ã€2048ã€4096çš„topK</li>
<li><code>bid=7, tid=1</code>ï¼šè´Ÿè´£<code>data_ia</code>ä¸º1793ã€2561ã€4609çš„topK</li>
</ul>
</li>
</ul>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">typedef cub::BlockReduce&lt;topK&lt;T, K&gt;, blockSize&gt; blockreduce; 
   __shared__ typename blockreduce::TempStorage tmp_storage;    
   topK&lt;T, K&gt; block_topk = blockreduce(tmp_storage).Reduce(thread_topK, reduce_functor&lt;T, K&gt;);  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<ul>
<li><code>cub::BlockReduce</code>æ˜¯NVIDIAæä¾›çš„CUB(CUDA UnBound)åº“ä¸­çš„ä¸€ä¸ªæ¨¡æ¿ç±»ï¼Œç›®çš„æ˜¯å°†çº¿ç¨‹å—ä¸­çš„æ•°æ®(ç”±æ¯ä¸ªçº¿ç¨‹è´Ÿè´£ä¸€éƒ¨åˆ†)è§„çº¦ä¸ºå•ä¸€ç»“æœ<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template &lt;typename T, int BLOCK_DIM&gt; 
class cub::BlockReduce { 
public: 
	using TempStorage = typename ImplementationDefined; 
	BlockReduce(TempStorage&amp; temp_storage); 
	T Reduce(T input, ReduceOp reduce_op);
};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><code>tmp_storage</code>ï¼šä¾›çº¿ç¨‹å—ä¸­çš„çº¿ç¨‹é€šä¿¡å’Œå½’çº¦ä½¿ç”¨</li>
<li><code>block_topk</code>ï¼šåˆå¹¶æ¯ä¸ªçº¿ç¨‹å—ä¸­çš„çº¿ç¨‹çš„topKï¼Œå¾—åˆ°æ¯ä¸ªçº¿ç¨‹å—çš„topK</li>
</ul>
<p>æœ€åæ¯ä¸ªblockåªä½¿ç”¨ç¬¬ä¸€ä¸ªçº¿ç¨‹åšè½¬ç§»ï¼Œå°†block_topkä¸ªæ•°æ®è½¬ç§»åˆ°topK_valså’ŒtopK_idsä¸­ã€‚<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/7745326675b006f165b0b23a6397ba1.jpg" alt=""><br>â‘£ç¬¬äºŒæ¬¡topK<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T, int beam_width, int K, int blockSize, int BlockPerBeam&gt;  
__global__ void topK_kernel_round2(const int* topK_ids, 
								   const T* topK_vals,  
								   int* final_topK_ids, 
								   T* final_topK_vals){  
    int tid = threadIdx.x;  
    int bid = blockIdx.x;  
    int row_id = bid; // æ”¹åŠ¨1ï¼šæ¯ä¸ªbatchåªç”¨ä¸€ä¸ªblockè¡¨ç¤ºï¼ŒåŒæ—¶æ²¡æœ‰block_lane
    topK&lt;T, K&gt; thread_topK;  
    thread_topK.init();  
    // ä¸‹é¢åšthreadå±‚æ¬¡çš„reduce  
    for(int data_id = tid; data_id &lt; beam_width * BlockPerBeam * K; data_id += blockSize){ // æ”¹åŠ¨2ï¼šdata_idçš„åˆå§‹ä¸ç”¨è€ƒè™‘è¯¥batchçš„ç¬¬å‡ ä¸ªblockï¼Œæ­¥é•¿ä¸ºblockSize
        int data_offset = data_id + bid * beam_width * BlockPerBeam * K; // æ”¹åŠ¨3ï¼šbatchå†…çš„åç§»ç¡®å®šåï¼Œdata_offsetåœ¨æ¯ä¸ªbatchä¹‹é—´çš„åç§»å°±æ˜¯beam_width*BlockPerBeam*K 
		thread_topK.insertHeap(topK_vals[data_offset], 
							   topK_ids[data_offset]);  
    }    
    
    typedef cub::BlockReduce&lt;topK&lt;T, K&gt;, blockSize&gt; blockreduce;  
    __shared__ typename blockreduce::TempStorage tmp_storage;  
    topK&lt;T, K&gt; block_topk = blockreduce(tmp_storage).Reduce(thread_topK, reduce_functor&lt;T, K&gt;);  
  
    if(tid == 0){  
        int beam_id = (blockDim.x * blockIdx.x + tid) / BlockPerBeam/ K; // æ”¹åŠ¨4ï¼šå†™å…¥æ—¶éœ€è¦è€ƒè™‘beam_idï¼Œæ„Ÿè§‰è¿™æ¡å…¬å¼æœ‰ç‚¹å¥‡æ€ªï¼Ÿ
        for(int k_offset = 0; k_offset &lt; K; k_offset++){  
            int dst_offset = bid * beam_width * K + 
				             beam_id * K + 
				             k_offset; // æ”¹åŠ¨5
            final_topK_vals[dst_offset] = block_topk.val[k_offset];  
            final_topK_ids[dst_offset] = block_topk.id[k_offset];  
        }    
    }
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/b25acc5c3bf310a0925e8b3119ca82d.jpg" alt=""><p></p>
<h1 id="Lesson18-FusedSoftmax-and-Sampling"><a href="#Lesson18-FusedSoftmax-and-Sampling" class="headerlink" title="Lesson18 FusedSoftmax and Sampling"></a>Lesson18 FusedSoftmax and Sampling</h1><p>è®²è§£äº†ï¼š<br><code>src/kernels/sampling.cu</code><br><code>src/kernels/sampling.h</code><br><code>src/utils/params.h</code><br><code>tests/unittests/test_sampling.cu</code></p>
<p>åœ¨GPUä¸Šç”Ÿæˆéšæœºæ•°ï¼Œ<strong>ä¸»æœºä»…ä¼ ç»™è®¾å¤‡ä¸€ä¸ªä¿¡å·ï¼Œæ˜¯çš„å¤šä¸ªéšæœºæ•°åœ¨deviceç«¯è¢«ç”Ÿæˆ</strong>ï¼š<code>curand_kernel</code></p>
<p><code>params.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">using IntDict = std::unordered_map&lt;std::string, int&gt;;
using floatDict = std::unordered_map&lt;std::string, float&gt;;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><br>é”®ä¸ºå­—ç¬¦ä¸²ï¼Œå€¼ä¸ºintæˆ–float<p></p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">__device__ void curand_init(unsigned long long seed, unsigned long long subsequence, unsigned long long offset, curandState_t* state)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><code>seed</code>ï¼šæ—¶é—´ç§å­ã€‚<br><code>subsequence</code>ï¼šåºåˆ—å·ï¼ŒåŒºåˆ†ä¸åŒçº¿ç¨‹å—çš„éšæœºæ•°ç”Ÿæˆå™¨ï¼Œç¡®ä¿æ¯ä¸ªå—æœ‰è‡ªå·±çš„éšæœºæ•°ç”Ÿæˆå™¨ã€‚<br><code>offset</code>ï¼šåœ¨æŒ‡å®šåºåˆ—ä¸­çš„åç§»é‡ï¼Œç”¨äºè·³è¿‡åºåˆ—çš„å‰å‡ ä¸ªå€¼ä»¥è·å¾—ä¸åŒçš„éšæœºæ•°ï¼Œè¿™é‡Œè¡¨ç¤ºä»åºåˆ—çš„èµ·ç‚¹å¼€å§‹ç”Ÿæˆéšæœºæ•°ã€‚<br><code>state</code>ï¼šæŒ‡å‘<code>curandState_t</code>çš„æŒ‡é’ˆï¼Œä¿å­˜ç”Ÿæˆå™¨çš„å†…éƒ¨çŠ¶æ€ã€‚</p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">__device__ float curand_uniform(curandState_t* state)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>è¿”å›åœ¨<code>0.0f</code>å’Œ<code>1.0f</code>ä¹‹é—´å‡åŒ€åˆ†å¸ƒçš„æµ®åŠ¨å€¼</p>
<p><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/eeb944a0bce156627540b5e069888b6.jpg" alt="|425"><br>åœ¨ä¸Šå›¾çš„ä¾‹å­ä¸­ï¼Œ<code>thredhold-topk_val[0]&gt;0ï¼Œthredhold-topk_val[0]-topk_val[1]&lt;0</code>ï¼Œå› æ­¤é‡‡æ ·å€¼è½åœ¨<code>topk_val[1]</code>ä¸Š</p>
<ul>
<li>gridï¼š<code>[batch_size]</code></li>
<li>blockï¼š<code>[K]</code></li>
</ul>
<h1 id="Lesson19-allocator"><a href="#Lesson19-allocator" class="headerlink" title="Lesson19 allocator"></a>Lesson19 allocator</h1><p>è®²è§£äº†ï¼š<br><code>src/memory/allocator/base_allocator.h</code><br><code>src/memory/allocator/cuda_allocator.h</code><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/2f3403abcd9639c30a2c174db5c3798.jpg" alt=""><br><code>base_allocator.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">class BaseAllocator // å…¬å…±çš„çˆ¶ç±»  
{  
public:  
    virtual ~BaseAllocator(){};
  
    template&lt;class T&gt;  
    T* Malloc(T* ptr, size_t size, bool is_host){  
        return(T*)UnifyMalloc((void*)ptr, size, is_host); 
    }    
    virtual void* UnifyMalloc(void* ptr, size_t size, bool is_host = false) = 0; 
  
    template&lt;typename T&gt;  
    void Free(T* ptr, bool is_host = false){  
        UnifyFree((void*)ptr, is_host);  
    }    
    virtual void UnifyFree(void* ptr, bool is_host = false) = 0;  
};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li>çˆ¶ç±»çš„ææ„å‡½æ•°è¦å£°æ˜ä¸ºè™šå‡½æ•°ï¼šç¡®ä¿å½“ä½¿ç”¨åŸºç±»æŒ‡é’ˆæŒ‡å‘æ´¾ç”Ÿç±»å¯¹è±¡æ—¶ï¼Œé”€æ¯å¯¹è±¡æ—¶ä¼š<strong>æ­£ç¡®è°ƒç”¨æ´¾ç”Ÿç±»çš„ææ„å‡½æ•°</strong>ã€‚</li>
<li><code>(void*)ptr</code>ï¼šCPUçš„åˆ†é…å‡½æ•°mallocè¿”å›çš„æ˜¯ä¸€ä¸ªvoidç±»å‹çš„ï¼Œæ‰€ä»¥æŠŠä¼ è¿›å»çš„æŒ‡é’ˆå¼ºè½¬ä¸ºvoid</li>
<li>å®šä¹‰<code>UnifyMalloc</code>å’Œ<code>UnifyFree</code>ä¸ºè™šå‡½æ•°ï¼Œåœ¨å­ç±»é‡Œä¸€å®šè¦å®ç°è¿™ä¸ªå‡½æ•° </li>
</ul>
<p><code>cuda_allocator.h</code><br>â‘ å®šä¹‰ä¸¤ç§å—<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">struct CudaBigBlock {  
    void *data;  
    size_t size;  
    bool is_allocated;  
    CudaBigBlock() = default; // æ„é€ å‡½æ•°  
    CudaBigBlock(void* data_, size_t size_, bool is_allocated_): // æ„é€ å‡½æ•°  
        data(data_), size(size_), is_allocated(is_allocated_){}  
};  
  
struct CudaSmallBlock {  
    void* data;  
    size_t size;  
    bool is_allocated;  
    CudaSmallBlock() = default; // æ„é€ å‡½æ•°  
    CudaSmallBlock(void* data_, size_t size_, bool is_allocated_): // æ„é€ å‡½æ•°  
            data(data_), size(size_), is_allocated(is_allocated_){}  
};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>å¤§å°å—çš„å®šä¹‰ç›¸åŒ<p></p>
<ul>
<li>å¤§å†…å­˜å—ï¼šä¸æ˜“é€ æˆå†…å­˜ç¢ç‰‡</li>
<li>å°å†…å­˜å—ï¼šç¢ç‰‡åŒ–è¾ƒä¸¥é‡ï¼Œæ„å»ºå°å—çš„å†…å­˜æ± ä¸»è¦ä¸ºäº†æ”¶é›†ç¢ç‰‡å¤§å°å½’è¿˜OS(æœ‰æ—¶ä¸æ˜¯å†…å­˜ä¸å¤Ÿï¼Œè€Œæ˜¯ç¢ç‰‡å¤ªå¤šå¯èƒ½ä¼šæŠ¥out of memoryçš„é”™</li>
</ul>
<p>â‘¡å®šä¹‰åˆ†é…å™¨<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">class CudaAllocator: public BaseAllocator {  
private:  
    //{device id: block}    // æ¯ä¸ªè®¾å¤‡éƒ½æœ‰å†…å­˜æ±   
    std::map&lt;int, std::vector&lt;CudaSmallBlock&gt; &gt; cudaSmallBlockMap;  
    std::map&lt;int, std::vector&lt;CudaBigBlock&gt; &gt; cudaBigBlockMap;  
    std::map&lt;int, size_t&gt; FreeSize;  
    int dev_id;  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>å®šä¹‰äº†<p></p>
<ul>
<li>è®¾å¤‡IDä¸ä»¥<code>CudaSmallBlock</code>ä¸ºå¯¹è±¡çš„æ•°ç»„çš„æ˜ å°„(æ¯ä¸ªè®¾å¤‡éƒ½æœ‰ä¸€ä¸ªå¤§ã€å°å†…å­˜æ± )</li>
<li>è®¾å¤‡IDä¸ä»¥<code>CudaBigBlock</code>ä¸ºå¯¹è±¡çš„æ•°ç»„çš„æ˜ å°„</li>
<li>è®¾å¤‡IDä¸è¯¥è®¾å¤‡ç©ºé—²å†…å­˜å¤§å°çš„æ˜ å°„</li>
</ul>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">public:  
    CudaAllocator() {  
        cudaGetDevice(&amp;dev_id);  
    }   
     ~CudaAllocator() {  
    }    <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>ä¸º<code>CudaAllocator</code>å®ç°<code>UnifyMalloc</code></p>
<p>0ï¼‰å¯¹é½32bytesä»¥å®ç°<code>float4</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void* UnifyMalloc(void* ptr, size_t size, bool is_host) { 
	size = ((size + 31) / 32 ) * 32;  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><br>1ï¼‰å¦‚æœæ˜¯ä¸»æœºä¸Šç”³è¯·bufferï¼Œç”¨<code>malloc</code>ç”³è¯·<br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">if(is_host){  
    ptr = malloc(size); 
    memset(ptr, 0, size);
    return ptr;  
}  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li><code>memset</code>ï¼šåˆå§‹åŒ–ä»<code>ptr</code>æŒ‡å‘å¼€å§‹çš„sizeä¸ªå€¼ï¼Œåˆå§‹åŒ–çš„æ•°å€¼ä¸º0<br>2ï¼‰åœ¨bigblocksä¸­æ‰¾ç©ºé—²çš„å—ï¼Œå³è¢«freeå‡ºæ¥ä½†æ˜¯è¿˜æœªå½’è¿˜åˆ°OSçš„<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">if(size &gt; 1024 * 1024){
    auto BigBlocks = cudaBigBlockMap[dev_id];  
    int blockID = -1;  
    for(int i = 0; i &lt; BigBlocks.size(); i++){ 
        if(BigBlocks[i].size &gt;= size&amp;&amp;!BigBlocks[i].is_allocated &amp;&amp; BigBlocks[i].size - size &lt; 1024 * 1024){  
            if(blockID == -1 || BigBlocks[blockID].size &gt; BigBlocks[i].size){ 
                blockID = i;  
            }        
        }    
    }    
    if(blockID != -1){  
        BigBlocks[blockID].is_allocated = true;  
        return BigBlocks[blockID].data;  
    }    
    void* new_buffer;  
    cudaMalloc(&amp;new_buffer, size);  
    BigBlocks.push_back(CudaBigBlock(new_buffer, size, false));  
    return new_buffer;  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li>å¦‚æœ<code>size</code>å¤§äº1024kå°±ç”¨bigblock</li>
<li><code>if(BigBlocks[i].size &gt;= size &amp;&amp; !BigBlocks[i].is_allocated &amp;&amp; BigBlocks[i].size - size &lt; 1024 * 1024)</code> <ul>
<li><code>BigBlocks[i].size &gt;= size</code>ï¼šè¯¥å†…å­˜å—çš„å¤§å°è¦å¤§äºç”³è¯·çš„å†…å­˜</li>
<li><code>!BigBlocks[i].is_allocated</code>ï¼šè¯¥å†…å­˜å—æ²¡æœ‰è¢«åˆ†é…å‡ºå»</li>
<li><code>BigBlocks[i].size - size &lt; 1024 * 1024</code>ï¼šè¯¥å†…å­˜å—åˆ†é…ä¹‹åå‰©ä½™çš„å†…å­˜ä¸ä¼šè¶…è¿‡1024k(ç¢ç‰‡åŒ–ï¼Ÿ)</li>
</ul>
</li>
<li><code>if(blockID == -1 || BigBlocks[blockID].size &gt; BigBlocks[i].size)</code><ul>
<li><code>blockID == -1</code>ï¼šå¦‚æœå½“å‰è¿˜æ²¡åˆ†é…å†…å­˜å—</li>
<li>æˆ–è€…<code>BigBlocks[blockID].size &gt; BigBlocks[i].size</code>ï¼šå·²ç»åˆ†é…ç»™è¯¥å†…å­˜çš„å†…å­˜å—æ¯”å½“å‰çš„å†…å­˜å—è¦å¤§ï¼Œåˆ™æ›¿æ¢å½“å‰å†…å­˜å—æ¥å­˜å‚¨</li>
</ul>
</li>
<li>åˆ†é…å†…å­˜å—ä¹‹åï¼Œè¿”å›ä¸€ä¸ªvoidç±»å‹çš„æŒ‡é’ˆ</li>
<li>å¦‚æœæœªèƒ½æ‰¾åˆ°åˆé€‚çš„ï¼Œç›´æ¥<code>cudaMalloc</code><br>3ï¼‰åœ¨smallblocksä¸­æ‰¾ç©ºé—²çš„å—ï¼Œå³è¢«freeå‡ºæ¥ä½†æ˜¯è¿˜æœªå½’è¿˜åˆ°OSçš„<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">auto SmallBlocks = cudaSmallBlocksMap[dev_id];  
for(int i = 0; i &lt; SmallBlocks.size(); i++){  
    if(SmallBlocks[i].size &gt;= size&amp;&amp;!SmallBlocks[i].is_allocated &amp;&amp;SmallBlocks[i].size - size &lt; 1024 * 1024){  
        SmallBlocks[i].is_allocated = true;  
        FreeSize[dev_id] += SmallBlocks[i].size; // è¿™é‡Œå»æ‰
        return SmallBlocks[i].data;  
    }        
}  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li>åŒ¹é…ç­–ç•¥ï¼šç®€å•é¦–æ¬¡åŒ¹é…ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªç¬¦åˆè¦æ±‚çš„å†…å­˜å—è€Œä¸å†æ¯”è¾ƒ</li>
<li><code>FreeSize[dev_id] += SmallBlocks[i].size;</code>ï¼šå°†åˆ†é…å‡ºæ¥çš„å†…å­˜å—å¤§å°åŠ åˆ°å¯¹åº”è®¾å¤‡çš„<code>FreeSize</code>ä¸­ï¼Œä»¥ä¾¿ä¹‹åé‡Šæ”¾å†…å­˜<br>4ï¼‰æ²¡æœ‰æ‰¾åˆ°åˆé€‚å†…å­˜çš„<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">    void* newBuffer = (void*)ptr;  
    CHECK(cudaMalloc(&amp;newBuffer, size));  
    CHECK(cudaMemset(newBuffer, 0, size)); // sizeæ˜¯åˆå§‹åŒ–çš„å­—èŠ‚æ•°  
    SmallBlocks.push_back(CudaSmallBlock(newBuffer, size, false));  
    return new_buffer;  
}  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><code>__host__ cudaError_t cudaMemset(void* devPtr, int value, size_t count)</code><ul>
<li>Initializes or sets device memory to a value.</li>
<li>devPtrï¼šPointer to device memory</li>
<li>valueï¼šValue to set for each byte of specified memory</li>
<li>countï¼š Size in bytes to set<br>0ï¼‰å¦‚æœæŒ‡é’ˆæŒ‡å‘ä¸»æœºç«¯çš„å†…å­˜ï¼Œç›´æ¥é‡Šæ”¾<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void UnifyFree(void* ptr, bool is_host) {  
	if (ptr == nullptr) {  
		return;  
	}
      if(is_host){  
           cudaFree(ptr);  
       } <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
1ï¼‰å½“ç´¯ç§¯çš„å°å†…å­˜å—è¶…è¿‡1Gæ—¶ï¼Œæ¸…ç†æœªåˆ†é…å‡ºå»çš„smallblocksï¼Œå·²åˆ†é…çš„ä¿ç•™åœ¨smallmapä¸­<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">for(auto&amp; it : cudaSmallBlocksMap){  
    if(FreeSize[it.first]) &gt; 1024 * 1024 * 1024{  
        auto&amp; cudaBlocks = it.second;  
        std::vector&lt;CudaSmallBlock&gt; tmp;  
        for(int i = 0; i &lt; cudaBlocks.size(); ++i){  
            if(!cudaBlocks[i].is_allocated){  
                cudaSetDevice(it.first);  
                cudaFree(cudaBlocks[i].data); // æœªåˆ†é…ï¼Œå½’è¿˜OS
            } else{  
                tmp.push_back(cudaBlocks[i]); // å·²åˆ†é…ï¼Œå­˜å›mapä¸­
            }  
        }                
        cudaBlocks.clear(); 
        it.second = tmp;  
        FreeSize[it.first] = 0; 
    }        
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
</ul>
</li>
<li><code>for(auto&amp; it : cudaSmallBlocksMap)</code>ï¼š<ul>
<li><code>&amp;it</code>ï¼šå¯¹å®¹å™¨å…ƒç´ çš„å¼•ç”¨ï¼Œ<code>&amp;</code>è¡¨ç¤ºå¯¹<code>it</code>çš„ä¿®æ”¹ä¼šç›´æ¥ä½œç”¨äºå®¹å™¨ä¸­çš„å…ƒç´ è€Œä¸ä¼šåˆ›å»ºå‰¯æœ¬</li>
<li><code>it.first</code>å’Œ<code>it.second</code>ï¼šåˆ†åˆ«æ˜¯è®¾å¤‡IDå’Œå†…å­˜å—å‘é‡</li>
</ul>
</li>
<li><code>__host__ cudaError_t cudaSetDevice(int device)</code>ï¼šSet device to be used for GPU executions.</li>
<li><code>cudaBlocks.clear()</code>ï¼šåœ¨æ›´æ–°cudaBlocksä¹‹å‰å…ˆæ¸…ç©º</li>
<li><code>FreeSize[it.first] = 0</code>ï¼šå¯¹å½“å‰è®¾å¤‡çš„FreeSizeå½’é›¶<br>3ï¼‰æ‰¾åˆ°å¾…freeçš„å†…å­˜å—çš„ä½ç½®ï¼Œè®¾<code>is_allocated = false</code>ï¼Œå¤§å°blockéƒ½ä¸å½’è¿˜åˆ°OSï¼Œé™¤éæ²¡æœ‰åœ¨å¤§å°blocké‡Œé¢æ‰¾åˆ°å¾…freeçš„æŒ‡é’ˆ<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">        for(auto&amp; it : cudaSmallBlocksMap){  
            auto&amp; cudaBlocks = it.second;  
            for(int i = 0; i &lt; cudaBlocks.size(); i++){  
                if(cudaBlocks[i].data == ptr){  
                    cudaBlocks[i].is_allocated = false;  
                    FreeSize[it.first] += cudaBlocks[i].size;
                    return;  
                }            
            }            
            auto&amp; bigBlocks = cudaBigBlocksMap[it.first];  
            for(int i = 0; i &lt; bigBlocks.size(); i++){  
                if(bigBlocks[i].data == ptr){  
                    bigBlocks[i].is_allocated = false;  
                    return;  
                }            
            }        
        }        
	    cudaFree(ptr);  
    }
};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ul>
<p><code>a.size</code>å’Œ<code>a.size()</code></p>
<ul>
<li>å½“<code>a</code>æ˜¯æ ‡å‡†å®¹å™¨(<code>std::vecotr</code>ï¼Œ<code>std::map</code>ç­‰ç­‰)æ—¶ï¼Œ<code>size</code>æ˜¯ä¸€ä¸ªæˆå‘˜å‡½æ•°ï¼Œç”¨äºè·å–å®¹å™¨çš„å¤§å°ï¼Œå†™æ³•ä¸º<code>a.size()</code>ï¼Œè°ƒç”¨æˆå‘˜å‡½æ•°</li>
<li>å½“<code>a</code>æ˜¯ç”¨æˆ·è‡ªå®šä¹‰çš„ç±»ï¼Œ<code>public: size_t size;</code>æ—¶ï¼Œ<code>size</code>æ˜¯ä¸€ä¸ªæˆå‘˜å˜é‡ï¼Œå†™æ³•ä¸º<code>a.size</code>ï¼›<code>public: size(){};</code>æ—¶ï¼Œ<code>size</code>æ˜¯æˆå‘˜å‡½æ•°ï¼Œå†™æ³•ä¸º<code>a.size()</code></li>
</ul>
<h1 id="Lesson-20-Context-attention"><a href="#Lesson-20-Context-attention" class="headerlink" title="Lesson 20 Context attention"></a>Lesson 20 Context attention</h1><p><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/5731d9562b034964b53c0e7d24bd858.jpg" alt=""></p>
<h2 id="20-1src-layers-attention-context-attention-cpp"><a href="#20-1src-layers-attention-context-attention-cpp" class="headerlink" title="20.1src/layers/attention/context_attention.cpp"></a>20.1<code>src/layers/attention/context_attention.cpp</code></h2><h3 id="20-1-1-æ„é€ å‡½æ•°"><a href="#20-1-1-æ„é€ å‡½æ•°" class="headerlink" title="20.1.1 æ„é€ å‡½æ•°"></a>20.1.1 æ„é€ å‡½æ•°</h3><p><code>LLaMAContextAttentionLayer&lt;T&gt;::LLaMAContextAttentionLayer</code>ï¼šæ„é€ å‡½æ•°<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">head_num(head_num),  
kv_head_num(kv_head_num),  
head_size(head_size),  
stream(stream),  
cublas_wrapper(cublas_wrapper),  
allocator(allocator), 
hidden_units(head_num * head_size),  
attn_static_params(attn_params),   
q_head_per_kv(head_num / kv_head_num),  
scale(float(1 / sqrt(head_size)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<h3 id="20-1-2-åˆ†é…å†…å­˜"><a href="#20-1-2-åˆ†é…å†…å­˜" class="headerlink" title="20.1.2 åˆ†é…å†…å­˜"></a>20.1.2 åˆ†é…å†…å­˜</h3><p><code>LLaMAContextAttentionLayer&lt;T&gt;::allocForForward(LLaMAAttentionDynParams&amp; params)</code>ï¼šåˆ†é…forwardæ‰€éœ€è¦çš„buffer</p>
<ul>
<li><p><code>LLaMAAttentionDynParams</code>å®šä¹‰æ¥æºï¼š<code>src/models/llama_llama_params.h</code></p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">struct LLaMAAttentionDynParams {  
    int batch_size;  
    int num_tokens;  
    int max_q_len;  
    int max_k_len;  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
<li><p>å…ˆå®šä¹‰æŒ‡é’ˆ</p>
<ul>
<li><code>new</code>ï¼šå®ƒä»å †ä¸Šåˆ†é…æŒ‡å®šç±»å‹çš„å†…å­˜ï¼Œå¹¶è¿”å›ä¸€ä¸ªæŒ‡å‘è¯¥å†…å­˜å—çš„æŒ‡é’ˆã€‚ä½¿ç”¨ <code>new</code> åˆ†é…çš„å†…å­˜ä¸ä¼šåƒæ ˆä¸Šåˆ†é…çš„å˜é‡é‚£æ ·åœ¨å‡½æ•°ç»“æŸæ—¶è‡ªåŠ¨é‡Šæ”¾ï¼Œéœ€è¦æ‰‹åŠ¨é‡Šæ”¾ã€‚</li>
<li>å’Œ<code>malloc</code>åŒºåˆ«ï¼š<ul>
<li><code>new</code>ï¼šä¸ä»…åˆ†é…å†…å­˜ï¼Œè¿˜ä¼šè°ƒç”¨å¯¹è±¡çš„æ„é€ å‡½æ•°ï¼ˆå¦‚æœæ˜¯ç±»å¯¹è±¡çš„è¯ï¼‰</li>
<li><code>malloc</code>ï¼šåªè´Ÿè´£åˆ†é…å†…å­˜ï¼Œä¸ä¼šè°ƒç”¨æ„é€ å‡½æ•°</li>
</ul>
</li>
</ul>
</li>
<li>å†åˆ†é…å†…å­˜<ul>
<li><code>allocator-&gt;Malloc</code></li>
<li>å¯¹<code>k_cache_buf</code>å’Œ<code>v_cache_buf</code>åˆ†é…å†…å­˜æ—¶ï¼Œåœ¨<code>k_cache_buf</code>åˆ†é…ä¸¤å€çš„å†…å­˜ï¼Œå†ä»¤<code>v_cache_buf</code>çš„æ•°æ®æŒ‡é’ˆæŒ‡å‘<code>k_cache_buf</code>åç§»<code>batch_size * head_num * max_k_len * head_size</code>çš„åœ°æ–¹ã€‚è¿™æ ·å¯ä»¥å‡å°‘ä¸€æ¬¡å†…å­˜åˆ†é…<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">k_cache_buf-&gt;data = allocator-&gt;Malloc(k_cache_buf-&gt;data, 2 * sizeof(T) * batch_size * head_num * max_k_len * head_size);

v_cache_buf-&gt;data = (T*)k_cache_buf-&gt;data + batch_size * head_num * max_k_len * head_size;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
â‘ <code>fusedQkvGemm</code><br><code>input</code></li>
</ul>
</li>
<li><code>input tensor</code><br><code>output</code></li>
<li><code>qkv_buf_wo_pad</code>: <code>[num_tokens, qkv_head_num, head_size]</code><br>ä½œç”¨ï¼šåšlinearå°†è¾“å…¥çš„tensorä¹˜ä¸Šqkvæƒé‡ï¼Œå¾—åˆ°qkv<br>â‘¡<code>AddbiasAndPaddingAndRope</code><br><code>output</code></li>
<li><code>q_buf_w_pad</code>: <code>[bs, head_num, max_q_len, head_size]</code></li>
<li><code>k_buf_w_pad</code>: <code>[bs, kv_head_num, max_q_len, head_size]</code></li>
<li><code>v_buf_w_pad</code>: <code>[bs, kv_head_num, max_q_len, head_size]</code><br>ä½œç”¨ï¼šæ·»åŠ åç½®ï¼Œè¿›è¡Œpaddingä½¿åŒä¸€æ‰¹æ¬¡çš„å¥å­é•¿åº¦ç›¸åŒï¼Œè¿›è¡Œä½ç½®æ—‹è½¬ç¼–ç <br>â‘¢<code>ConcatPastKVcache</code><br><code>output</code></li>
<li><code>k_cache_buf</code>: <code>[bs, head_num, max_q_len, head_size]</code></li>
<li><code>v_cache_buf</code>: <code>[bs, head_num, max_q_len, head_size]</code><br>ä½œç”¨ï¼šå°†æ–°å¾—åˆ°çš„KVå­˜å‚¨åˆ°cacheä¸­<br>â‘£<code>qk gemm</code><br><code>output</code></li>
<li><code>qk_buf</code>: <code>[bs, head_num, max_q_len, max_k_len]</code><br>ä½œç”¨ï¼šè¿›è¡Œqkç›¸ä¹˜ï¼Œå¾—åˆ°$QK^T$<br>â‘¤<code>FusedMaskAndScaleSoftmax</code><br><code>output</code></li>
<li><code>qk buf</code><br>ä½œç”¨ï¼šåŠ ä¸Šmaskå¹¶è¿›è¡Œscaleå’Œsoftmaxï¼Œå¾—åˆ°$Softmax(\dfrac{QK^T}{\sqrt{d_k}})$<br>â‘¥<code>qk*v gemm</code><br><code>output</code></li>
<li><code>qkv_buf_w_pad</code>: <code>[bs, head_num, max_q_len, head_size]</code><br>ä½œç”¨ï¼šå¾—åˆ°$Softmax(\dfrac{QK^T}{\sqrt{d_k}})V$<br>â‘¦<code>RemovingPadding</code><br><code>output</code></li>
<li><code>qkv_buf_wo_pad_1</code>: <code>[num_tokens, head_num, head_size]</code><br>ä½œç”¨ï¼šå°†paddingå»æ‰</li>
</ul>
<h3 id="20-1-3-é‡Šæ”¾å†…å­˜"><a href="#20-1-3-é‡Šæ”¾å†…å­˜" class="headerlink" title="20.1.3 é‡Šæ”¾å†…å­˜"></a>20.1.3 é‡Šæ”¾å†…å­˜</h3><p><code>src/utils/macro.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">inline void syncAndCheck(const char* const file, int const line){  
    cudaDeviceSynchronize();  
    cudaError_t result = cudaGetLastError();  
    if (result) {  
        throw std::runtime_error(std::string("[TM][ERROR] CUDA runtime error: ") + (_cudaGetErrorEnum(result)) + " " + file + ":" + std::to_string(line) + " \n");  
    }
}  
  
#define DeviceSyncAndCheckCudaError() syncAndCheck(__FILE__, __LINE__)  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li><code>syncAndCheck</code><ul>
<li><code>cudaDeviceSynchronize()</code>ï¼šç¡®ä¿å½“å‰æ‰€æœ‰ CUDA æ“ä½œå®Œæˆ</li>
<li><code>cudaGetLastError()</code>ï¼šæ£€æŸ¥CUDAè¿è¡Œæ—¶çš„æœ€åä¸€ä¸ªé”™è¯¯</li>
<li>å‚æ•°ï¼š<ul>
<li><code>file</code>ï¼šè®°å½•å‘ç”Ÿé”™è¯¯çš„æºæ–‡ä»¶åç§°</li>
<li><code>line</code>ï¼šè®°å½•å‘ç”Ÿé”™è¯¯çš„è¡Œå·</li>
</ul>
</li>
</ul>
</li>
<li><code>#define DeviceSyncAndCheckCudaError() syncAndCheck(__FILE__, __LINE__)</code><ul>
<li>è°ƒç”¨<code>syncAndCheck</code>å‡½æ•°ï¼Œå¹¶è‡ªåŠ¨æ•è·å½“å‰çš„æ–‡ä»¶åå’Œè¡Œå·ï¼Œåœ¨è°ƒç”¨æ—¶ä¸éœ€è¦æ˜¾å¼ä¼ é€’ <code>__FILE__</code> å’Œ <code>__LINE__</code></li>
</ul>
</li>
</ul>
<p>é‡Šæ”¾<code>qkv_buf_wo_pad</code>ã€<code>q_buf_w_pad</code>ã€<code>k_cache_buf</code>ã€<code>qk_buf</code>ã€<code>qkv_buf_w_pad</code>ã€<code>qkv_buf_wo_pad_1</code>ï¼Œåœ¨æ¯ä¸ª<code>Free</code>çš„åé¢åŠ ä¸Š<code>DeviceSyncAndCheckCudaError()</code>ï¼Œæ£€æŸ¥æ˜¯å¦å‘ç”Ÿé”™è¯¯</p>
<h3 id="20-1-4-å‰å‘ä¼ æ’­"><a href="#20-1-4-å‰å‘ä¼ æ’­" class="headerlink" title="20.1.4 å‰å‘ä¼ æ’­"></a>20.1.4 å‰å‘ä¼ æ’­</h3><p><code>src/utils/tensor.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">struct TensorMap{  
    std::unordered_map&lt;std::string, Tensor*&gt; tensor_map_;  
  
    TensorMap() = default;  
    
    TensorMap(std::initializer_list&lt;std::pair&lt;std::string, Tensor*&gt;&gt; tensor_map){  
        for (auto&amp; pair : tensor_map){  
            if (isValid(pair.second)){  
                tensor_map_.insert(pair.first, pair.second); 
            }  
            else{  
                LLM_CHECK_WITH_INFO(isValid(pair.second),fmtstr("%s is not a valid tensor, skipping insert into TensorMap", pair.first.c_str()));  
            }        
        }    
    }  
    
    TensorMap(const std::unordered_map&lt;std::string, Tensor*&gt;&amp; tensor_map) {  
		for(auto it = tensor_map.begin(); it != tensor_map.end(); it++){
            if (isValid(it-&gt;second)) {  
                tensor_map_.insert(it-&gt;first, it-&gt;second);  
            }            
            else {  
                // TODO: add a reminder info  
            }  
        }    
    };
    // ...
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li><code>TensorMap(std::initializer_list&lt;std::pair&lt;std::string, Tensor*&gt;&gt; tensor_map)</code><ul>
<li>æ¥å—ä¸€ä¸ª <code>std::initializer_list</code> ç±»å‹çš„å‚æ•°ï¼Œå…¶å…ƒç´ æ˜¯é”®å€¼å¯¹ <code>std::pair&lt;std::string, Tensor*&gt;</code>ï¼Œé€‚ç”¨äºåˆå§‹åŒ–å®¹å™¨</li>
<li>ä¾‹å­ğŸ‘‡<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">Tensor* tensor1 = new Tensor(); 
Tensor* tensor2 = nullptr; // æ— æ•ˆæŒ‡é’ˆ
TensorMap tmap = {{"key1", tensor1}, 
				 {"key2", tensor2}} // æ— æ•ˆï¼Œä¼šè¢«è·³è¿‡<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li>
</ul>
</li>
<li><code>TensorMap(const std::unordered_map&lt;std::string, Tensor*&gt;&amp; tensor_map)</code><ul>
<li>æ¥å—ä¸€ä¸ª <code>std::unordered_map&lt;std::string, Tensor*&gt;</code> ç±»å‹çš„å‚æ•°ï¼Œä½¿ç”¨ç°æœ‰å“ˆå¸Œè¡¨åˆå§‹åŒ–</li>
<li>ä¾‹å­ğŸ‘‡<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">std::unordered_map&lt;std::string, 
Tensor*&gt; umap = {{"key1", tensor1}, 
				{"key2", tensor2}}; 
TensorMap tmap(umap);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ul>
</li>
</ul>
<p>â‘ å…¥å‚<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
void LLaMAContextAttentionLayer&lt;T&gt;::forward
(TensorMap&amp; inputs, 
TensorMap&amp; outputs, 
LLaMAattentionWeights&lt;T&gt;&amp; weights,
LLaMAAttentionDynParams&amp; params, 
LLaMAAttentionStaticParams&amp; static_params)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li><code>inputs</code>ï¼šå…ƒç´ å¤§æ¦‚æ˜¯<code>{"attention_input",tensor1},{"padding_offset",tensor2}</code><ul>
<li>å› ä¸ºå¾ˆå¤šå‡½æ•°éœ€è¦TensorWrapperï¼Œè€Œä¼ è¿›å»çš„æ˜¯Tensorï¼Œå¯¹äºéœ€è¦Tensorå¼ºè½¬ä¸ºTensorWrapperçš„æƒ…å†µï¼Œç”¨åˆ°ğŸ‘‡(<code>src/utils/tensor.h</code>)<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
TensorWrapper&lt;T&gt;* as(){  
    return static_cast&lt;TensorWrapper&lt;T&gt;*&gt;(this); // Tensorè½¬å­ç±»TensorWrapperçš„ä¸‹è¡Œè½¬æ¢  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li>
</ul>
</li>
<li><code>outputs</code>ï¼šåŒä¸Š</li>
<li><code>weights</code>ï¼š<code>src/weights/llama/attention_weights.h</code>ä¸­ï¼Œå†…ç½®å±æ€§æœ‰<code>BaseWeight&lt;T&gt; qkv;  BaseWeight&lt;T&gt; output;</code></li>
<li><code>params</code>ï¼š<code>src/models/llama/llama_params.h</code>ï¼Œå†…ç½®å±æ€§æœ‰<code>int batch_size; int num_tokens; int max_q_len; int max_k_len;</code></li>
<li><code>static_params</code>ï¼š<code>src/models/llama/llama_params.h</code>ï¼Œæ˜¯å…³äºæ—‹è½¬ç¼–ç çš„å±æ€§<code>int rotary_embedding_dim; float rotary_embedding_base;  int max_position_embeddings; bool use_dynamic_ntk;</code></li>
</ul>
<p>â‘¡å‡†å¤‡å†…å­˜<br>ä½¿ç”¨20.1.2ä¸­çš„åˆ†é…å†…å­˜</p>
<p>â‘¢qkv linear<br><code>src/kernels/linear.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchLinearGemm(attention_input-&gt;as&lt;T&gt;(), weights.qkv, qkv_buf_wo_pad, cublas_wrapper)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br>å¯¹åº”<code>input</code>ã€<code>weight</code>ã€<code>output</code>ã€<code>cublas_wrapper</code>ã€<code>trans_a</code>ã€<code>trans_b</code><br>å®Œæˆ<code>fusedQkvGemm</code><p></p>
<p>â‘£qkv bias and rope and padding<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchAddFusedQKVBiasTransposeAndRoPE(qkv_buf_w_pad, 
									  k_buf_w_pad, 
									  v_buf_w_pad, 
									  qkv_buf_wo_pad,
									  weights.qkv, 
									  padding_offset-&gt;as&lt;int&gt;(), 
									  history_length-&gt;as&lt;int&gt;(), 
									  input_length-&gt;as&lt;int&gt;(),
									  static_params);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>æœ€ååœ¨<code>k_buf_w_pad</code>å’Œ<code>v_buf_w_pad</code>å¾—åˆ°ropeå’Œpaddingçš„ç‰ˆæœ¬<p></p>
<p>â‘¤concat past kv cache<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchConcatKVCache(k_buf_w_pad, v_buf_w_pad, 
					layer_id-&gt;as&lt;int&gt;(), 
					input_length-&gt;as&lt;int&gt;(), 
					history_length-&gt;as&lt;T&gt;(),  
                    all_k_cache-&gt;as&lt;T&gt;(), 
                    all_v_cache-&gt;as&lt;T&gt;());<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>æœ€ååœ¨<code>all_k_cache</code>å’Œ<code>all_v_cache</code>å¾—åˆ°kvcache<br>å› ä¸º<code>layer_id</code>æ˜¯åœ¨CPUä¸Šåˆ†é…çš„<code>int layer = layer_id-&gt;getVal();</code>å› æ­¤éœ€è¦è½¬ä¸ºTensorWrapper<p></p>
<p>â‘¥repeat kv<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchRepeatKVCache(all_k_cache-&gt;as&lt;T&gt;(), 
					all_v_cache-&gt;as&lt;T&gt;(), 
					context_length-&gt;as&lt;int&gt;(),  
                    layer_id-&gt;as&lt;int&gt;(),
                    k_cache_buf, 
                    v_cache_buf);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br><code>input</code>:<br>    <code>all_k_cache</code>&amp;<code>all_v_cache</code>: <code>[num_layers, batch_size, kv_head_num, max_seq_len, head_size]</code><br><code>output</code>:<br>    <code>k_cache_buf</code>&amp;<code>v_cache_buf</code>: <code>[bs, head_num, max_k_len, head_size]</code><br>ä½œç”¨æ˜¯å°†kvcacheçš„<code>kv_head_num</code>è¡¥æˆ<code>head_num</code><p></p>
<p>â‘¦qk<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchLinearStridedBatchGemm(q_buf_w_pad, k_cache_buf, qk_buf, cublas_wrapper, false, true);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br><code>input</code>:<br>    <code>q_buf_w_pad</code>: <code>[bs, head_num, max_q_len, head_size]</code><br>    <code>k_cache_buf</code>: <code>[bs, head_num, max_k_len, head_size]</code>(trans_b = true)<br><code>output</code>:<br>    <code>qk_buf</code>: <code>[bs, head_num, max_q_len, max_k_len]</code><p></p>
<p>â‘§scale + mask + softmax<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchScaleMaskAndSoftmax(qk_buf, attention_mask-&gt;as&lt;T&gt;(), qk_buf, scale);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br>ç»™<code>qk_buf</code>åŠ scaleã€maskã€softmax<p></p>
<p>â‘¨qk*v<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchLinearStridedBatchGemm(qk_buf, v_cache_buf, qkv_buf_w_pad, cublas_wrapper, false, false);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br><code>input</code>:<br>    <code>qk_buf</code>: <code>[bs, head_num, max_q_len, max_k_len]</code><br>    <code>v_cache_buf</code>: <code>[bs, head_num, max_k_len, head_size]</code><br><code>output</code>:<br>    <code>qkv_buf_w_pad</code>: <code>[bs, head_num, max_q_len, head_size]</code><p></p>
<p>â‘©transpose + removepadding<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchTransposeOutRemovePadding(qkv_buf_w_pad, padding_offset-&gt;as&lt;T&gt;(), qkv_buf_wo_pad_1);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br><code>input</code>:<br>    <code>qkv_buf_w_pad</code>: <code>[bs, head_num, max_q_len, head_size]</code><br>    å…ˆtransposeå˜æˆ<code>[bs, max_q_len, head_num, head_size]</code><br><code>output</code>:<br>    <code>qkv_buf_wo_pad_1</code>: <code>[numtokens, hiddenunits]</code><p></p>
<p>â‘ output linear<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchLinearGemm(qkv_buf_wo_pad_1, weights.output, attention_output, cublas_wrapper, false, true);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br>ä¹˜ä¸Šè¾“å‡ºçš„æƒé‡<p></p>
<p>â‘¡freebuf<br>é‡Šæ”¾æ‰€æœ‰çš„ç¼“å­˜</p>
<h2 id="20-2examples-cpp-attention-context-attn-example-cpp"><a href="#20-2examples-cpp-attention-context-attn-example-cpp" class="headerlink" title="20.2examples/cpp/attention/context_attn_example.cpp"></a>20.2examples/cpp/attention/context_attn_example.cpp</h2><p>å˜é‡ï¼š</p>
<ul>
<li>åŸºæœ¬å‚æ•°ï¼š<ul>
<li><code>head_num</code>&amp;<code>kv_head_num</code>ï¼šå‰è€…æ˜¯qçš„ï¼Œåè€…æ˜¯kå’Œvçš„</li>
<li><code>head_size</code></li>
<li><code>num_layers</code></li>
<li><code>max_seq_len</code>ï¼škv cacheæœ€å¤§çš„ä¸Šä¸‹æ–‡é•¿åº¦</li>
<li><code>hidden_units</code>&amp;<code>q_hidden_units</code>ï¼šå‰è€…æ˜¯qkvæ€»å’Œçš„ï¼Œåè€…æ˜¯qçš„</li>
<li>ä½œä¸ºåˆå§‹åŒ–æ¯ä¸ªkernelé‡Œå¤§å°çš„å‚æ•°</li>
</ul>
</li>
<li>é™æ€å‚æ•°ï¼š(å¤šæ•°æ˜¯ä½ç½®ç¼–ç çš„)<ul>
<li><code>rotary_embedding_dim</code></li>
<li><code>rotary_embedding_base</code></li>
<li><code>max_position_embeddings</code></li>
<li><code>use_dynamic_ntk</code></li>
</ul>
</li>
<li>åŠ¨æ€å‚æ•°ï¼š<ul>
<li><code>batch_size</code></li>
<li><code>num_tokens</code></li>
<li><code>max_q_len</code>&amp;<code>max_k_len</code><ul>
<li><code>max_q_len</code>ï¼špaddingä¹‹å‰åŒä¸€batchä¸‹çš„æœ€é•¿çš„å¥å­é•¿åº¦</li>
<li><code>max_k_len</code>ï¼šåŒä¸€ä¸ªbatchä¸­ä¸Šä¸‹æ–‡çš„æœ€å¤§å€¼</li>
</ul>
</li>
</ul>
</li>
<li>è¾“å…¥è¾“å‡ºå€¼(ä»ä¸»æœºä¸Šè·å–æ•°æ®ï¼Œå¤åˆ¶åˆ°è®¾å¤‡ä¸Š)<ul>
<li><code>attention_input</code>ï¼š<code>[num_tokensï¼Œq_hidden_units]</code>ï¼Œæ˜¯æœ€åˆçš„è¾“å…¥</li>
<li><code>qkv_weights</code>ï¼š<code>[q_hidden_units, hidden_units]</code>ï¼Œåš<code>qkvgemm</code>æ—¶ç”¨åˆ°</li>
<li><code>mask</code>ï¼š<code>[batch_size, max_q_len, max_k_len]</code>ï¼Œå½“å‰çš„toeknä¸èƒ½è®¿é—®åˆ°å…¶åé¢çš„token</li>
<li><code>qkv_bias</code>ï¼š<code>[hidden_units]</code>ï¼Œqkvçš„åç½®</li>
<li><code>all_k_cache</code>ï¼š<code>[num_layers, batch_size, kv_head_num, max_seq_len, head_size]</code></li>
<li><code>all_v_cache</code>ï¼š<code>[num_layers, batch_size, kv_head_num, max_seq_len, head_size]</code></li>
<li><code>padding_offset</code>ï¼š<code>[num_tokens]</code>ï¼Œæ¯ä¸ªtokenéƒ½æœ‰ä¸€ä¸ªâ€åœ¨è¯¥tokenä¹‹å‰çš„paddingä¸ªæ•°çš„æ•°å€¼â€œ</li>
<li><code>history_length</code>ï¼š<code>[batch_size]</code></li>
<li><code>layer_id</code>ï¼š</li>
<li><code>ctx_len</code>ï¼š<code>[batch_size]</code>ï¼Œæ¯å¥è¯çš„ä¸Šä¸‹æ–‡é•¿åº¦ï¼Ÿ</li>
<li><code>attention_output</code>ï¼š<code>[num_tokens, q_hidden_units]</code></li>
<li><code>output_weights</code>ï¼š<code>[q_hidden_units, q_hidden_units]</code></li>
</ul>
</li>
</ul>
<h1 id="Lesson21-mask-self-attention-layer"><a href="#Lesson21-mask-self-attention-layer" class="headerlink" title="Lesson21 mask self attention layer"></a>Lesson21 mask self attention layer</h1><p>è¯´æ˜¯å†™çš„GQAéƒ¨åˆ†<br>åŒºåˆ«ä¸context decoder: </p>
<ul>
<li>è‡ªå›å½’ç”Ÿæˆæ¨¡å¼ï¼Œå› æ­¤ä¸éœ€è¦maskå’Œpadding(å’Œremove padding)</li>
</ul>
<p>layeræ­å»ºé¡ºåºå’Œcontext attentionç±»ä¼¼</p>
<h2 id="21-1src-layers-attention-masked-self-attentioon-cpp"><a href="#21-1src-layers-attention-masked-self-attentioon-cpp" class="headerlink" title="21.1src/layers/attention/masked_self_attentioon.cpp"></a>21.1src/layers/attention/masked_self_attentioon.cpp</h2><p>â‘ åˆ†é…å†…å­˜<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">allocForForward(params);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br>â‘¡qkv linear<br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchLinearGemm(attention_input-&gt;as&lt;T&gt;(), weights.qkv, qkv_buf, cublas_wrapper);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br>åŒæ—¶ï¼Œè¿™é‡Œä¸éœ€è¦åœ¨åé¢åŠ ä¸Š<code>DeviceSyncAndCheckCudaError();</code>å› ä¸º<code>cublasWrapper</code>è‡ªå¸¦äº†<code>CHECK_CUBLAS</code>(å› æ­¤æ¶‰åŠåˆ°cublasçš„éƒ½ä¸éœ€è¦å†è¿›è¡Œæ£€æŸ¥)<br>â‘¢fused decoder self attention<br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchDecoderMaskedMHA(qkv_buf, weights.qkv, 
					   layer_id-&gt;as&lt;int&gt;(),  
                       k_cache-&gt;as&lt;T&gt;(), 
                       v_cache-&gt;as&lt;T&gt;(), 
                       finished-&gt;as&lt;bool&gt;(),  
                       step-&gt;as&lt;int&gt;(), 
                       mha_output-&gt;as&lt;T&gt;(), 
                       static_params);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>æœ€åä¸€ä¸ªå…¥å‚æ˜¯<code>LLaMAAttentionStaticParams&amp; static_param</code>ï¼Œæ˜¯ä¸€ä¸ªå«æœ‰ä½ç½®ç¼–ç å±æ€§çš„ç»“æ„ä½“<p></p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">struct LLaMAAttentionStaticParams {  
    int   rotary_embedding_dim;  
    float rotary_embedding_base;  
    int   max_position_embeddings;  
    bool  use_dynamic_ntk; // for dyn scaling rope  
};
template&lt;typename T&gt;  
class LLaMASelfAttentionLayer {
private:
	LLaMAAttentionStaticParams attn_static_params;
public:
	LLaMAAttentionStaticParams&amp; GetAttnStaticParams(){  
    return attn_static_params;  // è¿™é‡Œçš„è¿”å›å€¼æ˜¯å¼•ç”¨ï¼Œå‡½æ•°çš„è°ƒç”¨ä¸ä¼šå¤åˆ¶attn_static_paramsï¼Œè€Œæ˜¯ç›´æ¥è¿”å›å®ƒçš„å†…å­˜åœ°å€
}
template&lt;typename T&gt;  
void LLaMASelfAttentionLayer&lt;T&gt;::forward(TensorMap&amp; inputs, TensorMap&amp; outputs, LLaMAattentionWeights&lt;T&gt;&amp; weights, LLaMAAttentionDynParams&amp; params){
	LLaMAAttentionStaticParams static_params = GetAttnStaticParams();
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>ä¸ºä»€ä¹ˆå¯ä»¥ç›´æ¥ä½¿ç”¨<code>LLaMAAttentionStaticParams static_params = GetAttnStaticParams();</code>ï¼š<ul>
<li>ç¼–è¯‘å™¨å¯¹<strong>å¼•ç”¨</strong>æŒ‡å‘çš„<code>attn_static_params</code>æ‰§è¡Œæ‹·è´æ„é€ ï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„<code>LLaMAAttentionStaticParams</code>å®ä¾‹<ul>
<li>å±€éƒ¨å˜é‡<code>static_params</code>æ˜¯ä¸€ä¸ª<strong>å€¼ç±»å‹</strong></li>
<li><code>GetAttnStaticParams()</code>è¿”å›ä¸€ä¸ªæŒ‡å‘ç±»ä¸­æˆå‘˜å˜é‡<code>attn_static_params</code>çš„<strong>å¼•ç”¨</strong></li>
</ul>
</li>
<li>å¦‚æœä¿®æ”¹<code>static_params</code>ï¼Œä¸ä¼šå½±å“<code>attn_static_params</code></li>
</ul>
</li>
<li>å¦‚æœæ˜¯å¦ä¸€ç§æƒ…å†µ<code>LLaMAAttentionStaticParams&amp; static_params = GetAttnStaticParams();</code><ul>
<li><code>static_params</code>åªæ˜¯<code>attn_static_params</code>çš„ä¸€ä¸ªåˆ«åï¼Œç¼–è¯‘å™¨ä¸ä¼šä¸º<code>static_params</code>åˆ†é…æ–°çš„å†…å­˜ç©ºé—´ï¼Œä»–å’Œ<code>attn_static_params</code>å…±ç”¨ä¸€å—å†…å­˜<ul>
<li>å±€éƒ¨å˜é‡<code>static_params</code>æ˜¯ä¸€ä¸ª<strong>å¼•ç”¨ç±»å‹</strong></li>
<li><code>GetAttnStaticParams()</code>è¿”å›ä¸€ä¸ªæŒ‡å‘ç±»ä¸­æˆå‘˜å˜é‡<code>attn_static_params</code>çš„<strong>å¼•ç”¨</strong></li>
</ul>
</li>
</ul>
</li>
<li>å¼•ç”¨ä¸æŒ‡é’ˆçš„åŒºåˆ«<ul>
<li>å¼•ç”¨ï¼šä¸€æ—¦ç»‘å®šåˆ°æŸä¸ªå˜é‡å°±ä¸èƒ½å†ç»‘å®šåˆ°å…¶ä»–å˜é‡ï¼›æœ¬è´¨ä¸Šæ˜¯å˜é‡çš„åˆ«åï¼Œä¸éœ€è¦å ç”¨é¢å¤–çš„å†…å­˜</li>
<li>æŒ‡é’ˆï¼šå¯ä»¥é‡æ–°æŒ‡å‘å…¶ä»–å˜é‡ï¼›æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å˜é‡ï¼Œéœ€è¦å ç”¨å†…å­˜æ¥å­˜å‚¨åœ°å€</li>
</ul>
</li>
</ul>
<p>â‘£output<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchLinearGemm(mha_output, weights.output, attention_output-&gt;as&lt;T&gt;, cublas_wrapper);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p></p>
<h2 id="21-2src-examples-cpp-attention-self-attention-example-cpp"><a href="#21-2src-examples-cpp-attention-self-attention-example-cpp" class="headerlink" title="21.2src/examples/cpp/attention/self_attention_example.cpp"></a>21.2src/examples/cpp/attention/self_attention_example.cpp</h2><p>å˜é‡ï¼š</p>
<ul>
<li>åŸºæœ¬å‚æ•°ï¼š<ul>
<li><code>head_num</code>&amp;<code>kv_head_num</code>ï¼šå‰è€…æ˜¯qçš„ï¼Œåè€…æ˜¯kå’Œvçš„</li>
<li><code>head_size</code></li>
<li><code>num_layers</code></li>
<li><code>max_seq_len</code>ï¼škv cacheæœ€å¤§çš„ä¸Šä¸‹æ–‡é•¿åº¦</li>
<li><code>hidden_units</code>&amp;<code>q_hidden_units</code>ï¼šå‰è€…æ˜¯qkvæ€»å’Œçš„ï¼Œåè€…æ˜¯qçš„</li>
<li>ä½œä¸ºåˆå§‹åŒ–æ¯ä¸ªkernelé‡Œå¤§å°çš„å‚æ•°</li>
</ul>
</li>
<li>é™æ€å‚æ•°ï¼š(å¤šæ•°æ˜¯ä½ç½®ç¼–ç çš„)<ul>
<li><code>rotary_embedding_dim</code></li>
<li><code>rotary_embedding_base</code></li>
<li><code>max_position_embeddings</code></li>
<li><code>use_dynamic_ntk</code></li>
</ul>
</li>
<li>åŠ¨æ€å‚æ•°ï¼š<ul>
<li><code>batch_size</code></li>
</ul>
</li>
<li>è¾“å…¥è¾“å‡ºå€¼(ä»ä¸»æœºä¸Šè·å–æ•°æ®ï¼Œå¤åˆ¶åˆ°è®¾å¤‡ä¸Š)<ul>
<li><code>attention_input</code>ï¼š<code>[num_tokensï¼Œq_hidden_units]</code>ï¼Œæ˜¯æœ€åˆçš„è¾“å…¥</li>
<li><code>all_k_cache</code>ï¼š<code>[num_layers, batch_size, kv_head_num, max_seq_len, head_size]</code></li>
<li><code>all_v_cache</code>ï¼š`[num_layers, batch_size, kv_head_num, max_seq_len, </li>
<li><code>layer_id</code></li>
<li><code>finished</code>ï¼š<code>[batch_size]</code></li>
<li><code>qkv_weights</code>ï¼š<code>[q_hidden_units, hidden_units]</code>ï¼Œåš<code>qkvgemm</code>æ—¶ç”¨åˆ°</li>
<li><code>output_weights</code>ï¼š<code>[q_hidden_units, q_hidden_units]</code></li>
<li><code>qkv_bias</code>ï¼š<code>[hidden_units]</code>ï¼Œqkvçš„åç½®</li>
<li><code>attention_output</code>ï¼š<code>[num_tokens, q_hidden_units]</code></li>
</ul>
</li>
</ul>
<h1 id="Lesson22-FFN"><a href="#Lesson22-FFN" class="headerlink" title="Lesson22 FFN"></a>Lesson22 FFN</h1><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/peixu/p/16842247.html" title="å‘å¸ƒäº 2022-10-30 21:04">å…³äºTransformerä¸­feed forward layerç†è§£</a><br><a target="_blank" rel="noopener" href="https://juejin.cn/post/7389923941492375579">Transformer è®ºæ–‡é€šä¿—è§£è¯»ï¼šFFN çš„ä½œç”¨</a></p>
<h2 id="22-1-src-layers-ffn-ffn-h"><a href="#22-1-src-layers-ffn-ffn-h" class="headerlink" title="22.1 src/layers/ffn/ffn.h"></a>22.1 <code>src/layers/ffn/ffn.h</code></h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void allocForForward(LLaMAAttentionDynParams&amp; params);  
void allocForForward(int batch_size);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>é‡è½½å‡½æ•°</p>
<ul>
<li>context attentionä¸­åœ¨remove paddingåæ•°æ®çš„ç¬¬ä¸€ç»´æ˜¯<code>num_tokens</code>(ä¼ å…¥çš„æ˜¯<code>params.num_tokens</code>)</li>
<li>self attentionä¸­æ•°æ®çš„ç¬¬ä¸€ç»´ä¸€ç›´æ˜¯<code>batch_size</code>(<code>[batch_size, 1, ...]</code>)</li>
</ul>
<h2 id="22-2-src-layers-ffn-ffn-cpp"><a href="#22-2-src-layers-ffn-ffn-cpp" class="headerlink" title="22.2 src/layers/ffn/ffn.cpp"></a>22.2 <code>src/layers/ffn/ffn.cpp</code></h2><p><code>forward()</code><br>â‘ ç¡®å®šä½¿ç”¨å“ªç§forwardçš„å†…å­˜åˆ†é…<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">if (params.num_tokens &gt; 0) {  
    allocForForward(params);  
	} else {                  
    allocForForward(params.batch_size);  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>å¦‚æœå­˜åœ¨num_tokensåˆ™ä¸ºcontext attentionï¼Œå¯¹åº”<code>params</code>ï¼›<br>å¦‚æœä¸å­˜åœ¨åˆ™ä¸ºself attentionï¼Œå¯¹åº”<code>batch_size</code><p></p>
<p>â‘¡fusedGateUp projs<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchLinearGemm(ffn_input-&gt;as&lt;T&gt;(), weights.gateAndup, SwiGLU_input, cublas_wrapper, false, true);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p></p>
<ul>
<li>è¾“å…¥<code>ffn_input</code>ï¼š<code>[bs(/num_tokens), q_hidden_units]</code></li>
<li>æƒé‡<code>weights.gateAndup</code>ï¼š<code>[q_hidden_units, 2 * inter_size]</code></li>
<li>è¾“å‡º<code>SwiGLU_input</code>ï¼š<code>[bs(/num_tokens), 2 * inter_size]</code></li>
<li>ç»è¿‡Gate Linearå’ŒUp Linearçš„è¾“å…¥éƒ½æ˜¯<code>[bs(/num_tokens), q_hidden_units]</code>ï¼Œå› æ­¤å°†ä»–ä»¬åƒfusedQKVGemmä¸€æ ·è¿›è¡ŒfusedGateUpGemmï¼Œè¾“å‡ºä½¿ç”¨åŒä¸€å—buf</li>
<li>ä¸ºå•¥è¿™é‡Œtrans_b=trueï¼Ÿ</li>
</ul>
<p>â‘¢swiGLU<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchAct(SwiGLU_input, down_proj_input);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p></p>
<ul>
<li>è¾“å…¥<code>SwiGLU_input</code>ï¼š<code>[bs(/num_tokens), 2 * inter_size]</code><ul>
<li>ä¸¤ä¸ªå¤§å°ä¸º<code>[bs(/num_tokens), inter_size]</code>çš„Gateæ•°ç»„å’ŒUpæ•°ç»„çš„ç›¸åŒåç§»é‡çš„æ•°æ®ä¸€èµ·è®¡ç®—ï¼Œå› æ­¤æœ€åçš„è¾“å‡ºçš„ç¬¬äºŒç»´å¤§å°ä¸ºåŸæ¥çš„ä¸€åŠ</li>
</ul>
</li>
<li>è¾“å‡º<code>down_proj_input</code>ï¼š<code>[bs(/num_tokens), inter_size]</code></li>
</ul>
<p>â‘£down proj<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchLinearGemm(down_proj_input, weights.down, ffn_output-&gt;as&lt;T&gt;(), cublas_wrapper, false, true);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p></p>
<ul>
<li>è¾“å…¥<code>down_proj_input</code>ï¼š<code>[bs(/num_tokens), inter_size]</code></li>
<li>æƒé‡<code>weights.gateAndup</code>ï¼š<code>[q_hidden_units, inter_size]</code> trans_b=true</li>
<li>è¾“å‡º<code>SwiGLU_input</code>ï¼š<code>[bs(/num_tokens), q_hidden_units]</code></li>
</ul>
<h2 id="22-3examples-cpp-ffn-ffn-example-cpp"><a href="#22-3examples-cpp-ffn-ffn-example-cpp" class="headerlink" title="22.3examples/cpp/ffn/ffn_example.cpp"></a>22.3<code>examples/cpp/ffn/ffn_example.cpp</code></h2><p>å˜é‡ï¼š</p>
<ul>
<li>åŸºæœ¬å‚æ•°ï¼š<ul>
<li>`head_num</li>
<li><code>head_size</code></li>
<li><code>inter_size</code></li>
<li>`hidden_units</li>
<li>ä½œä¸ºåˆå§‹åŒ–æ¯ä¸ªkernelé‡Œå¤§å°çš„å‚æ•°</li>
</ul>
</li>
<li>åŠ¨æ€å‚æ•°ï¼š<ul>
<li><code>num_tokens</code></li>
</ul>
</li>
<li>è¾“å…¥è¾“å‡ºå€¼(ä»ä¸»æœºä¸Šè·å–æ•°æ®ï¼Œå¤åˆ¶åˆ°è®¾å¤‡ä¸Š)<ul>
<li><code>ffn_input</code>ï¼š<code>[hidden_units, num_tokens]</code></li>
<li><code>gate_up</code>ï¼š<code>[hidden_units, 2 * inter_size]</code></li>
<li><code>down</code>ï¼š<code>[hidden_units, inter_size]</code></li>
</ul>
</li>
<li>è®¾ç½®ä¸ºè®¾å¤‡å‚æ•°<ul>
<li><code>ffn_output</code></li>
</ul>
</li>
</ul>
<h2 id="22-4-å…³äºCMakeList-txt"><a href="#22-4-å…³äºCMakeList-txt" class="headerlink" title="22.4 å…³äºCMakeList.txt"></a>22.4 å…³äºCMakeList.txt</h2><p><code>src/layers/ffn/CMakList.txt</code></p>
<ul>
<li>å°†<code>ffn.cpp</code>ç¼–è¯‘åˆ°é™æ€åº“ä¸­å¹¶å‘½åä¸º<code>Llamaffn</code><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">add_library(Llamaffn STATIC ffn.cpp)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li>é“¾æ¥<code>Llamaffn</code>æ‰€ç”¨åˆ°çš„å‡½æ•°(<code>launchLinearGemm</code>,<code>launchAct</code>)å¯¹åº”çš„é™æ€åº“(<code>linear</code>,<code>act</code>)<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">target_link_libraries(Llamaffn PUBLIC  
                             -lcudart  
                             -lcudadevrt  
                             act  
                             linear)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<code>examples/cpp/ffn/CMakeList.txt</code></li>
<li>å°†<code>ffn_example.cpp</code>ç¼–è¯‘åˆ°å¯æ‰§è¡Œç›®æ ‡æ–‡ä»¶ä¸­å¹¶å‘½åä¸º<code>ffnExample</code><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">add_executable(ffnExample ffn_example.cpp)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li>é“¾æ¥<code>ffnExample</code>æ‰€ç”¨åˆ°çš„å‡½æ•°(<code>ffn.cpp</code>)å¯¹åº”çš„é™æ€åº“(<code>Llamaffn</code>)</li>
</ul>
<h1 id="Lesson23-llama-layer-weight"><a href="#Lesson23-llama-layer-weight" class="headerlink" title="Lesson23 llama layer weight"></a>Lesson23 llama layer weight</h1><p>è®²è§£äº†ï¼š<br><code>src/weights/llama/layer_weights.h</code><br><code>src/weights/llama/layer_weights.cc</code><br><code>src/weights/llama/CMakelists.txt</code><br><code>src/utils/weights_utils.h</code><br><code>src/utils/weights_utils.cu</code><br><code>src/utils/CMakelists.txt</code></p>
<p>æœ‰weightçš„åœ°æ–¹</p>
<ul>
<li>llama weights<ul>
<li>Embedding</li>
<li>LMhead(æœ¬è´¨ä¸Šä¹Ÿæ˜¯ä¸€ä¸ªlinear)</li>
</ul>
</li>
<li>layer weights<ul>
<li>ç‰¹ç‚¹æ˜¯æœ‰å¾ˆå¤štransformerå †å èµ·æ¥</li>
<li><code>LayerNormWeight&lt;T&gt; attn_norm_weight;</code><ul>
<li>RMSNorm</li>
</ul>
</li>
<li><code>LLaMAattentionWeights&lt;T&gt; self_attn_weight;</code><ul>
<li>QKVgemm</li>
<li>output linear</li>
</ul>
</li>
<li><code>LayerNormWeight&lt;T&gt; ffn_norm_weight;</code><ul>
<li>FusedAddbiasResidualAndRMSNorm</li>
</ul>
</li>
<li><code>LLaMAFFNWeights&lt;T&gt; ffn_weight;</code><ul>
<li>Gate</li>
<li>Up</li>
<li>down</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><code>src/utils/weights_utils.cu</code><br>åœ¨æºæ–‡ä»¶ä¸­çš„æ¨¡æ¿ä¸æ˜¯ä¸€ä¸ªå‡½æ•°ï¼Œåªæœ‰åœ¨å®ä¾‹åŒ–åæ‰æ˜¯ä¸€ä¸ªå‡½æ•°å¹¶ä¸”å¯ä»¥è¿›è¡Œé“¾æ¥<br>å®ç°äº†<code>GPUMalloc</code>å’Œ<code>GPUFree</code>ï¼Œç›®çš„ï¼šåœ¨åˆ†é…å†…å­˜å’Œé‡Šæ”¾å†…å­˜æ—¶è¿›è¡Œæ£€æŸ¥</p>
<p><code>src/weights/llama/layer_weights.h</code><br>å®šä¹‰äº†å››ä¸ªå‡½æ•°</p>
<p><code>src/weights/llama/layer_weights.cc</code><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/57edc5cf50e98d36315d8564f2c5111.png" alt=""><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/39931b88ed57cae22fab8e5d4700e67.jpg" alt=""><br>â‘ <code>attn_norm_weight.gamma</code>ï¼šcontext decoderå’Œself decoderå…±ç”¨</p>
<ul>
<li>ç±»å‹ä¸ºï¼š<code>LayerNormWeight&lt;T&gt;</code></li>
<li>æˆå‘˜æœ‰ï¼š<code>T* gamma</code><br>â‘£<code>ffn_norm_weight.gamma</code>ï¼šcontext decoderå’Œself decoderå…±ç”¨<br>åŒä¸Š</li>
</ul>
<p>â‘¡<code>self_attn_weight.qkv</code>ï¼šcontext decoderå’Œself decoderå…±ç”¨</p>
<ul>
<li>ç±»å‹ä¸ºï¼š<code>BaseWeight&lt;T&gt;</code></li>
<li>æˆå‘˜æœ‰ï¼š<ul>
<li><code>std::vector&lt;int&gt; shape;</code></li>
<li><code>T* data;</code></li>
<li><code>WeightType type;</code></li>
<li><code>T* bias;</code>ä¸ä¸€å®šæ¯ä¸ªweightéƒ½æœ‰<br>â‘¢â‘¤â‘¥â‘¦åŒä¸Š<br>â‘¢<code>self_attn_weight.output</code><br>â‘¤<code>ffn_weight.gate</code>ï¼šcontext decoderå’Œself decoderå…±ç”¨<br>â‘¥<code>ffn_weight.up</code>ï¼šcontext decoderå’Œself decoderå…±ç”¨<br>â‘¦<code>ffn_weight.down</code>ï¼šcontext decoderå’Œself decoderå…±ç”¨</li>
</ul>
</li>
</ul>
<p>åœ¨<code>loadWeights</code>è¿™ä¸€æ­¥ä¸­ï¼Œå¯ä»¥åŠ å…¥å‡çš„æ•°æ®ï¼Œçœå»åŠ è½½æ¨¡å‹è¿™ä¸€æ­¥ï¼Œä¸»è¦ç”¨äºæµ‹è¯•æ€§èƒ½ï¼Œä¸å…³æ³¨ç²¾åº¦</p>
<ul>
<li>æµç¨‹ï¼š<code>cudaMalloc</code>å„ç§d_weightså˜é‡ -&gt; <code>malloc</code>å„ç§h_weightså˜é‡ -&gt; h_weightsè½½å…¥å‡æ•°æ® -&gt; é€šè¿‡<code>cudaMemcpy</code>å°†h_weightså¤åˆ¶åˆ°d_weights -&gt; å†å°†d_weightsèµ‹å€¼ç»™</li>
</ul>
<p><code>freeWights(BaseWeight&lt;T&gt;&amp; weights)</code>ï¼šå°†<code>bias</code>ä¹Ÿç»™é‡Šæ”¾<br>æœ€åææ„å‡½æ•°ä¸­é‡Šæ”¾æ‰€æœ‰çš„ç¼“å­˜</p>
<h1 id="Lesson24-AddBiasResidual"><a href="#Lesson24-AddBiasResidual" class="headerlink" title="Lesson24 AddBiasResidual"></a>Lesson24 AddBiasResidual</h1><p>è®²è§£äº†ï¼š<br><code>src/kernels/add_residual.h</code><br><code>src/kernels/add_residual.cu</code><br><code>tests/unittest/test_residual.cu</code></p>
<p><code>residual</code>æ¥æº<code>FusedAddbiasResidualAndRMSNorm</code>ä¸­<code>FusedAddbiasResidual</code>çš„è¾“å‡º(åŒæ—¶æ˜¯<code>RMSNorm</code>çš„è¾“å…¥)<br><code>decoder_out</code>æ¥æº<code>Down Linear</code></p>
<p><code>decoder_out</code> += <code>residual</code></p>
<ul>
<li><code>decoder_out</code>ï¼š<code>[num_tokens, hidden_units]</code></li>
<li><code>redisual</code>ï¼š<code>[num_tokens, hidden_units]</code>â†åœ¨context decoderä¸­ï¼Œåœ¨self decoderä¸­ç¬¬ä¸€ç»´æ˜¯batch_size</li>
<li>ä½œç”¨ï¼šä»£è¡¨äº†åˆæ­¥èåˆåçš„ç‰¹å¾ï¼Œæ˜¯ä¸€ç§åŒ…å«åŸå§‹ä¿¡æ¯ä¸æ–°ç‰¹å¾çš„ä¿¡æ¯æµï¼Œéšåä¼šä¼ é€’åˆ°å½’ä¸€åŒ–æ“ä½œï¼ˆå¦‚ RMSNormï¼‰ä¸­ï¼Œä»¥ä¾¿ä¸ºä¸‹æ¸¸æ¨¡å—æä¾›ç¨³å®šçš„åˆ†å¸ƒã€‚</li>
</ul>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">Vec_t* dout = reinterpret_cast&lt;Vec_t*&gt;(decoder_out + batch_id * hidden_units);
Vec_t* rsd = reinterpret_cast&lt;Vec_t*&gt;(residual + batch_id * hidden_units);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>å°†<code>decoder_out</code>å’Œ<code>residual</code>è½¬åŒ–ä¸ºå‘é‡åŒ–ç±»å‹ï¼Œå¹¶ä¸”æ¯ä¸ª<code>dout</code>/<code>rsd</code>è¡¨ç¤ºæ¯ä¸€ä¸ªtokenæˆ–batchæ¯ä¸€è¡Œçš„æ•°æ®éƒ½èƒ½è¢«è½¬æ¢ä¸ºVec_tç±»å‹çš„æŒ‡é’ˆ</p>
<p>ä¸€èˆ¬å®ç°(fp32)å’Œç‰¹åŒ–å®ç°(ä¸“ç»™fp16ä½¿ç”¨)çš„åŒºåˆ«<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">for(int i = tid; i &lt; hidden_units/vec_size; i+=blockDim.x){
    dout[i].x += rsd[i].x; // doutæ—¢æ˜¯è¾“å…¥ä¹Ÿæ˜¯è¾“å‡º  
    dout[i].y += rsd[i].y;  
    dout[i].z += rsd[i].z;  
    dout[i].w += rsd[i].w;  
}

for(int i = tid; i &lt; hidden_units/vec_size; i+=blockDim.x){
    dout[i] = __hadd2(dout[i], rsd[i]); // ä¸¤ä¸ªhalf2åšåŠ æ³•  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br><code>for</code>å¾ªç¯ä¿è¯ä¸€ä¸ªblockçš„çº¿ç¨‹èƒ½å¤Ÿéå†å®Œ<code>hidden_units</code>çš„å…ƒç´ <p></p>
<p><code>tests/uinttests/test_residual.cu</code><br>åœ¨<code>CPUresidual</code>ä¸­ï¼Œå¯ä»¥é€šè¿‡AVX-512 kernel + openMPè¿›è¡Œæ€§èƒ½æ”¹å–„</p>
<ul>
<li><code>AVX-512</code>(Advanced Vector Extensions 512)<ul>
<li>SIMD</li>
<li>æ”¯æŒ512ä½å®½çš„å¯„å­˜å™¨å’ŒçŸ¢é‡æ“ä½œ<ul>
<li>æ¯æ¬¡å¤„ç†å¯ä»¥åŠ è½½16ä¸ªæµ®ç‚¹æ•°</li>
</ul>
</li>
</ul>
</li>
<li><code>OpenMP</code>(Open Multi-Processing)<ul>
<li>å¤šçº¿ç¨‹å¹¶è¡Œç¼–ç¨‹æ¥å£ï¼Œç”¨äºåœ¨å…±äº«å†…å­˜ç¯å¢ƒä¸­é€šè¿‡ä»»åŠ¡åˆ’åˆ†å’Œçº¿ç¨‹æ§åˆ¶å®ç°å¹¶è¡ŒåŠ é€Ÿ</li>
</ul>
</li>
</ul>
<h1 id="Lesson25-Context-Decoder"><a href="#Lesson25-Context-Decoder" class="headerlink" title="Lesson25 Context Decoder"></a>Lesson25 Context Decoder</h1><p><code>src/layers/decoder</code></p>
<p><code>Input embedding</code> -&gt; <code>RMSNorm</code> -&gt; <code>Context Attention</code> -&gt; <code>FusedAddbiasResidualAndRMSNorm</code>(å‡¡æ˜¯æ®‹å·®åŠ ï¼Œæ®‹å·®æ¥è‡ªä¸Šä¸€ä¸ª<code>RMSNorm</code>çš„è¾“å…¥) -&gt; <code>FFN</code> -&gt; <code>AddbiasResidual</code></p>
<p>å››ä¸ªä¸­é—´bufferï¼š</p>
<ul>
<li><code>decoder_residual</code>ï¼š(ä¸åŒæ—¶)å­˜å‚¨(ä¸¤ä¸ªåœ°æ–¹çš„)æ®‹å·®</li>
<li><code>attention_mask</code>ï¼šä¿å­˜ç”Ÿæˆçš„<code>CausalMask</code></li>
<li><code>padding_offset</code></li>
<li><code>cum_seqlens</code>ï¼šç´¯ç§¯å¥å­é•¿åº¦ï¼Œå’Œ<code>padding_offset</code>çš„ç”Ÿå‘½å‘¨æœŸä¸€æ ·</li>
</ul>
<p><code>src/layers/decoder/context_decoder.cpp</code><br><code>LlamaContextDecoder&lt;T&gt;::forward</code></p>
<ul>
<li>å…¥å‚ï¼š<ul>
<li><code>TensorMap&amp; input_tensors</code></li>
<li>`const std::vector<llamalayerweight<t>*&gt;&amp; layerWeights</llamalayerweight<t></li>
<li><code>TensorMap&amp; output_tensors</code></li>
<li><code>LLaMAAttentionDynParams&amp; dyn_params</code><br>â‘ å†…å­˜åˆ†é…ï¼šå¯¼å…¥åŠ¨æ€å˜é‡å¹¶åˆ†é…å››ä¸ªä¸­é—´bufferçš„å†…å­˜<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">allocForForward(dyn_params);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
â‘¡è·å¾—åç§»<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">Tensor* seq_lens = input_tensors["input_length"];  
launchCalPaddingoffset(padding_offset, cum_seqlens, seq_lens-&gt;as&lt;int&gt;());<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li>
</ul>
</li>
<li>å…¥å‚ï¼š<ul>
<li><code>TensorWrapper&lt;int&gt;* padding_offset</code></li>
<li><code>TensorWrapper&lt;int&gt;* cum_seqlens</code>ï¼Œç´¯ç§¯çš„å¥å­é•¿åº¦ï¼Œæ˜¯æ‰€æœ‰batchçš„ç´¯ç§¯</li>
<li><code>TensorWrapper&lt;int&gt;* input_lengths</code>ï¼Œæ¯ä¸ªå¥å­çš„è¾“å…¥é•¿åº¦<br>â‘¢è·å–æ©ç é•¿åº¦<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">Tensor* context_length = input_tensors["context_length"];  
launchBuildCausalMasks(attention_mask, seq_lens-&gt;as&lt;int&gt;(), context_length-&gt;as&lt;int&gt;());<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li>
</ul>
</li>
<li>å…¥å‚ï¼š<ul>
<li><code>TensorWrapper&lt;T&gt;* mask</code></li>
<li><code>TensorWrapper&lt;int&gt;* q_lens</code>ï¼Œæ¯ä¸ªå¥å­çš„è¾“å…¥é•¿åº¦</li>
<li><code>TensorWrapper&lt;int&gt;* k_lens</code>ï¼Œæ¯ä¸ªå¥å­çš„ä¸Šä¸‹æ–‡é•¿åº¦<br>â‘£æ­å»º32å±‚context decoder layerå¹¶è¿›è¡Œcontext attention<br>ä¸ºå•¥ï¼šæœâ€œç–‘é—®â€<br>1)ä»å‡½æ•°è¾“å…¥çš„<code>input_tensors</code>å’Œ<code>output_tensors</code>è·å¾—ç›¸åº”é”®å€¼å¯¹å¹¶å–åœ°å€ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºç©º<br>2)åˆå§‹åŒ–<code>ctx_attn_inputs</code>å’Œ<code>ctx_attn_outpus</code>é”®å€¼å¯¹ï¼Œè¿™é‡ŒæŒ‡çš„æ˜¯æ¯ä¸€å±‚çš„è¾“å…¥å’Œè¾“å‡º<br>3)è¿›è¡Œ32å±‚Layerçš„context attention</li>
</ul>
</li>
<li>ä»forå¾ªç¯çš„å˜é‡è·å–<code>layer_id</code>å¹¶æ›´æ–°åˆ°<code>ctx_attn_inputs[layer_id]</code>ä¸­</li>
<li>è·å–context attntionçš„è¾“å…¥</li>
<li>è¿›è¡Œcontext attentionï¼Œè¾“å‡ºä¸º<code>ctx_attn_outputs</code></li>
<li>è¿›è¡ŒFusedAddBiasResidualRMSNormï¼Œè¾“å‡ºä¸º<code>decoder_output</code>(ctx_attn_outputs[â€œattention_outputâ€]çš„æŒ‡é’ˆ)</li>
<li>è¿›è¡Œffnï¼Œè¾“å…¥ä¸º<code>decoder_output</code>ï¼Œè¾“å‡ºç›´æ¥å¤ç”¨è¾“å…¥çš„å†…å­˜åŒº</li>
<li>è¿›è¡ŒAddResidualï¼Œè¯¥kernelåœ¨å®ç°æ—¶ï¼Œæ®‹å·®åŠ çš„ç»“æœæ”¾åœ¨decoder_outä¸Š</li>
<li>æŠŠå½“å‰Layerçš„è¾“å‡ºä½œä¸ºä¸‹ä¸€å±‚Layerçš„è¾“å…¥ï¼Œç›´æ¥ç”¨å½“å‰Layerçš„è¾“å‡º<code>decoder_output</code>æ›´æ–°åˆ°keyä¸ºâ€attention_inputâ€çš„valueä¸­</li>
</ul>
<h1 id="Lesson26-Self-Decoder"><a href="#Lesson26-Self-Decoder" class="headerlink" title="Lesson26 Self Decoder"></a>Lesson26 Self Decoder</h1><p>å¾…è§£å†³ï¼šllama2æ˜¯ä¸åŒ…å«ç¬¬ä¸€ä¸ªaddbiasçš„ï¼Œæ‰€ä»¥åªå‰©ä¸‹RoPEï¼Œå› ä¸ºæ—‹è½¬ç¼–ç çš„æ—‹è½¬å¤§å°æ˜¯64ï¼Œæ‰€ä»¥ä¸èƒ½èåˆåˆ°Fused Masked self Attetioné‡Œï¼Œå¦‚æœæ˜¯2å°±å¯ä»¥</p>
<ul>
<li>RoPEæ²¡æœ‰å‡ºç°ï¼<ul>
<li>æœ‰çš„ï¼Œæ˜¯åœ¨Fused Masked self Attentioné‡Œ</li>
</ul>
</li>
</ul>
<p>é™¤äº†ä¸€äº›ä¸éœ€è¦çš„å˜é‡ï¼Œæ­¥éª¤æ–¹æ³•å’Œcontext decoderçš„æ­å»ºå·®ä¸å¤š</p>
<h1 id="Lesson27-llama-weights"><a href="#Lesson27-llama-weights" class="headerlink" title="Lesson27 llama weights"></a>Lesson27 llama weights</h1><p><code>tools/convert_downloaded_llama_weights.py</code><br><code>tools/weights_conver.py</code></p>
<ul>
<li>åœ¨pythonä¸­<ul>
<li>ä¸‹è½½æ¨¡å‹</li>
<li>è½¬æ¢<ul>
<li>å°†åŸæœ¬çš„.pthå½¢å¼çš„æƒé‡ï¼Œå¯¹åº”è¾“å‡ºåˆ°æ¯ä¸€å±‚çš„æ¯ä¸€ä¸ªç±»å‹çš„(qkvgemm, gate, down, upç­‰ç­‰çš„)æƒé‡</li>
<li>åˆå¹¶qkvçš„æƒé‡ï¼Œåˆå¹¶gateå’Œupçš„æƒé‡ï¼ŒæŠŠæ‰€æœ‰æƒé‡æ–‡ä»¶è½¬ä¸º.binæ ¼å¼çš„ï¼Œå³è½¬ä¸ºäºŒè¿›åˆ¶</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><code>src/weights/llama/llama_weights.cc</code><br><code>src/weights/llama/llama_weights.h</code></p>
<ul>
<li>è¯»å–ä»pythonæ–‡ä»¶ä¸­å¾—åˆ°çš„æƒé‡ï¼Œå¹¶èµ‹å€¼ç»™å·²åˆ†é…å¥½æ˜¾å­˜çš„æŒ‡é’ˆ</li>
<li>å››ä¸ªpublicæˆå‘˜(llama weights)<ul>
<li><code>llama_layer_weight</code>ï¼Œæœ‰<code>num_layer</code>å±‚</li>
<li><code>out_rmsnorm_weight</code></li>
<li><code>post_decoder_embedding_weight</code> (samplingçš„LMhead)</li>
<li><code>pre_decoder_embedding_weight</code></li>
</ul>
</li>
</ul>
<p><code>src/utils/weights_utils.cc</code><br><code>src/utils/weights_utils.h</code></p>
<ul>
<li>å½“pyhtonè½¬æ¢å®Œçš„æƒé‡æ ¼å¼ä¸ç›®æ ‡æ ¼å¼ä¸ä¸€è‡´æ—¶ï¼Œå¦‚halfå’Œfloatï¼Œåˆ™è¿›è¡Œè½¬æ¢å†åŠ è½½äºŒè¿›åˆ¶æ–‡ä»¶</li>
</ul>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">llama_layer_weight.reserve(num_layer);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<ul>
<li>vector.push_backå’Œvector.reserveçš„åŒºåˆ«<ul>
<li>push_backï¼Œ2-&gt;4-&gt;6-&gt;8-&gt;16-&gt;32-&gt;â€¦ï¼Œå½“å‘é‡ä¸­æœ‰2ä¸ªå…ƒç´ ï¼Œpush_backåˆ°3ä¸ªå…ƒç´ æ—¶ï¼Œvectoråœ°å€è‡ªåŠ¨é‡æ–°åˆ†é…å¹¶ä¸”å®¹é‡å˜ä¸º4ï¼Œåç»­çš„å¢åŠ åŒç†</li>
<li>reserveï¼ŒæŒ‡å®šå®¹é‡ï¼Œä¸ä¼šè‡ªåŠ¨é‡æ–°åˆ†é…</li>
</ul>
</li>
</ul>
<h1 id="Lesson28-llamaç±»"><a href="#Lesson28-llamaç±»" class="headerlink" title="Lesson28 llamaç±»"></a>Lesson28 llamaç±»</h1><p>æ›´é«˜å±‚æ¬¡çš„æŠ½è±¡</p>
<p><code>std::function&lt;è¿”å›å€¼ç±»å‹(å‚æ•°åˆ—è¡¨)&gt;</code>å®šä¹‰äº†ä¸€ä¸ªå¯è°ƒç”¨å¯¹è±¡çš„ç­¾å</p>
<ul>
<li>å¯è°ƒç”¨å¯¹è±¡çš„ç­¾ååŒ…æ‹¬è¿”å›å€¼ç±»å‹å’Œå‚æ•°åˆ—è¡¨<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/Pasted%20image%2020250115113416.png" alt=""></li>
</ul>
<p><code>src/models/llama/llama.cpp</code><br><code>src/models/llama/llama.h</code></p>
<p><code>std::string Llama&lt;T&gt;::Response(const std::vector&lt;std::string&gt; &amp;input, CallBack PrintRes)</code></p>
<ul>
<li>å…¥å‚<ul>
<li><code>input</code>ï¼šç”¨æˆ·è¾“å…¥çš„ç±»å‹ä¸ºå­—ç¬¦ä¸²å‘é‡çš„å¥å­</li>
<li><code>PrintRes</code>ï¼šæ‰“å°ç»“æœ</li>
</ul>
</li>
<li><code>Encode</code><ul>
<li><code>input</code>ã€<code>history_str</code>ã€<code>total_str</code></li>
<li>ä¸Šé¢è¿™ä¸‰ä¸ªå’Œ<code>MakeInput</code>å‡½æ•°æœ‰å…³</li>
<li><code>MakeInput</code>ä¸­çš„<code>ret</code>å°±æ˜¯<code>Response</code>ä¸­çš„<code>input</code><ul>
<li><code>total_str</code>æ˜¯æ‰€æœ‰è½®æ¬¡çš„inputï¼ŒåŒ…æ‹¬ç°åœ¨å’Œä¹‹å‰çš„</li>
<li><code>history_str</code>æ˜¯ä¹‹å‰è½®æ¬¡çš„input</li>
<li><code>input</code>æ˜¯ç°åœ¨è½®æ¬¡çš„input</li>
</ul>
</li>
<li>å¾—åˆ°ä¸‰è€…çš„token indexs</li>
<li>è¿™ä¸‰è€…çš„é•¿åº¦æ˜¯<code>int_params_first_token</code>å­—å…¸ä¸­çš„å€¼</li>
</ul>
</li>
<li><code>attn_dyn_params</code> llamaç±»æ¨¡å‹é‡ŒåŠ¨æ€æ”¹å˜çš„å˜é‡<ul>
<li><code>batch_size</code>ï¼šç¡¬å†™ä¸º1</li>
<li><code>num_tokens</code>ï¼šå½“å‰è¾“å…¥çš„é•¿åº¦</li>
<li><code>max_q_len</code>ï¼šbatchä¸­qçš„æœ€å¤§é•¿åº¦ï¼Œå› ä¸ºä¸€ä¸ªbatchåªæœ‰ä¸€ä¸ªå¥å­ï¼Œæ‰€ä»¥ç­‰äºnum_tokens</li>
<li><code>max_k_len</code>ï¼šåŠ¨æ€æœ€å¤§ä¸Šä¸‹æ–‡ï¼Œ<code>step</code>çš„å€¼ä¸å…¶ç›¸åŒ(åœ¨self decoderä¸­ç”¨åˆ°)</li>
</ul>
</li>
<li>è·å¾—æ‰€æœ‰è½®æ¬¡çš„token string<ul>
<li>è‡ªå®šä¹‰<code>self_token_limit</code></li>
<li><code>firstTokenGen</code><ul>
<li>å…¥å‚ï¼š<code>attn_dyn_params</code>ã€<code>int_params_first_token</code></li>
<li><code>InitializeForContextDecoder</code><ul>
<li>ä¼ å…¥æ‰€æœ‰è½®æ¬¡ã€ä¹‹å‰è½®æ¬¡å’Œç°åœ¨è½®æ¬¡åˆ°CPUä¸­ï¼Œå†å¤åˆ¶åˆ°GPUä¸­</li>
</ul>
</li>
<li><code>inputEmbedding</code><ul>
<li>å¾—åˆ°è¾“å…¥çš„å¥å­å¯¹åº”çš„tokenåœ¨embed tableé‡Œçš„è¯å‘é‡</li>
</ul>
</li>
<li>åŒ…è£…<code>decoder_inputs</code>å’Œ<code>decoder_outputs</code>è¿™ä¸¤ä¸ªTensorMap</li>
<li>è¿›è¡Œæ¨ç†</li>
<li>è¿›è¡ŒRMSNorm</li>
<li>è¿›è¡ŒLMHeadå’ŒtopkSample<ul>
<li>LMHead<ul>
<li>å¦‚æœæ˜¯context decoder<ul>
<li>å–è¾“å‡ºé‡Œçš„æœ€åä¸€ä¸ªtokenä½œä¸ºLMHeadçš„è¾“å…¥ï¼Œç»è¿‡çŸ©é˜µç›¸ä¹˜å¾—åˆ°<code>probs</code>ç»´åº¦ä¸ºï¼š<code>[1, vocab_size]</code>ï¼Œå°±çŸ¥é“è¿™å‡ ä¸ªvocabçš„å¯èƒ½æ€§</li>
</ul>
</li>
<li>å¦‚æœæ˜¯self decoder<ul>
<li><code>decoder_output</code>å°±æ˜¯å”¯ä¸€çš„tokenï¼Œç›´æ¥ä½œä¸ºLMHeadçš„è¾“å…¥</li>
</ul>
</li>
</ul>
</li>
<li>topkSample<br>  * </li>
</ul>
</li>
</ul>
</li>
<li><code>continueTokenGen</code></li>
</ul>
</li>
</ul>
<h1 id="Lesson29"><a href="#Lesson29" class="headerlink" title="Lesson29"></a>Lesson29</h1><p>æä¾›æ¥å—ç”¨æˆ·è¾“å…¥æˆ–è€…promtçš„æ¥å£<br>å®ç°å¤§å°è£…çš„APIï¼Œåˆ›å»ºC++ç±»</p>
<p><code>std::unique_ptr</code></p>
<ul>
<li>ç‹¬å æ‰€æœ‰æƒï¼Œä¸èƒ½è¢«å…±äº«</li>
<li>è‡ªåŠ¨é‡Šæ”¾å†…å­˜</li>
<li>ä¸å¯ä»¥è¢«å¤åˆ¶ï¼Œæ‰€æœ‰æƒå¯ä»¥è½¬è®©ï¼Œè½¬è®©ååŸæ¥çš„å˜ä¸ºç©ºæŒ‡é’ˆ</li>
</ul>
<p><code>LLMengine-learn/user_entry.cpp</code></p>
<ul>
<li>ä¼ å…¥æ¨¡å‹å’Œtokenizerçš„åœ°å€ï¼Œç›´æ¥è°ƒç”¨Response</li>
</ul>
<h1 id="debugæ€è·¯"><a href="#debugæ€è·¯" class="headerlink" title="debugæ€è·¯"></a>debugæ€è·¯</h1><ol>
<li>æ‰“å°/ä¿å­˜ä¸­é—´æ•°æ®ï¼šå°†è¾“å‡ºå­˜ä¸ºä¸€ä¸ª.binæ–‡ä»¶ï¼Œå†ä½¿ç”¨<code>std::ifstream</code>è¯»å–ï¼Œå¯ä»¥é€ä¸€æ¯”è¾ƒ(ä¸huggingfaceçš„)ç»“æœ</li>
</ol>
<ul>
<li>fp32ä¸¤ä¸ªç»“æœçš„è¯¯å·®å¤§äº$10^{-6}$å°±è¯´æ˜æœ‰è¯¯å·®</li>
</ul>
<ol>
<li><code>DeviceSynAndCheckCudaError</code></li>
</ol>
<ul>
<li>æ£€æŸ¥æœ‰æ²¡æœ‰è¿è¡Œæ—¶é”™è¯¯</li>
<li>æœ‰<code>cudaDeviceSynchronize()</code>ï¼šCPUç­‰å¾…GPUä¸Šæ‰€æœ‰ä»»åŠ¡å®Œæˆï¼Œå› æ­¤æœ€å¥½åœ¨ç¡®ä¿é¡¹ç›®æ²¡æœ‰é—®é¢˜æ—¶å…³æ‰</li>
</ul>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">// CMakeList.txtä¸­
option(PERF
	  "measure the model inference performance"
	  OFF)
if(PERF)
	add_compile_option(-DPRINT_DATA)
endif()
// context_attention.cppä¸­
#ifndef PERF
	DeviceSynAndCheckCudaError();
#else
#endif<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>CMakeé€‰é¡¹å®šä¹‰ï¼š<code>option(é€‰é¡¹åç§° â€œé€‰é¡¹æè¿°â€ é»˜è®¤å€¼)</code></li>
<li>å¦‚æœ<code>PERF</code>æ˜¯ONï¼Œé‚£ä¹ˆåœ¨ç¼–è¯‘é€‰é¡¹ä¸­æ·»åŠ <code>-DPRINT_DATA</code>ï¼Œå³åœ¨ç¼–è¯‘æ—¶æ·»åŠ ä¸€ä¸ªå®<code>PRINT_DATA</code></li>
<li><code>cmake .. -DPERF=ON</code>æ—¶æ‰“å¼€ä¸è¿›è¡ŒCheck(å³ä¸æ‰“å¼€â€œæ£€æŸ¥è¿è¡Œæ—¶é”™è¯¯â€œ)</li>
</ul>
<ol>
<li>PRINT_DATA</li>
</ol>
<ul>
<li>é€šå¸¸åœ¨é¦–å…ˆè¾“å…¥çš„kernelé‡Œéœ€è¦ï¼Œæ£€æŸ¥æ”¾åœ¨lanuché‡Œ</li>
</ul>
<h1 id="å…¶ä»–"><a href="#å…¶ä»–" class="headerlink" title="å…¶ä»–"></a>å…¶ä»–</h1><ol>
<li>emmé…å®Œannacondaä¹‹åå†…å­˜å¿«ç‚¸äº†ï¼ŒæŸ¥çœ‹ä»»åŠ¡ç®¡ç†å™¨å‘ç°æ˜¯vmmemçš„é—®é¢˜ï¼Œæ˜¯è™šæ‹Ÿæœºèµ„æºåˆ†é…çš„é—®é¢˜ï¼Œè¿™ä¸ªé—®é¢˜ä¸Šç½‘æŸ¥è§£å†³æ–¹æ³•ï¼Œä¸å¤æ‚</li>
</ol>
<ul>
<li><code>win+R</code>ï¼Œè¾“å…¥<code>%UserProfile%</code></li>
<li>å¦‚æœæ²¡æœ‰<code>.wslconfig</code>ç»“å°¾çš„æ–‡ä»¶ï¼Œå¯ä»¥æ–°å»ºä¸€ä¸ªï¼Œå¯ä»¥å«<code>Vmmem.wslconfig</code></li>
<li>æ·»åŠ ä»¥ä¸‹å†…å®¹<pre class="line-numbers language-none"><code class="language-none">#.wslconfig
[wsl2]
memory=3GB //åˆ†é…ç»™WSLå†…å­˜ï¼Œå¯ä»¥æ˜¯å†…å­˜çš„1/3æˆ–1/4
swap=0     //è®¾ç½®äº¤æ¢åˆ†åŒº
localhostForwarding=true<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li>é‡å¯WSLï¼š<code>win+R</code>ï¼Œè¾“å…¥<code>services.msc</code>ï¼Œæ‰¾åˆ°<code>LxssManager</code>ï¼Œé‡æ–°å¯åŠ¨</li>
</ul>
<ol>
<li>è¿™ä¸ªvmmemå¥½åƒæ˜¯ä¸ªç¡¬éª¨å¤´å•Šï¼<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/166102340">wslå¯¼è‡´vmmemå ç”¨é«˜è§£å†³åŠæ³• - çŸ¥ä¹</a><br>(ğŸ‘†æ–‡ä¸­å€Ÿé‰´<a target="_blank" rel="noopener" href="https://github.com/microsoft/WSL/issues/4166">WSL 2 consumes massive amounts of RAM and doesnâ€™t return it - github.com</a>)<br>æŒ‰ç…§è¿™ä¸ªæ–¹æ³•åšåˆ°æœ€åä¸€æ­¥å‘ç°ğŸ‘‡<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/Pasted%20image%2020241011223651.png" alt="|500"><br>ç„¶åå°±æ‰¾åˆ°äº†è¿™ä¸ªæ–¹æ³•ğŸ‘‡<br><a target="_blank" rel="noopener" href="https://bbs.csdn.net/topics/396240104">drop_cashesæ— æ³•æ“ä½œ no such file or directory-CSDNç¤¾åŒº</a></li>
</ol>
<ul>
<li>å…ˆ<code>sudo su</code>è¿›å…¥root</li>
<li>è¾“å…¥<code>echo 3 &gt; /proc/sys/vm/drop_caches</code></li>
<li>ç„¶åå†å›å»<code>sudo stat -c '%y' /root/drop_caches_last_run</code>å°±èƒ½çœ‹åˆ°æ¸…é™¤ç¼“å­˜çš„è®°å½•äº†</li>
</ul>
<ol>
<li>vscodeç–¯ç‹‚çˆ†çº¢ï¼Œè½¬clionå»äº†</li>
</ol>
<h2 id="è½¯ä»¶æŠ½è±¡èµ„æºå’Œç¡¬ä»¶èµ„æºçš„å¯¹åº”å…³ç³»"><a href="#è½¯ä»¶æŠ½è±¡èµ„æºå’Œç¡¬ä»¶èµ„æºçš„å¯¹åº”å…³ç³»" class="headerlink" title="è½¯ä»¶æŠ½è±¡èµ„æºå’Œç¡¬ä»¶èµ„æºçš„å¯¹åº”å…³ç³»"></a>è½¯ä»¶æŠ½è±¡èµ„æºå’Œç¡¬ä»¶èµ„æºçš„å¯¹åº”å…³ç³»</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41554005/article/details/119765334">ã€GPUç»“æ„ä¸CUDAç³»åˆ—4ã€‘GPUå­˜å‚¨èµ„æºï¼šå¯„å­˜å™¨ï¼Œæœ¬åœ°å†…å­˜ï¼Œå…±äº«å†…å­˜ï¼Œç¼“å­˜ï¼Œæ˜¾å­˜ç­‰å­˜å‚¨å™¨ç»†èŠ‚_gpuå†…å¯„å­˜å™¨ - CSDN</a></p>
<h2 id="å¦‚ä½•é«˜æ•ˆè®¿é—®gpuå…¨å±€å†…å­˜"><a href="#å¦‚ä½•é«˜æ•ˆè®¿é—®gpuå…¨å±€å†…å­˜" class="headerlink" title="å¦‚ä½•é«˜æ•ˆè®¿é—®gpuå…¨å±€å†…å­˜"></a>å¦‚ä½•é«˜æ•ˆè®¿é—®gpuå…¨å±€å†…å­˜</h2><p>(è§£ç­”<em>ä¸ºä»€ä¹ˆvæ˜¯æŒ‰è¡Œè¿ç»­åˆ†å¸ƒå’Œä¸ºä»€ä¹ˆè¦é‚£æ ·è®¡ç®—qkvgemm</em>)</p>
<ul>
<li><p>è¶Šé è¿‘CPUæ’åºï¼šå¯„å­˜å™¨Register &gt; ç¼“å­˜Cache &gt; å†…å­˜Memory &gt; ç¡¬ç›˜</p>
</li>
<li><p>â€œCå’ŒCUDAä¸­çš„å¤šç»´æ•°ç»„å…ƒç´ æ˜¯æ ¹æ®<strong>è¡Œä¼˜å…ˆ</strong>çº¦å®šæ”¾ç½®åœ¨çº¿æ€§å¯»å€çš„å†…å­˜ç©ºé—´ä¸­çš„â€</p>
<ul>
<li>éæŒ‰è¡Œæ’åˆ—ï¼š<strong>cublas API</strong>æ¥å—çš„è¾“å…¥ä»¥åŠè¾“å‡ºçš„å†…å­˜æ’å¸ƒå…¨éƒ¨éƒ½é»˜è®¤ä¸º<strong>åˆ—ä¸»åº</strong></li>
</ul>
</li>
<li>â€œå½“çº¿ç¨‹è®¿é—®çŸ©é˜µæ•°æ®æ—¶ï¼Œå¦‚æœ<strong>æ•°æ®æ’åˆ—çš„é¡ºåºä¸çº¿ç¨‹è®¿é—®é¡ºåºåŒ¹é…</strong>ï¼Œå†…å­˜å¸¦å®½çš„åˆ©ç”¨ç‡ä¼šæ›´é«˜â€ã€‚è¿™é‡Œåº”è¯¥å’Œwarpæœ‰å…³<ul>
<li>Fused SelfDecoder Attention kernelä¸­ï¼Œ<code>block</code>çš„å¤§å°æ˜¯<code>head_size</code>ï¼Œ<code>grid</code>çš„å¤§å°æ˜¯<code>head_num*batch_size</code><br>ğŸ‘‡æ ¹æ®çº¿ç¨‹è®¿é—®é¡ºåºåŒ¹é…(ä»blockIdxåˆ°threadIdxï¼ŒthreadIdxçš„è·¨åº¦æ˜¯é¡ºåºçš„)<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">cache_offset = blockIdx.y * kv_head_num * max_seq_len * head_size +
			blockIdx.x/(head_num/kv_head_num)*max_seq_len*head_size+
			threadIdx.x * vec_size;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/c22b854c31b08ae192c8e310cb4a8fa%201.jpg" alt=""><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/8ea8ba5aa4aa7e6763e069de3b29f14.jpg" alt=""></li>
</ul>
</li>
<li>ä¸€æ¬¡æ•°æ®ä¼ è¾“çš„æ•°æ®é‡é»˜è®¤æƒ…å†µä¸‹æ˜¯32ä¸ªå­—èŠ‚</li>
<li>$åˆå¹¶åº¦=\dfrac{çº¿ç¨‹æŸè¯·æ±‚çš„å­—èŠ‚æ•°}{ç”±è¯¥è¯·æ±‚å¯¼è‡´çš„æ‰€æœ‰æ•°æ®ä¼ è¾“å¤„ç†çš„å­—èŠ‚æ•°}$</li>
<li>æ•°æ®ä¼ è¾“å¯¹æ•°æ®åœ°å€çš„è¦æ±‚ï¼šåœ¨ä¸€æ¬¡æ•°æ®ä¼ è¾“ä¸­ï¼Œä»å…¨å±€å†…å­˜è½¬ç§»åˆ°L2ç¼“å­˜çš„ä¸€ç‰‡å†…å­˜é¦–åœ°å€ä¸€å®šæ˜¯ä¸€ä¸ªæœ€å°é¢—ç²’åº¦(è¯¥ä¾‹å­æ˜¯32)çš„æ•´æ•°å€ã€‚<ul>
<li>ä¸€æ¬¡ä¼ è¾“åªå–0~31ã€32~63ã€64~95â€¦â€¦<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/808198b24f332fb853b67f74add48cd.jpg" alt=""><br>ğŸ‘‡å·¦è¡Œå³åˆ—å¯¼è‡´è·¨è¶Šå¼çš„éåˆå¹¶è®¿é—®<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/7b1158679c6058fdd34cdd461ad7d17.jpg" alt=""></li>
</ul>
</li>
</ul>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        æ–‡ç« ä½œè€…:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">WB</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        æ–‡ç« é“¾æ¥:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://wabbybabb0.github.io/2024/10/08/llm2-tui-li-yin-qing/">https://wabbybabb0.github.io/2024/10/08/llm2-tui-li-yin-qing/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        ç‰ˆæƒå£°æ˜:
                    </i>
                </span>
                <span class="reprint-info">
                    æœ¬åšå®¢æ‰€æœ‰æ–‡ç« é™¤ç‰¹åˆ¥å£°æ˜å¤–ï¼Œå‡é‡‡ç”¨
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    è®¸å¯åè®®ã€‚è½¬è½½è¯·æ³¨æ˜æ¥æº
                    <a href="/about" target="_blank">WB</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>å¤åˆ¶æˆåŠŸï¼Œè¯·éµå¾ªæœ¬æ–‡çš„è½¬è½½è§„åˆ™</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">æŸ¥çœ‹</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                                <a href="/tags/CUDA/">
                                    <span class="chip bg-color">CUDA</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>å¾®ä¿¡æ‰«ä¸€æ‰«å³å¯åˆ†äº«ï¼</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">èµ</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">ç»™WBæ¥åŒ…è¾£æ¡</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">æ”¯ä»˜å®</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">å¾® ä¿¡</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="æ”¯ä»˜å®æ‰“èµäºŒç»´ç ">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="å¾®ä¿¡æ‰“èµäºŒç»´ç ">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    
        <div class="card" data-aos="fade-up">
    <div id="utteranc-container" class="card-content">
        <script src="https://utteranc.es/client.js"
                repo="Wabbybabb0/commit-utterance"
                issue-term="pathname"
                theme="github-light"
                crossorigin="anonymous"
                async>
        </script>
    </div>
</div>
    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;ä¸Šä¸€ç¯‡</div>
            <div class="card">
                <a href="/2025/03/19/mla/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/22.jpg" class="responsive-img" alt="Multi-head Latent Attentionæ¨¡å‹ç†è§£">
                        
                        <span class="card-title">Multi-head Latent Attentionæ¨¡å‹ç†è§£</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            ä½œä¸ºFlashMLAçš„æ ¸å¿ƒä¹‹ä¸€ï¼ŒMLAè®¾è®¡äº†åœ¨ä¿æŒæ€§èƒ½çš„åŒæ—¶ç€é‡å‡å°‘KV Cacheçš„attentionæœºåˆ¶ï¼Œååˆ†å€¼å¾—ç»†ç©¶ï¼
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/" class="post-category">
                                    é«˜æ€§èƒ½è®¡ç®—
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/GPU/">
                        <span class="chip bg-color">GPU</span>
                    </a>
                    
                    <a href="/tags/DeepSeek/">
                        <span class="chip bg-color">DeepSeek</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                ä¸‹ä¸€ç¯‡&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2024/08/02/cudac/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/10.png" class="responsive-img" alt="CUDAç¼–ç¨‹ï¼šåŸºç¡€ä¸å®è·µå­¦ä¹ ç¬”è®°">
                        
                        <span class="card-title">CUDAç¼–ç¨‹ï¼šåŸºç¡€ä¸å®è·µå­¦ä¹ ç¬”è®°</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            æ¨Šå“²å‹‡è€å¸ˆçš„CUDAç¼–ç¨‹æ•™ç¨‹åŒ…æ‹¬CUDAç¼–ç¨‹çš„è¯­æ³•çŸ¥è¯†ã€ä¼˜åŒ–ç­–ç•¥åŠç¨‹åºå¼€å‘å®è·µï¼Œå¯¹æ–°æ‰‹å¾ˆå‹å¥½ï¼
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-08-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/" class="post-category">
                                    é«˜æ€§èƒ½è®¡ç®—
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/CUDA/">
                        <span class="chip bg-color">CUDA</span>
                    </a>
                    
                    <a href="/tags/GPU/">
                        <span class="chip bg-color">GPU</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + 'æ¥æº: Wabbybabboçš„æ‘¸é±¼åœ£åœ°<br />'
            + 'æ–‡ç« ä½œè€…: Wabbybabbo<br />'
            + 'æ–‡ç« é“¾æ¥: <a href="' + url + '">' + url + '</a><br />'
            + 'æœ¬æ–‡ç« è‘—ä½œæƒå½’ä½œè€…æ‰€æœ‰ï¼Œä»»ä½•å½¢å¼çš„è½¬è½½éƒ½è¯·æ³¨æ˜å‡ºå¤„ã€‚';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- ä»£ç å—åŠŸèƒ½ä¾èµ– -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- ä»£ç è¯­è¨€ -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- ä»£ç å—å¤åˆ¶ -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- ä»£ç å—æ”¶ç¼© -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;ç›®å½•</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC æ‚¬æµ®æŒ‰é’®. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* ä¿®å¤æ–‡ç« å¡ç‰‡ div çš„å®½åº¦. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // åˆ‡æ¢TOCç›®å½•å±•å¼€æ”¶ç¼©çš„ç›¸å…³æ“ä½œ.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    




<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="8590828427"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2023-2025</span>
            
            <span id="year">2023</span>
            <a href="/about" target="_blank">Wabbybabbo</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;æ€»è®¿é—®é‡:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;æ¬¡
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;æ€»è®¿é—®äººæ•°:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;äºº
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Wabbybabb0" class="tooltipped" target="_blank" data-tooltip="è®¿é—®æˆ‘çš„GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:Wabbybabb0@outlook.com" class="tooltipped" target="_blank" data-tooltip="é‚®ä»¶è”ç³»æˆ‘" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=497056512" class="tooltipped" target="_blank" data-tooltip="QQè”ç³»æˆ‘: 497056512" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS è®¢é˜…" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- æœç´¢é®ç½©æ¡† -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;æœç´¢</span>
            <input type="search" id="searchInput" name="s" placeholder="è¯·è¾“å…¥æœç´¢çš„å…³é”®å­—"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- å›åˆ°é¡¶éƒ¨æŒ‰é’® -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

	
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
