<!DOCTYPE HTML>
<html lang="zh-CN">


<head>
    <meta charset="utf-8">
    <meta name="keywords" content="LLM2框架搭建过程, Wabbybabbo的摸鱼圣地">
    <meta name="description" content="上班？下班！">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no">
    <meta name="renderer" content="webkit|ie-stand|ie-comp">
    <meta name="mobile-web-app-capable" content="yes">
    <meta name="format-detection" content="telephone=no">
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <!-- Global site tag (gtag.js) - Google Analytics -->


    <title>LLM2框架搭建过程 | Wabbybabbo的摸鱼圣地</title>
    <link rel="icon" type="image/png" href="/grape200x200.png">

    <link rel="stylesheet" type="text/css" href="/libs/awesome/css/all.css">
    <link rel="stylesheet" type="text/css" href="/libs/materialize/materialize.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/aos/aos.css">
    <link rel="stylesheet" type="text/css" href="/libs/animate/animate.min.css">
    <link rel="stylesheet" type="text/css" href="/libs/lightGallery/css/lightgallery.min.css">
    <link rel="stylesheet" type="text/css" href="/css/matery.css">
    <link rel="stylesheet" type="text/css" href="/css/my.css">

    <script src="/libs/jquery/jquery.min.js"></script>

<meta name="generator" content="Hexo 6.3.0">
<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
<link rel="alternate" href="/atom.xml" title="Wabbybabbo的摸鱼圣地" type="application/atom+xml">
</head>




<body>
    <header class="navbar-fixed">
    <nav id="headNav" class="bg-color nav-transparent">
        <div id="navContainer" class="nav-wrapper container">
            <div class="brand-logo">
                <a href="/" class="waves-effect waves-light">
                    
                    <img src="/medias/grape200x200.png" class="logo-img" alt="LOGO">
                    
                    <span class="logo-span">Wabbybabbo的摸鱼圣地</span>
                </a>
            </div>
            

<a href="#" data-target="mobile-nav" class="sidenav-trigger button-collapse"><i class="fas fa-bars"></i></a>
<ul class="right nav-menu">
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/" class="waves-effect waves-light">
      
      <i class="fas fa-sharp fa-solid fa-fish" style="zoom: 0.6;"></i>
      
      <span>主页</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/tags" class="waves-effect waves-light">
      
      <i class="fas fa-soild fa-tags" style="zoom: 0.6;"></i>
      
      <span>标签</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/categories" class="waves-effect waves-light">
      
      <i class="fas fa-bookmark" style="zoom: 0.6;"></i>
      
      <span>分类</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/archives" class="waves-effect waves-light">
      
      <i class="fas fa-archive" style="zoom: 0.6;"></i>
      
      <span>档案</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="/about" class="waves-effect waves-light">
      
      <i class="fas fa-user-circle" style="zoom: 0.6;"></i>
      
      <span>关于</span>
    </a>
    
  </li>
  
  <li class="hide-on-med-and-down nav-item">
    
    <a href="" class="waves-effect waves-light">

      
      <i class="fas fa-list" style="zoom: 0.6;"></i>
      
      <span>分享</span>
      <i class="fas fa-chevron-down" aria-hidden="true" style="zoom: 0.6;"></i>
    </a>
    <ul class="sub-nav menus_item_child ">
      
      <li>
        <a href="/musics">
          
          <i class="fas fa-music" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>音乐</span>
        </a>
      </li>
      
      <li>
        <a href="/movies">
          
          <i class="fas fa-film" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>电影</span>
        </a>
      </li>
      
      <li>
        <a href="/books">
          
          <i class="fas fa-book" style="margin-top: -20px; zoom: 0.6;"></i>
          
          <span>书籍</span>
        </a>
      </li>
      
    </ul>
    
  </li>
  
  <li>
    <a href="#searchModal" class="modal-trigger waves-effect waves-light">
      <i id="searchIcon" class="fas fa-search" title="搜索" style="zoom: 0.85;"></i>
    </a>
  </li>
</ul>


<div id="mobile-nav" class="side-nav sidenav">

    <div class="mobile-head bg-color">
        
        <img src="/medias/grape200x200.png" class="logo-img circle responsive-img">
        
        <div class="logo-name">Wabbybabbo的摸鱼圣地</div>
        <div class="logo-desc">
            
            上班？下班！
            
        </div>
    </div>

    

    <ul class="menu-list mobile-menu-list">
        
        <li class="m-nav-item">
	  
		<a href="/" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-sharp fa-solid fa-fish"></i>
			
			主页
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/tags" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-soild fa-tags"></i>
			
			标签
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/categories" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-bookmark"></i>
			
			分类
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/archives" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-archive"></i>
			
			档案
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="/about" class="waves-effect waves-light">
			
			    <i class="fa-fw fas fa-user-circle"></i>
			
			关于
		</a>
          
        </li>
        
        <li class="m-nav-item">
	  
		<a href="javascript:;">
			
				<i class="fa-fw fas fa-list"></i>
			
			分享
			<span class="m-icon"><i class="fas fa-chevron-right"></i></span>
		</a>
            <ul  style="background:  ;" >
              
                <li>

                  <a href="/musics " style="margin-left:75px">
				  
				   <i class="fa fas fa-music" style="position: absolute;left:50px" ></i>
			      
		          <span>音乐</span>
                  </a>
                </li>
              
                <li>

                  <a href="/movies " style="margin-left:75px">
				  
				   <i class="fa fas fa-film" style="position: absolute;left:50px" ></i>
			      
		          <span>电影</span>
                  </a>
                </li>
              
                <li>

                  <a href="/books " style="margin-left:75px">
				  
				   <i class="fa fas fa-book" style="position: absolute;left:50px" ></i>
			      
		          <span>书籍</span>
                  </a>
                </li>
              
            </ul>
          
        </li>
        
        
        <li><div class="divider"></div></li>
        <li>
            <a href="https://github.com/Wabbybabb0/Wabbybabb0.github.io" class="waves-effect waves-light" target="_blank">
                <i class="fab fa-github-square fa-fw"></i>Fork Me
            </a>
        </li>
        
    </ul>
</div>


        </div>

        
            <style>
    .nav-transparent .github-corner {
        display: none !important;
    }

    .github-corner {
        position: absolute;
        z-index: 10;
        top: 0;
        right: 0;
        border: 0;
        transform: scale(1.1);
    }

    .github-corner svg {
        color: #0f9d58;
        fill: #fff;
        height: 64px;
        width: 64px;
    }

    .github-corner:hover .octo-arm {
        animation: a 0.56s ease-in-out;
    }

    .github-corner .octo-arm {
        animation: none;
    }

    @keyframes a {
        0%,
        to {
            transform: rotate(0);
        }
        20%,
        60% {
            transform: rotate(-25deg);
        }
        40%,
        80% {
            transform: rotate(10deg);
        }
    }
</style>

<a href="https://github.com/Wabbybabb0/Wabbybabb0.github.io" class="github-corner tooltipped hide-on-med-and-down" target="_blank"
   data-tooltip="Fork Me" data-position="left" data-delay="50">
    <svg viewBox="0 0 250 250" aria-hidden="true">
        <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
        <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2"
              fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
        <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z"
              fill="currentColor" class="octo-body"></path>
    </svg>
</a>
        
    </nav>

</header>

    



<div class="bg-cover pd-header post-cover" style="background-image: url('/medias/featureimages/11.png')">
    <div class="container" style="right: 0px;left: 0px;">
        <div class="row">
            <div class="col s12 m12 l12">
                <div class="brand">
                    <h1 class="description center-align post-title">LLM2框架搭建过程</h1>
                </div>
            </div>
        </div>
    </div>
</div>




<main class="post-container content">

    
    <link rel="stylesheet" href="/libs/tocbot/tocbot.css">
<style>
    #articleContent h1::before,
    #articleContent h2::before,
    #articleContent h3::before,
    #articleContent h4::before,
    #articleContent h5::before,
    #articleContent h6::before {
        display: block;
        content: " ";
        height: 100px;
        margin-top: -100px;
        visibility: hidden;
    }

    #articleContent :focus {
        outline: none;
    }

    .toc-fixed {
        position: fixed;
        top: 64px;
    }

    .toc-widget {
        width: 345px;
        padding-left: 20px;
    }

    .toc-widget .toc-title {
        padding: 35px 0 15px 17px;
        font-size: 1.5rem;
        font-weight: bold;
        line-height: 1.5rem;
    }

    .toc-widget ol {
        padding: 0;
        list-style: none;
    }

    #toc-content {
        padding-bottom: 30px;
        overflow: auto;
    }

    #toc-content ol {
        padding-left: 10px;
    }

    #toc-content ol li {
        padding-left: 10px;
    }

    #toc-content .toc-link:hover {
        color: #42b983;
        font-weight: 700;
        text-decoration: underline;
    }

    #toc-content .toc-link::before {
        background-color: transparent;
        max-height: 25px;

        position: absolute;
        right: 23.5vw;
        display: block;
    }

    #toc-content .is-active-link {
        color: #42b983;
    }

    #floating-toc-btn {
        position: fixed;
        right: 15px;
        bottom: 76px;
        padding-top: 15px;
        margin-bottom: 0;
        z-index: 998;
    }

    #floating-toc-btn .btn-floating {
        width: 48px;
        height: 48px;
    }

    #floating-toc-btn .btn-floating i {
        line-height: 48px;
        font-size: 1.4rem;
    }
</style>
<div class="row">
    <div id="main-content" class="col s12 m12 l9">
        <!-- 文章内容详情 -->
<div id="artDetail">
    <div class="card">
        <div class="card-content article-info">
            <div class="row tag-cate">
                <div class="col s7">
                    
                    <div class="article-tag">
                        
                            <a href="/tags/LLM/">
                                <span class="chip bg-color">LLM</span>
                            </a>
                        
                            <a href="/tags/CUDA/">
                                <span class="chip bg-color">CUDA</span>
                            </a>
                        
                    </div>
                    
                </div>
                <div class="col s5 right-align">
                    
                    <div class="post-cate">
                        <i class="fas fa-bookmark fa-fw icon-category"></i>
                        
                            <a href="/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/" class="post-category">
                                高性能计算
                            </a>
                        
                    </div>
                    
                </div>
            </div>

            <div class="post-info">
                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-minus fa-fw"></i>发布日期:&nbsp;&nbsp;
                    2024-10-08
                </div>
                

                
                <div class="post-date info-break-policy">
                    <i class="far fa-calendar-check fa-fw"></i>更新日期:&nbsp;&nbsp;
                    2025-03-14
                </div>
                

                
                <div class="info-break-policy">
                    <i class="far fa-file-word fa-fw"></i>文章字数:&nbsp;&nbsp;
                    17.3k
                </div>
                

                

                
                    <div id="busuanzi_container_page_pv" class="info-break-policy">
                        <i class="far fa-eye fa-fw"></i>阅读次数:&nbsp;&nbsp;
                        <span id="busuanzi_value_page_pv"></span>
                    </div>
				
            </div>
        </div>
        <hr class="clearfix">

        
        <!-- 是否加载使用自带的 prismjs. -->
        <link rel="stylesheet" href="/libs/prism/prism.css">
        

        

        <div class="card-content article-card-content">
            <div id="articleContent">
                <h1 id="Lesson1-整体架构"><a href="#Lesson1-整体架构" class="headerlink" title="Lesson1 整体架构"></a>Lesson1 整体架构</h1><p>Llama2：生成式模型以decoder-only为架构<br>由两个Decoder组成：<br>①Context Decoder，位于prompt阶段，用来生成一个token；全量推理：输入是一个句子，只需要生成第一个token；具有并行计算的特点</p>
<p>②Mask self Decoder，位于generate阶段，用来生成第二个token；增量推理：输入是一个token，在gpt上的表现为每次吐出为一个token；每次输入的都是上一个输出的token<br><a target="_blank" rel="noopener" href="https://www.jianshu.com/p/c7b40d8526dd">Transformer系列：注意力机制的优化，MQA和GQA原理简述 - 简书</a></p>
<h1 id="Lesson2-项目搭建-amp-embedding-kernel"><a href="#Lesson2-项目搭建-amp-embedding-kernel" class="headerlink" title="Lesson2 项目搭建&amp;embedding kernel"></a>Lesson2 项目搭建&amp;embedding kernel</h1><p>讲解了：<br><code>src/utils/tensor.h</code><br><code>src/kernels/input_embedding.cu</code><br><code>src/kernels/input_embedding.h</code><br><code>tests/unittests/test_input_embedding.cu</code></p>
<pre class="line-numbers language-none"><code class="language-none">-src
|-kernels
|-|-input_embeding.cu
|-utils
|-|-tensor.h
|-weights<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p><code>LLMengine/src/utils/tensor.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">Struct Tensor{
	Device location,
	DataType dtype,
	std::vector&lt;int&gt; shape;
	...
	virtual int size() const {
&nbsp; &nbsp; &nbsp; &nbsp; if (shape.size() == 0) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // TODO: add an reminder info
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; return 0;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; return std::accumulate(shape.begin(), shape.end(), (int)1, std::multiplies&lt;int&gt;());
&nbsp; &nbsp; }
&nbsp; &nbsp; ...
&nbsp; &nbsp; template&lt;typename T&gt;
&nbsp; &nbsp; TensorWrapper&lt;T&gt;* as(){
&nbsp; &nbsp; &nbsp; &nbsp; return static_cast&lt;TensorWrapper&lt;T&gt;*&gt;(this); // 下行转换(显式)，将this(Tensor类型的当前对象)转换为TensorWrapper&lt;T&gt;类型的指针
&nbsp; &nbsp; }
}

Class TensorWrap: public Tensor {
	T * data;
	...
}

Struct TensorMap{
	std::unordered_map(std::string, Tensor*&gt; tensor_map);
	...
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li><code>std::unorder_map</code>：是一个关联容器，用于存储键值对，键是该Tensor的名字，值是指向Tensor类型变量的指针</li>
<li>关于为什么要在<code>TensorWrap</code>中先继承父类<code>Tensor</code>再实现模板化<code>T* data</code>：<code>Tensor</code>要放到<code>TensorMap</code>中，而C++作为强类型语言，不支持字典存放不同类型的tensor(因为类型定义为<code>Tensor</code>的指针，如果在<code>Tensor</code>中加入了<code>T*</code>作为成员，可能会乱套了)</li>
<li><code>std::accumulate(shape.begin(), shape.end(), (int)1, std::multiplies&lt;int&gt;());</code>：做乘积，初始乘的值为1</li>
</ul>
<p>如果<code>.cpp</code>文件调用带有cuda语法的函数，则其定义不能存在<code>.h</code>文件里，例如含有<code>&lt;&lt;&lt; &gt;&gt;&gt;</code><br>    例子：在<code>src/kernel/input_embedding.cu</code>中</p>
<ul>
<li>定义了<code>launchInputEmbedding</code>👇<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;
void launchInputEmbedding(TensorWrapper&lt;int&gt;* input_ids, &nbsp; &nbsp;// INT [token num]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; TensorWrapper&lt;T&gt;* output, &nbsp; &nbsp; &nbsp; // FP32 [token num, hidden_size] = [token num, 4096]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; EmbeddingWeight&lt;T&gt;* embed_table// FP32 [vocal_size, hidden_size]
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ) {
&nbsp; &nbsp; // 分配线程块，核函数需要的维度信息
&nbsp; &nbsp; const int blockSize = 256;
&nbsp; &nbsp; const int max_context_token_num = output-&gt;shape[0]; // token num
&nbsp; &nbsp; const int hidden_size = output-&gt;shape[1];
&nbsp; &nbsp; const int gridSize = 2048;
&nbsp; &nbsp; LLM_CHECK_WITH_INFO(max_context_token_num == input_ids-&gt;shape[0], "input ids 1st shape should equal to 1st shape of output");
&nbsp; &nbsp; embeddingFunctor&lt;T&gt;&lt;&lt;&lt;gridSize, blockSize&gt;&gt;&gt;(input_ids-&gt;data,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;output-&gt;data,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;embed_table-&gt;data,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;max_context_token_num,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;hidden_size);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li>实例化<ul>
<li>显式实例化是告诉编译器生成一个模板函数的特定实例。在模板函数定义中，只是定义了一个通用的逻辑，但没有真正生成代码。<strong>只有在模板实例化的时候，编译器才会根据具体的数据类型来生成相应的函数代码。</strong></li>
<li>原因：<ul>
<li>避免代码膨胀：如果不显式实例化，那么每次使用不同类型调用模板函数时，编译器都会生成新的代码</li>
<li>CUDA编译限制</li>
</ul>
</li>
<li>分别生成了👇两种类型的具体实例<code>T=float</code>、<code>T=half</code><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">// 显式实例化模版函数，由于cuda的语法规则，不能存在.cpp文件里，因此只能在此实例化
template void launchInputEmbedding(TensorWrapper&lt;int&gt;* input_ids, &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;TensorWrapper&lt;float&gt;* output, &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;EmbeddingWeight&lt;float&gt;* embed_table);
template void launchInputEmbedding(TensorWrapper&lt;int&gt;* input_ids, &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;TensorWrapper&lt;half&gt;* output, &nbsp; &nbsp; &nbsp;
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;EmbeddingWeight&lt;half&gt;* embed_table);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ul>
</li>
</ul>
<p><code>src/kernels/input_embedding.cu</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">__global__ void embeddingFunctor(const int* input_ids,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;T* output,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;const T* embed_table,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;const int max_context_token_num,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;const int hidden_size)
{
&nbsp; &nbsp; int index = blockIdx.x * blockDim.x + threadIdx.x;
&nbsp; &nbsp; while (index &lt; max_context_token_num * hidden_size) {
&nbsp; &nbsp; &nbsp; &nbsp; int id = input_ids[index / hidden_size];
&nbsp; &nbsp; &nbsp; &nbsp; output[index] = embed_table[id * hidden_size + index % hidden_size];
&nbsp; &nbsp; &nbsp; &nbsp; index += blockDim.x * gridDim.x;
&nbsp; &nbsp; }
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/6382f7e2cd4bf0b302b43f58c93ea08.jpg" alt="">index的索引对应的是output的输出的每个位置？<p></p>
<p>这个kernel的用处：将原本输入格式的[batch size, sequence length]变成[batch size, sequence length, hidden size]</p>
<p>有<code>.cpp</code>文件、<code>.cc</code>文件、<code>.cu</code>文件的目录下需要放<code>CMakeLists.txt</code>文件</p>
<h1 id="Lesson3-Calculate-padding-offset-kernel"><a href="#Lesson3-Calculate-padding-offset-kernel" class="headerlink" title="Lesson3 Calculate padding offset kernel"></a>Lesson3 Calculate padding offset kernel</h1><p>讲解了：<br><code>src/kernels/cal_paddingoffset.cu</code><br><code>src/kernels/cal_paddingoffset.h</code><br><code>tests/unittests/test_cal_paddingoffset.cu</code></p>
<p><a target="_blank" rel="noopener" href="https://github.com/bytedance/effective_transformer">Padding Offset思想来源</a><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/Pasted%20image%2020241011153928.png" alt="|425"></p>
<p><code>src/lkernels/cal_paddingoffset.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void launchCalPaddingoffset(TensorWrapper&lt;int&gt;* padding_offset,
							TensorWrapper&lt;int&gt;* cum_seqlens,
							TensorWrapper&lt;int&gt;* input_lengths
);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li>参数：<br>  <code>padding_offset</code>：<code>[batch size, max q_seq length]</code>记录每个token在其之前的padding个数<br>  <code>cum_seqlens</code>：<code>[batch size + 1]</code>第一个句子累积长度是它本身，第二个句子累积长度是第一句+第二句长度<br>  <code>input_lengths</code>：<code>[batch size]</code>每个句子的输入长度，本身的<br>  <code>launchCalPaddingoffset</code>函数的目的是输出padding个数和累积长度</li>
<li>例子：<br>  11100<br>  11000<br>  11111<br>  batch size = 3<br>  seqlen = [3, 2, 5]<br>  max_q_len = 5<br>  padding_offset = [0, 0, 0, 0, 0<pre><code>              2, 2, 2, 2, 2
              5, 5, 5, 5, 5]
</code></pre>  cum_seqlens = [0, 3, 5, 10]</li>
</ul>
<p>相比于<code>Lesson2</code>中的模板化，这里不需要模板化的原因是，该函数的参数都是<code>int</code>类型，而<code>Lesson2</code>中的是<code>T</code>类型，需要对其做<code>FP16</code>和<code>FP32</code>的模板化</p>
<p><code>src/kernels/cal_paddingoffset.cu</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">__global__ void CalPaddingoffset(int* &nbsp; &nbsp; &nbsp; &nbsp; padding_offset,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;int* &nbsp; &nbsp; &nbsp; &nbsp; cum_seqlens,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;const int* &nbsp; input_lengths, //actual input lens
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;const int &nbsp; &nbsp;batch_size,
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;const int &nbsp; &nbsp;max_q_len) {
&nbsp; &nbsp; // 自己打的24-10-11
&nbsp; &nbsp; int cum_offset = 0;
&nbsp; &nbsp; int ind = 0;
&nbsp; &nbsp; int total_seqlen = 0;
&nbsp; &nbsp; for(int b = 0; b &lt; batch_size; b++) { // b对应每个batch中的第b+1个seq
&nbsp; &nbsp; &nbsp; &nbsp; int seqlen = input_lengths[b]; &nbsp; &nbsp;// 获取每个句子长度
&nbsp; &nbsp; &nbsp; &nbsp; cum_seqlens[b] = total_seqlen; &nbsp; &nbsp;// (1)将累积的seqlen存入到每个句子中，cum_seqlens[0] = 0, ..., cum_seqlens[0] = 最后一个句子的句子累积长度
&nbsp; &nbsp; &nbsp; &nbsp; for( int i =0; i &lt; seqlen; i++) {
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; padding_offset[ind] = cum_offset; // (2)将累积的offset存入到每个token中，padding_offset的下标应该是一个累积的值，所以应该在for的外部定义ind然后取其为下标
&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; ind++;
&nbsp; &nbsp; &nbsp; &nbsp; }
&nbsp; &nbsp; &nbsp; &nbsp; cum_offset += max_q_len - seqlen; &nbsp; &nbsp; // 获取每个句子累积的offset
&nbsp; &nbsp; &nbsp; &nbsp; total_seqlen += seqlen; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; // 获取每个句子累积的句子长度
&nbsp; &nbsp; }
&nbsp; &nbsp; cum_seqlens[batch_size] = total_seqlen;
&nbsp; &nbsp; }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<p><code>kernel</code>写完之后还需要写<code>CMake</code>文件<br><code>test/unittest/CMakelist.txt</code>：将test编译为可执行文件<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">add_executable(cal_paddingoffset // ※
	test_input_embedding.cu
)
target_link_libraries(
	cal_paddingoffset PUBLIC    //这要和※处的名称对应
	-lcudart
	-lcudadevrt
	paddingoffset               // 这里可以自己起
)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br><code>src/kernels/CMakelist.txt</code>(注意和上面的名称的对应)<br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">add_library(paddingoffset STATIC cal_paddingoffset.cu)
set_property(TARGET paddingoffset PROPERTY CUDA_SEPARABLE_COMPILATION   ON)
set_property(TARGET paddingoffset PROPERTY POSITION_INDEPENDENT_CODE ON)
set_property(TARGET paddingoffset PROPERTY CUDA_RESOLVE_DEVICE_SYMBOLE ON)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<h1 id="Lesson4-RMS-norm"><a href="#Lesson4-RMS-norm" class="headerlink" title="Lesson4 RMS norm"></a>Lesson4 RMS norm</h1><p>讲解了：<br><code>src/kernels/rmsnorm_kernel.cu</code><br><code>src/kernels/rmsnorm_kernel.h</code><br><code>tests/unittests/test_rmsnorm.cu</code><br><code>src/utils/vectorize_utils.h</code><br><code>src/weights/llama/norm_weights.h</code></p>
<p><code>src/utils/vectorize_utils.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;
struct Vec{
&nbsp; &nbsp; using Type = T;
&nbsp; &nbsp; static constexpr int size = 0;
};
// 除此之外还定义了float4(size=4)，half2(size=2)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br><code>static</code>：表示该成员<code>size</code>属于类而不是某个实例(对象)<br><code>constexpr</code>：定义一个静态的类成员，并且该成员是一个编译时常量，在编译时就确定<br><code>float4</code>和<code>half2</code>分别是包含4个<code>float</code>分量的向量和包含2个<code>half</code>分量的向量<br>作用是存储通用的向量化数据结构<p></p>
<p><code>src/weights/llama/norm_weights.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
struct LayerNormWeight {  
    T* gamma; 
};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<h2 id="4-1src-kernel-rmsnorm-kernel-cu"><a href="#4-1src-kernel-rmsnorm-kernel-cu" class="headerlink" title="4.1src/kernel/rmsnorm_kernel.cu"></a>4.1<code>src/kernel/rmsnorm_kernel.cu</code></h2><p>(1)<code>warpReduceSum</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
__device__ T warpReduceSum(T val){  
    for(int i = 32 / 2; i &gt; 0; i &gt;&gt;= 1){  
        val += __shfl_xor_sync(0xffffffff, val, i);  
    }    
    return val; // 最后这个warp的结果保存在第一个第一个线程(threadIdx.x=0)
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>将一个warp中的数据加起来<p></p>
<p>(2)<code>blockReduceSum</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
__device__ T blockReduceSum(T val){  
    int tid = threadIdx.x;  
    it wid = tid / 32;  
    int laneid = tid % 32;  
    int warpnum = (blockDim.x + 32 - 1) / 32;  
    val = warpReduceSum&lt;T&gt;(val);     // val是每个warp的总和的值
    static __shared__ T warpsum[64]; // 不能写warpnum，因为申请的是静态的，需要传入编译期常量64
    if(landid == 0) // 如果是wrap的第一个线程(存有该wrap的结果)
    { 
	    warpsum[wid] = val; // 将每个warp的求和放入warpsum中
    }    
    __syncthreads(); // 处理完共享内存的读写后要加上`__syncthreads();!!!
    T sum = tid &lt; warpnum ? warpsum[wid] : (T)0; 
    // 处理前warpnum个warpsum[wid]，并且确保使用线程id为0~warpnum-1来处理
    sum = warpReduceSum&lt;T&gt;(sum);  
    return sum;
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>将一个block的数据加起来<br>参数：<br>    <code>tid</code>：全局的thread idx (0~?)<br>    <code>wid</code>：wrap idx，每32个threads为一个wrap (0~?)<br>    <code>laneid</code>：wrap中的thread的编号(0~31)<br>    <code>warpnum</code>：用到的warp的个数，最小为1，所以这里需要向上取整<br>    <code>warpsum</code>：大小为64的类型为T的数组，存放每个warp的总和<p></p>
<p>(3)<code>RMSNorm</code><br>计算公式：$\dfrac{x_i×g_i}{\sqrt{\sum^iE(x_i^2)+eps}}$<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template &lt;typename T&gt;  
__global__ void RMSNorm(T* decoder_in,
                        T* decoder_residual,  
                        T* scale, //[q_hidden_units], RMSNorm weights  
                        float eps, //RMSNorm eps  
                        int num_tokens,  
                        int hidden_units) {  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>参数：<br>    <code>decoder_in</code>：是输入同时也是输出位置，<code>[num tokens, q_hidden_units]</code><br>    <code>decoder_residual</code>：暂时不知道这个的用处<br>    <code>scale</code>：可学习的参数(权重)，<code>[q_hidden_units]</code><br>    <code>eps</code>：很小的正数<br>    <code>num_tokens</code>：token的个数<br>    <code>hidden_units</code>：隐藏层的单元的数量<br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int vec_size = Vec&lt;T&gt;::size;
using Vec_t = typename Vec&lt;T&gt;::Type;  
Vec_t *dout = reinterpret_cast&lt;Vec_t*&gt;(decoder_in + blockIdx.x * hidden_units); // 每个线程需要读的数据的偏移; block的数量是token的数量  
Vec_t *rsd = reinterpret_cast&lt;Vec_t*&gt;(decoder_residual * blockIdx.x * hidden_units);  
float thread_sum = 0.0f;  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>参数：<br>    <code>vec_size</code>：读取vector的大小，比如<code>float4</code>的向量个数为4，<code>half2</code>的向量个数为2<br>    <code>Vec_t</code>：读取类型并存到<code>Vec_t</code>中<br>        ！前一句没有用<code>typename</code>而后一句用了的原因是：<br>        ①前者属于非依赖型，<code>size</code>的值在编译时可以确定，与<code>T</code>d的具体类型无关；<br>        ②后者时依赖类型，<code>Type</code>是一个类型别名，取决于<code>T</code>，因此需要<code>typename</code>关键字来告诉编译器他是一个类型<br>    <code>dout</code>：根据线程指向每一个以输入向量为起始的<code>block</code>的开头，每一个<code>block</code>对应一个<code>token</code>，每个<code>block</code>之间相差大小为<code>hidden units</code>的间隔<br>    <code>rsd</code>：同<code>dout</code><br>    <code>thread_sum</code>：用于求和<br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">for (int idx = threadIdx.x; idx &lt; hidden_units / vec_size; idx += blockDim.x) {  
        Vec_t vec = dout[idx];
        rsd[idx] = vec;  
        thread_sum += vec.x * vec.x;  
        thread_sum += vec.y * vec.y;  
        thread_sum += vec.z * vec.z;  
        thread_sum += vec.w * vec.w;  
    }    
thread_sum = blockReduceSum&lt;float&gt;(thread_sum);  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>用于求$\sum^iE(x_i)$，每个block得到一个总和<br>参数：<br>    <code>vec</code>：将<code>dout[idx]</code>的数据存到vec中<br>    <img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/492e42f3b277fbb6c44c818fa908be8.jpg" alt="|225"><br>    <code>thread_sum</code>：每个线程都有一个私有的副本<br>注意：<br>    <code>idx</code>的范围是从<code>threadIdx.x</code>开始的，范围是0~<code>blockDim.x-1</code><br>    因此每个for循环实际只处理了一个block的求和，<code>idx+=blockDimx.x</code>使得可以对下一个block进行求和<br>    所以说这里的求和是block层面的，也是每个token层面的<br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">__shared__ float inv_mean;  
if (threadIdx.x == 0) {  
    inv_mean = rdqrtf(thread_sum / hidden_units + eps);  
}    
__syncthreads(); // share memory inv_mean写入完成后要加上这句话  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>用于计算平均值$inv_mean\dfrac{1}{\sqrt{\sum^iE(x_i^2)+eps}}$<br>    <code>inv_mean</code>：因为均值是<code>block</code>层面的，所以最好把它设为share memory<br>    share memory写入完成后要加上<code>__syncthreads();</code><br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">Vec_t *s = reinterpret_cast&lt;Vec_t *&gt;(scale);  
    for (int idx = threadIdx.x; idx &lt; hidden_units / vec_size; idx += blockDim.x) {  
        Vec_t vec = dout[idx];  
        dout[idx].x = vec.x * inv_mean * s[idx].x; // 因为输入输出都是decoder_in，所以需要实实在在地进dout[idx]这个指针指向的buffer，等号左边不能用vec  
        dout[idx].y = vec.y * inv_mean * s[idx].y; // 因为vec size是4，所以累加4次  
        dout[idx].z = vec.z * inv_mean * s[idx].z;  
        dout[idx].w = vec.w * inv_mean * s[idx].w;  
    }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>用于计算$inv_mean × x_i ×g_i$<br>注意：需要把结果写回<code>dout</code>中<p></p>
<p>(4)<code>launchRMSNorm</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
void launchRMSNorm( TensorWrapper&lt;T&gt;* decoder_out,
                    TensorWrapper&lt;T&gt;* decoder_residual,
                    LayerNormWeight&lt;T&gt;&amp; attn_norm_weight,
                    bool is_last 
                    ){  
    int num_tokens = decoder_out-&gt;shape[0];  
    int hidden_units = decoder_out-&gt;shape[1];  
    int vec_size = Vec&lt;T&gt;::size;  
    int num_threads = hidden_units / 4;
    T* rsd = decoder_residual-&gt;data;  
    dim3 grid(num_tokens);   // num_tokens个block
    dim3 block(num_threads); // hidden_units / 4个block
    RMSNorm&lt;T&gt;&lt;&lt;&lt;grid, block&gt;&gt;&gt;(decoder_out-&gt;data,  
                            rsd,  
                            attn_norm_weight.gamma, // scale
                            eps,  
                            num_tokens,  
                            hidden_units);
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<h1 id="Lesson5-Casual-Mask"><a href="#Lesson5-Casual-Mask" class="headerlink" title="Lesson5 Casual Mask"></a>Lesson5 Casual Mask</h1><p>讲解了：<br><code>src/kernels/build_casual_mask.cu</code><br><code>src/kernels/build_casual_mask.h</code><br><code>tests/unittests/test_casual_mask.cu</code></p>
<p><code>src/kernel/build_casual_mask.cu</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
__global__ void BuildCausalMasksConsideringContextPastKV(T* mask,
                                                const int* q_lens,
                                                const int* k_lens,
                                                int max_q_len, 
                                                int max_k_len){
    int tid = threadIdx.x;  
    int qlen = q_lens[blockIdx.x];
    int klen = k_lens[blockIdx.x];
    mask += blockIdx.x * max_k_len * max_q_len; // 每个block只有256个线程，相应的，mask也需要有偏移量移动到下一个mask上，与block的移动同步  
    while(tid &lt; max_k_len * max_q_len){  
        int q = tid / max_k_len; // 目前处于哪一行  
        int k = tid % max_k_len; // 目前处于哪一列  
        bool is_one = q &lt; qlen &amp;&amp; k &lt; klen &amp;&amp; k &lt;= q + (klen - qlen);  
        mask[tid] =  static_cast&lt;T&gt;(is_one);  
        tid += blockDim.x; 
    }
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>参数：<br>    <code>mask</code>：<code>[batch_size, max_q_len, max_k_len]</code>每个<code>mask</code>是一个矩阵，用于表示哪些token对于目前对话是可见的(置1)和不可见的(置0)<br>    <code>q_lens</code>：<code>[batch_size]</code>，作为input lens，我的理解是当前对话的输入<br>    <code>k_lens</code>：<code>[batch_size]</code>，作为context lens，我的理解是结合一定程度的上下文的输入<br>    <code>max_q_len</code>&amp;<code>max_k_len</code>：分别是<code>q_lens</code>和<code>k_lens</code>中最大的<br>理解：<br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int qlen = q_lens[blockIdx.x];
int klen = k_lens[blockIdx.x];<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><br>👆每个<code>block</code>对应一个对话，<code>batch_size</code> = 对话个数。这里是分别取每个对话的<code>qlen</code>和<code>klen</code><br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">mask += blockIdx.x * max_k_len * max_q_len;
   while(tid &lt; max_k_len * max_q_len){  
       int q = tid / max_k_len; // 目前处于哪一行  
       int k = tid % max_k_len; // 目前处于哪一列  
       bool is_one = q &lt; qlen &amp;&amp; k &lt; klen &amp;&amp; k &lt;= q + (klen - qlen);  
       mask[tid] = static_cast&lt;T&gt;(is_one);  
       tid += blockDim.x; 
   }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li>为了确保<code>mask</code>里的每个数都能被处理到（第三句很重要）<ul>
<li><code>mask[tid] = static_cast&lt;T&gt;(is_one);</code>：<code>block</code>中的一个线程对应<code>mask</code>里的一个数，但是<code>blockDim.x=256</code>，所以需要加上第三句话</li>
<li>循环条件<code>tid &lt; max_k_len * max_q_len</code>：确保每个数都有对应线程处理</li>
<li><code>mask += blockIdx.x * max_k_len * max_q_len;</code>：</li>
<li><code>mask</code>的大小＞<code>block</code>的线程数的情况👇</li>
</ul>
</li>
</ul>
<p>那么就是一个<code>block</code>处理一个<code>mask</code>，如果<code>block</code>大小小于<code>mask</code>的话，就继续用该<code>block</code>的线程处理<code>mask</code>剩余的数</p>
<h1 id="Lesson6-Linear"><a href="#Lesson6-Linear" class="headerlink" title="Lesson6 Linear"></a>Lesson6 Linear</h1><p>讲解了：<br><code>src/kernels/cublas_utils.h</code>：定义cublas类<br><code>src/kernels/cublas_utils.cc</code>：实现cublas类<br><code>src/kernels/linear.cu</code><br><code>src/kernels/linear.h</code><br><code>tests/unittests/test_linear.cu</code></p>
<h2 id="6-1-cublas类的声明与定义"><a href="#6-1-cublas类的声明与定义" class="headerlink" title="6.1 cublas类的声明与定义"></a>6.1 cublas类的声明与定义</h2><p><code>src/kernels/cublas_utils.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">class cublasWrapper {  
private:  
    cublasHandle_t cublas_handle;
    cudaDataType_t Atype;
    cudaDataType_t Btype;
    cudaDataType_t Ctype;
    cublasComputeType_t computeType;
public:  
    cublasWrapper(cublasHandle_t cublas_handle);  
    ~cublasWrapper();
    void setFP32GemmConfig();
    void setFP16GemmConfig();
    void Gemm(cublasOperation_t transa,  
              cublasOperation_t transb,  
              const int         m,  
              const int         n,  
              const int         k,  
              const void*       A,  
              const int         lda,  
              const void*       B,  
              const int         ldb,  
              void*             C,  
              const int         ldc,  
              float             alpha,  
              float             beta);  
        // for qk*v and q*k    
    void stridedBatchedGemm(cublasOperation_t transa,  
	                        cublasOperation_t transb,  
							const int         m,  
							const int         n,  
							const int         k,  
							const void*       A,  
							const int         lda,  
							const int64_t     strideA,  
							const void*       B,  
							const int         ldb,  
							const int64_t     strideB,  
							void*             C,  
							const int         ldc,  
							const int64_t     strideC,  
							const int         batchCount,  
							float             f_alpha,  
							float             f_beta);  
};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>👆声明<code>cublasWrapper</code>类，<code>batchedGemm</code>相对于<code>Gemm</code>多了步长<code>stride</code>和<code>batchCount</code><p></p>
<p>定义部分：<br>①构造函数<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">cublasWrapper::cublasWrapper(cublasHandle_t cublas_handle,  
                             cublasLtHandle_t cublaslt_handle):  
    cublas_handle(cublas_handle),  
    cublaslt_handle(cublaslt_handle){  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li><code>cublasHandle_t</code>是cublas库中的一个类型，与句柄有关</li>
<li>传入<code>cublas_handle</code>返回到类中的<code>cublas_handle_</code></li>
<li><code>cublasHandle_t</code>和<code>cublasLtHandle_t</code><ul>
<li><code>cublasHanle_t</code>：用于一般的线性代数运算（如向量和矩阵操作）</li>
<li><code>cublasLtHandle_t</code>：用于更高级的矩阵运算，特别是自定义和优化矩阵乘法（GEMM），在需要复杂配置或多种数据类型时有用处</li>
</ul>
</li>
</ul>
<p>②单精度与半精度的配置<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void cublasWrapper::setFP32GemmConfig()  
{  
    Atype       = CUDA_R_32F;  
    Btype       = CUDA_R_32F;  
    Ctype       = CUDA_R_32F;  
    computeType = CUBLAS_COMPUTE_32F; // 
}  
  
void cublasWrapper::setFP16GemmConfig()  
{  
    Atype       = CUDA_R_16F;  
    Btype       = CUDA_R_16F;  
    Ctype       = CUDA_R_16F;  
    computeType = CUBLAS_COMPUTE_16F;  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li>对于<code>computeType</code>，当cuda version&lt;11.0时用CUDA_R_32F，cuda version&gt;11.0时使用CUBLAS_COMPUTE_32F，半精度的同理</li>
</ul>
<p>③为<code>alpha</code>和<code>beta</code>赋值<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">const void* alpha = is_fp16_computeType ? reinterpret_cast&lt;void*&gt;(&amp;(h_alpha)) : reinterpret_cast&lt;void*&gt;(&amp;f_alpha);  
const void* beta  = is_fp16_computeType ? reinterpret_cast&lt;void*&gt;(&amp;(h_beta)) : reinterpret_cast&lt;void*&gt;(&amp;f_beta);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p></p>
<ul>
<li>如果<code>is_fp16_computeTyp</code>为1，则传入半精度的<code>alpha</code>给<code>alpha</code>，<code>beta</code>同理</li>
</ul>
<p>④关于batchedGemm与Gemm同理</p>
<h2 id="6-2-Gemm"><a href="#6-2-Gemm" class="headerlink" title="6.2 Gemm"></a>6.2 Gemm</h2><p><code>src/kernels/linear.cu</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
void launchLinearGemm(TensorWrapper&lt;T&gt;* input,  
                      BaseWeight&lt;T&gt;&amp; weight,   
					  TensorWrapper&lt;T&gt;* output,  
                      cublasWrapper* cublas_wrapper,  
                      bool trans_a = false,  
                      bool trans_b = false){
                      {  
Bk = input-&gt;shape.size() == 3 ? input-&gt;shape[1] * input-&gt;shape[2] : input-&gt;shpe[1];  
Cm = output-&gt;shape.size() == 3 ? output-&gt;shape[1] * output-&gt;shape[2] : output-&gt;shpe[1];  
  
int lda = Am;  
int ldb = Bk;  
int ldc = Cm;  

cublasOperation_t transA = trans_b ? CUBLAS_OP_T : CUBLAS_OP_N; 
cublasOperation_t transB = trans_a ? CUBLAS_OP_T : CUBLAS_OP_N;

// 可能会出现输入为[bs, 1, hiddenunits] * [hiddenunits, hiddenunits]，所以需要检查输入的维度
if(!trans_b &amp;&amp; !trans_a){   
    LLM_CHECK_WITH_INFO(Ak == Bk, "2nd dim of input MUST = 1st dim of weight!");  
}  
  
cublas_wrapper-&gt;Gemm(transA,  
                     transB,  
                     trans_b ? Ak : Am,           // m
                     Cn,                          // n
                     Bk,                          // k
                     weight.data,  
                     lda,  
                     input-&gt;data,  
                     ldb,  
                     output-&gt;data,  
                     ldc,  
                     1.0f,  
                     0.0f);}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>① 关于A、B、C<br>(1)一般的gemm<br>A：<code>input shape = [seqlen, hidden_units]</code><br>B：<code>weight shape = [hidden_units, hidden_units]</code><br><code>A * B = C with trans_b = false</code><p></p>
<p>对于qkvlinear，是指将三次矩阵乘法融合到一次<br><code>input=[seqlen, hidden_units]</code><br><code>weight shape = [hidden_units, 3×hidden_units]</code></p>
<p>(2)出现在sampling的<code>LMHead</code><br>A：<code>input shape = [batch_size, hidden_units]</code><br>B：<code>weight_shape = [vocabulary_size, hidden_units]</code><br><code>A * B = C with transb = true</code></p>
<p>②重点与难点：</p>
<ul>
<li><code>torch.nn.linear</code>的计算公式是$y=x×w^T$，修改之前是$y=x×w$，因此<code>trans_b=True</code></li>
<li>cublas API接受的输入以及输出的内存排布全部都默认为<strong>列主序(column-major)</strong><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/Pasted%20image%2020241019161646.png" alt=""></li>
<li>因此，我们的思路是<ul>
<li>从原本的$y=x×w$，因为<code>nn</code>的计算方式</li>
<li>加上<code>trans_b=True</code>后可以实现$y=x×w^T$，因为列主序(从行主序到列主序需要将两边同时转置)<ul>
<li>这里的<code>trans_b</code>对应的是原本我们理解的$y=x×w$公式，<code>trans_b</code>对应$w$</li>
<li>变成column major之后<code>A</code>对应$w^T$，所以是用<code>trans_b</code>决定<code>trans_A</code></li>
</ul>
</li>
<li>将$y=x×w^T$变成$y^T=w×x^T$后可以实现列序列的要求，那么对应$y=x×w$就应该变成$y^T=w^T×x^T$</li>
<li>即从原始的$y=x×w$变成我们需要的公式，只需要<ul>
<li>添加<code>trans_b=True</code></li>
<li>公式$y^T=w^T×x^T$</li>
</ul>
</li>
</ul>
</li>
</ul>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int Am = weight.shape[1];
int Ak = weight.shape[0];  
int Bk = input-&gt;shape[1];  
int Bn = input-&gt;shape[0];  
int Cm = output-&gt;shape[1];  
int Cn = output-&gt;shape[0];<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h2 id="6-3-StrideBatchGemm"><a href="#6-3-StrideBatchGemm" class="headerlink" title="6.3 StrideBatchGemm"></a>6.3 StrideBatchGemm</h2><p>①关于input1和input2<br>$q×k$<br>input1：<code>q shape = [batch_size, head_nums, seqlen(=len_q), hidden_units]</code><br>input2：<code>k shape = [batch_size, head_nums, seqlen(=len_k), hidden_units]</code><br><code>A * B = C with trans_b = true</code><br>$qk×v$<br>input1：<code>qk shape = [batch_size, head_nums, seqlen(=len_q), seqlen(=len_k)]</code><br>input2：<code>v shape = [batch_size, head_nums, seqlen(=len_k), hidden_units]</code><br><code>A * B = C with transb = false</code><br>实际上在<code>src/kernels/linear.cu</code>中处理过程与Gemm差不多</p>
<p>②StrideBatchGemm和BatchGemm相比<br>假如<code>A=[1,2,3,4]</code></p>
<ul>
<li>StrideBatch多一个<code>Stride</code>变量，用于作地址偏移取出要相乘的值，偏移量等于<code>A[i]</code>和<code>A[i+1]</code>之间的距离<ul>
<li>=3*4</li>
</ul>
</li>
<li>两个都有<code>batchCount</code>变量<ul>
<li>对于StrideBatch是每个批次中需要相乘的矩阵个数 = 1*2</li>
<li>BatchGemm是A、B、C中指针个数，及矩阵个数<h2 id="6-4-其他"><a href="#6-4-其他" class="headerlink" title="6.4 其他"></a>6.4 其他</h2><code>cublasHanle_t</code>用于定义一个句柄，用于管理和配置 <code>cuBLAS</code> 库中的所有函数调用，类似一个控制器(开/关)</li>
</ul>
</li>
</ul>
<p>初始化列表例子 ：<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">class Person {
public:
	////传统方式初始化
	//Person(int a, int b, int c) {
	//	m_A = a;
	//	m_B = b;
	//	m_C = c;
	//}

	//初始化列表方式初始化
	Person(int a, int b, int c) :m_A(a), m_B(b), m_C(c) {}
	void PrintPerson() {
		cout &lt;&lt; "mA:" &lt;&lt; m_A &lt;&lt; endl;
		cout &lt;&lt; "mB:" &lt;&lt; m_B &lt;&lt; endl;
		cout &lt;&lt; "mC:" &lt;&lt; m_C &lt;&lt; endl;
...
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<p><code>RAII</code>机制可以自动析构掉一些类成员变量</p>
<p>huggingface的7b chat中linear的weight全是转置后的，比如<code>gate</code>的权重应该是<code>[q_hidden_units, inter_size]</code>，但是在huggingface里是<code>[inter_size, q_hidden_units]</code>，所以<code>launchLinearGemm</code>的<code>trans_b</code>对于所有linear weights来说都是<code>true</code></p>
<h1 id="Lesson7-Debug-一"><a href="#Lesson7-Debug-一" class="headerlink" title="Lesson7 Debug(一)"></a>Lesson7 Debug(一)</h1><p><code>src/kernels/rmsnorm_kernel.cu</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">dout[idx].x = __float2half(vec.x * inv_mean) * s[idx].x;
dout[idx].y = __float2half(vec.y * inv_mean) * s[idx].y;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><br>出现如下报错：<br><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">error: <span class="token function">more</span> than one operator <span class="token string">"*"</span> matches these operands:
built-in operator <span class="token string">"arithmetic * arithmetic"</span>
<span class="token keyword">function</span> <span class="token string">"operator*(const __half &amp;, const __half &amp;)"</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br>在编译器执行乘法运算时，发现有多个符合条件的*操作符但是不确定应该使用哪一个<p></p>
<ul>
<li><code>built-in operator "arithmetic * arithmetic"</code>：这是CUDA支持的基本算术类型之间的乘法操作（如整数或浮点数）。</li>
<li><code>function "operator*(const __half &amp;, const __half &amp;)"</code>：这是CUDA中针对<code>__half</code>类型（即半精度浮点数）提供的乘法操作符。<br>解决方法：<br>将代码改为<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">dout[idx].x = s[idx].x * __float2half(__half2float(vec.x) * inv_mean);
dout[idx].y = s[idx].y * __float2half(__half2float(vec.y) * inv_mean);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
</li>
</ul>
<p>编译顺序从<code>kernels</code>到<code>tests</code>原因：</p>
<ul>
<li>编译<code>tests</code>时需要调用到<code>src/kernels</code>的cuda函数或者launch函数，所以需要先编译<code>kernels</code>文件下的</li>
<li>在根目录下的<code>CMakeList.txt</code>中有先后顺序<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">add_subdirectory(src)
add_subdirectory(tests)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
</li>
</ul>
<h1 id="Lesson8-RoPE"><a href="#Lesson8-RoPE" class="headerlink" title="Lesson8 RoPE"></a>Lesson8 RoPE</h1><p><a target="_blank" rel="noopener" href="https://www.53ai.com/news/qianyanjishu/1291.html">一文看懂 LLaMA 中的旋转式位置编码（Rotary Position Embedding）</a><br>讲解了：<br><code>src/kernels/qkv_bias_and_RoPE.cu</code><br><code>src/kernels/qkv_bias_and_RoPE.h</code><br><code>src/models/llama/llama_params.h</code><br><code>tests/unittests/test_bias_and_rope.cu</code><br><code>src/utils/vectorize_utils.h</code></p>
<p>本节融合算子的作用</p>
<ul>
<li>将<code>qkv bias</code>加到<code>QKV</code>上，<code>QKV = [num tokens, qkv head num, head size]</code><ul>
<li><code>qkv head num</code> = <code>q head num</code> + <code>k head num</code> + <code>v head num</code></li>
<li><code>k head num</code> = <code>v head num</code></li>
</ul>
</li>
<li>padding后，<code>QKV</code>会被分割成三个矩阵<code>q</code>、<code>k</code>、<code>v</code>，<ul>
<li>shape(q)=<code>[bs, q head num, max q len, head size]</code></li>
<li>shape(k/v)=<code>[bs, kv head num, max q len, head size]</code></li>
</ul>
</li>
<li>rope &amp; attention</li>
<li>写回显存(gmem)</li>
</ul>
<p>输入：<br>    <code>QKV</code> shape=<code>[num tokens, qkv head num, head size]</code><br>    <code>qkv bias</code> shape = <code>[qkv head num, head size]</code><br>输出：<br>    <code>q</code>：<code>[bs, q head num, max q len, head size]</code><br>    <code>k</code>：<code>[bs, kv head num ,max q len, head size]</code><br>    <code>v</code>：<code>[bs, kv head num, max q len, head size]</code><br>    这里的<code>max q len</code>就是<code>seqlen</code><br>下一节会讲到<code>repeat kv</code></p>
<h2 id="8-1-src-kernels-qkv-bias-and-RoPE-cu"><a href="#8-1-src-kernels-qkv-bias-and-RoPE-cu" class="headerlink" title="8.1 src/kernels/qkv_bias_and_RoPE.cu"></a>8.1 <code>src/kernels/qkv_bias_and_RoPE.cu</code></h2><p>llama使用的是QGA(Grouped-Query Attention)，采用的是一组Q(N个)共享同一个KV</p>
<p><code>QKV</code>第一个维度是<code>token_num</code>，因此网格的第一个维度x也是<code>token_num</code>，网格的第二个维度y是<code>head_num</code>(<code>q head num</code>)</p>
<p><code>qkv</code>类型是<code>BaseWeight&lt;T&gt;</code>，在<code>src/weights/base_weights.h</code>中<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;
struct BaseWeight{
	std::vector&lt;int&gt; shape;
	T* data;
	WeightType type;
	T* bias; // qkv需要这一项
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<p>①<code>GetRoPRfreq()</code>是用来求$θ$和$m$的<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">inline __device__ float2 GetRoPEfreq(int zid, int rot_embed_dim, float base, float t_step) {  
    float inv_freq = t_step / powf(base, zid / (float)rot_embed_dim); // 求mθ  
    return{cos(inv_freq), sin(inv_freq)};  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><br>公式：$Θ=\{θ_i=10000^{-2(i-1)/d},i\in[1,2,…,d/2]\}$<br>入参：<p></p>
<ul>
<li><code>zid</code>：<code>2(i-1)</code></li>
<li><code>rot_embed_dim</code>：<code>d</code>，词嵌入向量的维度</li>
<li><code>base</code>：公式中的10000</li>
<li><code>t_step</code>：time step，是要求的<code>m</code>，<code>m</code>表示第<code>m</code>个token<br>变量：</li>
<li><code>inv_freq</code>：就是$mθ_i$<ul>
<li>$mθ_i=m\ ÷\ 10000^{2(i-1)/d}$</li>
<li>$10000^{2(i-1)/d}$ = <code>powf(base,zid / (float)d)</code><ul>
<li><code>base=10000</code></li>
<li><code>zid=2(i-1)</code></li>
<li>因为传进来的<code>rot_embed_dim</code>是<code>int</code>型的，所以加了个<code>float</code></li>
</ul>
</li>
</ul>
</li>
<li>返回的是$cos(mθ_i)$和$sin(mθ_i)$</li>
</ul>
<p>②<code>GetRoPEres()</code>是用来得到RoPE后的结果的<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">inline __device__ float2 GetRoPEres(float data, float data_rotate, const float2 coef){  
    float2 rot_v;
    rot_v.x = coef.x * data - coef.y * data_rotate;
    rot_v.y = coef.x * data_rotate + coef.y * data;
    retern rot_v;  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>入参：<p></p>
<ul>
<li><code>data</code>：<code>head_size</code>中的前半的数据</li>
<li><code>data_rotate</code>：<code>head_size</code>中后半的数据</li>
<li><code>coef</code>：通过<code>GetRoPRfreq()</code>得到的$cos(mθ_i)$和$sin(mθ_i)$<br>变量：</li>
<li>(举例)<code>rot_v.x</code>=$cos(mθ_0)\ <em>\ x_0\ -\ sin(mθ_0)\ </em>\ x_{64}$</li>
<li>(举例)<code>rot_v.y</code>=$cos(mθ_0)\ <em>\ x_{64}\ +\ sin(mθ_0)\ </em>\ x_0$</li>
<li>上面两个为一组<code>rot_v</code>，一组指的是他们共享$cos(mθ_0)$和$sin(mθ_0)$<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/d224a5131c3f2a304c66f0ccf912e90.jpg" alt=""></li>
</ul>
<p>③<code>add_fusedQKV_bias_transpose_kernel()</code><br>实际上并没有加上bias偏置项<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template &lt;typename T&gt;  
__global__ void add_fusedQKV_bias_transpose_kernel(
	T *q_buf,  
	T *k_buf,  
	T *v_buf,  
	T *QKV,  
	const int *padding_offset, // created before qkv linear  
	const int *history_length,  
	const int *input_length, // actual length of each seq  
	const int batch_size,  
	const int seq_len, // max_seq_len to pad to    
	const int head_num,  
	const int kv_head_num,  
	const int head_size,  
	const int rotary_embedding_dim,  
	float rotary_embedding_base // default 10000 in llama  
	)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<p>1)配置block、thread和padding<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int token_id = blockIdx.x;
int head_id = blockIdx.y; 
int tid = threadIdx.x;
int token_padding_offset = padding_offset[token_id];<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><br><code>token_id</code>和<code>head_id</code>用于获得数据偏移量<br><code>token_padding_offset</code>是该<code>token</code>之前的padding个数<p></p>
<p>2)为写到显存里面做准备<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int dst_token_id = token_id + token_padding_offset;
int batch_id = dst_token_id / seq_len;
int local_token_id = dst_token_id % seq_len;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><br><code>dst_token_id</code>：可以理解为当前<code>token_id</code>在全部<code>token</code>中的位置<br>    <code>token_id</code>是当前token在不考虑padding时的token位置<br>    <code>token_padding_offset</code>是当前token之前的padding个数<br><code>batch_id</code>：当前token所在位置的对应的句子id<br><code>local_token_id</code>：当前token在当前句子的位置(0~seq_len-1)<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/8b1b3ff66b49b6249741cc3c0f00791.jpg" alt="|275"><br>为了写到显存里才做的padding<p></p>
<p>3)基于(作为输入)QKV buffer的三个维度(<code>num tokens, qkv head num, head size</code>)获取q、k、v<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int qkv_head_num = head_num + 2 * kv_head_num; 
int q_id = token_id * qkv_head_num * head_size + head_id * head_size + tid;  
int k_id = token_id * qkv_head_num * head_size + head_id * head_size + tid + head_num * head_size;
int v_id = token_id * qkv_head_num * head_size + head_id * head_size + tid + head_num * head_size + kv_head_num * head_size;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><br><code>qkv_head_num</code>：其中<code>head_num</code>是<code>q_head_num</code><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/8d4904053dda536ed4635322867c4fa.jpg" alt="|425"><p></p>
<p>4)计算RoPE<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">const int cur_seq_history_len = history_length[batch_id];
const int context_length = cur_seq_history_len + input_length[batch_id]  
const int timestep = cur_seq_history_len + local_token_id; 
if(tid &gt;= rotary_embedding_dim / 2){ 
    return;  
}  
float2 cos_sin = GetRoPEfreq(tid * 2, rotary_embedding_dim, rotary_embedding_base, timestep);  
float2 q_rotate = GetRoPEres(QKV[q_id], QKV[q_id + head_size / 2], cos_sin);  
float2 k_rotate = GetRoPEres(QKV[k_id], QKV[k_id + head_size / 2], cos_sin);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>这里的长度都以token为单位<br><code>cur_seq_history_len</code>：当前序列的历史的序列长度总和<br><code>context_length</code>：当前序列长度+历史的序列长度<br><code>timestep</code>：历史序列长度+当前seq中的token，得到当前token在整个序列中的位置<p></p>
<p>llama的旋转编码是将head size切分成两半，左一半与右一半对应做RoPE，所以当<code>tid &gt;= rotary_embedding_dim/2</code>时就可以停止做RoPE计算，<code>rotary_embedding_dim</code>是词嵌入向量的维度，这里指的应该是token的维度</p>
<p>在<code>q_rotate</code>和<code>k_rotate</code>的计算过程中也能证实<code>data</code>和<code>data_rotate</code>对应的是线程，所以在上面的if语句中只需要一半的线程即可</p>
<p>5)写回gmem<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int dst_q_id = batch_id * seq_len * head_num * head_size +  
               head_id * seq_len * head_size +  
               local_token_id * head_size + tid;  
int dst_kv_id = batch_id * seq_len * kv_head_num * head_size +  
               head_id * seq_len * head_size +  
               local_token_id * head_size + tid;  
q_buf[dst_q_id] = q_rotate.x;  
q_buf[dst_q_id + head_size / 2] = q_rotate.y;  
if(head_id &lt; kv_head_num){  
    // 对于MQA和GQA  
    k_buf[dst_kv_id] = k_rotate.x;  
    k_buf[dst_kv_id + head_size / 2] = k_rotate.y;  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>下面给出了<code>dst_q_id</code>的例子<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/f602724b51ab74e9a401426a567845f.jpg" alt="|675"><p></p>
<p>④<code>rope_kernel_for_self_decoder()</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
__global__ void rope_kernel_for_self_decoder(T* q,  
                    T* k,  
                    const int batch_size,  
                    const int head_num,  
                    const int kv_head_num,  
                    const int head_size,  
                    const int step,  
                    int   rotary_embedding_dim,  
                    float rotary_embedding_base)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>这里主要针对self decoder<p></p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int tid = threadIdx.x;  
int q_head_id = blockIdx.x;  
int q_batch_id = blockIdx.y;  
int kv_head_id = q_head_id / (head_num / kv_head_num); // 将kv_head_id的数量膨胀到q_head_id的数量  
int kv_batch_id = q_batch_id;  
  
int batch_stride = head_num * head_size; // seq len=1  
int kv_batch_stride = kv_head_num * head_size;
int head_stride = head_size;  
int q_offset = q_batch_id * batch_stride + q_head_id * head_stride + tid;  
int k_offset = kv_batch_id * kv_batch_stride + kv_head_id * head_stride + tid;  
if(tid &gt;= rotary_embedding_dim / 2){  
    return;  
}  
float2 cos_sin = GetRoPEfreq(tid * 2, rotary_embedding_dim, rotary_embedding_base, step - 1); // 这里通过与hf相比发现要-1
float2 q_rotate = GetRoPEres(q[q_offset], q[q_offset + head_size / 2], cos_sin);  
float2 k_rotate = GetRoPEres(k[k_offset], k[k_offset + head_size / 2], cos_sin);  
  
q[q_offset] = q_rotate.x;  
q[q_offset + head_size / 2] = q_rotate.y;  
k[k_offset] = k_rotate.x;  
k[k_offset + head_size / 2] = k_rotate.y;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>最后<code>k[k_offset]</code>不需要判断<code>head_idx&lt;kv_head_num</code>是因为<code>int kv_head_id = q_head_id / (head_num / kv_head_num);</code>这里的对应关系不会令k head越出边界<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/7b5bd7706fad306560cabfdbc75cb21.jpg" alt="|475"></p>
<h2 id="8-2-其他"><a href="#8-2-其他" class="headerlink" title="8.2 其他"></a>8.2 其他</h2><p><code>using Vec_t = Vec&lt;t&gt;::type;</code>和<code>using Vec_t = typename Vec&lt;t&gt;::type;</code>的区别</p>
<ul>
<li>使用<code>typename</code>关键字用来明确告诉编译器<code>Vec&lt;t&gt;::type</code>是一个类型而不是一个(静态)成员</li>
<li>不使用<code>typename</code>的前提是编译器已经确定了<code>Vec&lt;t&gt;::type</code>是一个类型，不需要<code>typename</code>做提示<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">// 需要typename做提示
template&lt;typename T&gt;
struct Vec{
	using Type = T;
}

// 不需要typename做提示
struct Vec{
	using Type = int;
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ul>
<p><code>const_cast</code>主要用于移除(或添加)对象的<code>const</code>限定符，可以修改那些被声明为<code>const</code>的变量</p>
<h1 id="Lesson9-concat-past-kv-cache"><a href="#Lesson9-concat-past-kv-cache" class="headerlink" title="Lesson9 concat past kv cache"></a>Lesson9 concat past kv cache</h1><p>讲解了：<br><code>src/kernels/concat_past_kv.cu</code><br><code>src/kernels/concat_past_kv.h</code><br><code>tests/unittests/test_concat_kv.cu</code></p>
<p>llama中<code>max_q_len</code>(即<code>seq_len</code>)是8192，是关注对象；k和v写到<code>max_q_len</code>需要根据<code>history_len</code>找到相应的位置<br><code>kv cache shape = [num layers, bs, kv_head_num, max_seq_len, head_size]</code><br>↓其中，max_seq_len的位置是写到<br><code>[seqlen[history_len:history_len + max_q_len]]</code></p>
<p>这一节内容不多，但是折磨了我挺长时间的T.T<br>    主要是<code>max_q_len</code>、<code>max_seq_len</code>、<code>history_len</code>、<code>cur_query_len</code>这几个变量没弄明白(可能是视频默认我会吧哈哈)</p>
<ul>
<li><code>max_q_len</code>：做完旋转之后的k、v的对应的每个batch的长度，即token的个数</li>
<li><code>max_seq_len</code>：考虑上下文的每个batch的长度，即token的长度，什么叫考虑上下文呢，就是入参的时候会输入<code>history_len</code>的就是上文长度，<code>max_seq_len</code>作为该batch的最长的长度</li>
<li><code>history_len</code>：这个batch中的上文长度，即token的长度</li>
<li><code>cur_query_len</code>：需要进行查询的长度(新生成的token的长度)<br><code>history_len + cur_query_len &lt;= max_seq_len</code></li>
</ul>
<p>难点就是写入的位置的偏移<code>dst_offset</code>，实际上这一节也是要解决的问题就是kv cache的写入位置，结合代码看下图就好了<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/11adf571dc54da75266cc135ab66c1f.jpg" alt=""><br>👆当layer=1的情况<br>👇这里只放key的，value的和他差不多<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template &lt;typename T&gt;  
__global__ void append_key_cache(T *k_dst, // [num layers, bs, kv head num, max_q_len, head size]  
                                 const size_t layer_offset,  
                                 const T *k_src, // [bs, kv_head num, max_q_len, head size]  
                                 const int kv_head_num,  
                                 const int head_size,  
                                 const int *cur_query_length,  
                                 const int *history_length,// [batch_size]  
                                 const int max_q_len,  
                                 const int max_seq_len){  
    // 根据这里的dim3 grid(max_q_len, batch_size, kv_head_num);来写下面的三行  
    int batch_id = blockIdx.y;  
    int head_id = blockIdx.z;  
    int token_id = blockIdx.x;  
    int tid = threadIdx.x;  
    T* k_cache_dst = k_dst + layer_offset; // 将k写到当前的layer位置，算是一个定位；k_dst是所有k的起始位置  
    int cumsum_seq_len = history_length[batch_id]; // 当前batch在当前layer中累积的句子长度  

    int cur_seq_len = cur_query_length[batch_id];  
    if(token_id &lt; cur_seq_len){  
        // [bs, kv_head_num, max_q_len, head size] =&gt; [bs, kv_head_num, max_seq_len[cumsum_seq_len:cumsum_seq_len + max_q_len], head_size]  
        // 在k_src上的偏移  
        int src_offset = batch_id * kv_head_num * max_q_len * head_size 
				       + head_id * max_q_len * head_size  
                       + token_id * head_size + tid;  
        // 需要写入的位置的偏移  
        int dst_offset = batch_id * kv_head_num * max_seq_len*head_size 
				       + head_id * max_seq_len * head_size 
				       + (cumsum_seq_len + token_id) * head_size + tid;  
        k_cache_dst[dst_offset] = k_src[src_offset]; // k_src是当前layer的，dst_offset需要加上  
    }  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<h1 id="Lesson10-RepeatKV-for-MQA-amp-GQA-kernel"><a href="#Lesson10-RepeatKV-for-MQA-amp-GQA-kernel" class="headerlink" title="Lesson10 RepeatKV for MQA&amp;GQA kernel"></a>Lesson10 RepeatKV for MQA&amp;GQA kernel</h1><p>讲解了：<br><code>src/kernels/repeat_kv.cu</code><br><code>src/kernels/repeat_kv.h</code><br><code>test/unittests/test_repeat_kv.cu</code></p>
<p>写这个kernel的动机：将MHA转换为MQA，目的是平衡推理速度和MHA所能达到的精度；因为k和v的数量与头数量成正比，所以要减小头的数量和size以减小带宽压力，同时因为后面要做QKgemm，因此要矩阵对齐</p>
<p>尺寸变化：<br><code>[batch size, kv head num, max seq len, head size]=&gt;</code><br><code>[batch size, q head num, max k len, head size]</code></p>
<p><code>q_head_per_kv = head_num / kv_head_num</code>，即每一组k head或v head对应多少组q head共用</p>
<p><code>dim3 grid((max_k_len * head_size + blockSize - 1) / blockSize, batch_size, head_num);</code><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/6a7c0ad2ea6497df0a680878fb4c32d.jpg" alt=""></p>
<p><code>src/kernels/repeat_kv.cu</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template &lt;typename T&gt;  
__global__ void repeat_value_cache(T *v_dst,  
                                   const T *v_src,  
                                   const size_t layer_offset,  
                                   const int head_num,  
                                   const int q_head_per_kv,  
                                   const int head_size,  
                                   const int *context_length,  
                                   const int max_k_len,  
                                   const int max_seq_len){  
    const int batch_id = blockIdx.y;  
    const int head_id = blockIdx.z;  
    const int gtid = blockIdx.x * blockDim.x + threadId.x;  
    const auto val_src = v_src + layer_offset;  
    const T* val_dst = v_dst;  
    const int seq_len = context_length[batch_id];  
    const int v_head_size_id = gtid % head_size; 
    const int v_seq_len_id = gtid / head_size;  

    if(v_seq_len_id &lt; seq_len){  
        const int src_id = batch_id * (head_num / q_head_per_kv）*
					       head_size * max_seq_len +  
                           head_id / q_head_per_kv * head_size * 
                           max_seq_len +  
                           v_seq_len_id * head_size +  
                           v_head_size_id;  
  
        const int dst_id = batch_id * head_num * head_size * max_k_len + 
                           head_id * head_size * max_seq_len +  
                           v_seq_len_id * head_size +  
                           v_head_size_id;  
        val_dst[dst_id] = val_src[src_id];  
    }}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/ef1ab04ddb1556e1cd39bef5cf2ac47.jpg" alt=""><br>实际上就是按照<code>q head</code>的大小重新排布了<code>k head</code>或<code>v head</code>，使他们一一对应。(图中绿色部分为对应关系，每<code>q_head_num/kv_head_num</code>组<code>q hea</code>d共用一组<code>k head</code>或<code>v head</code>)<p></p>
<p>总觉得这里的<code>max_k_len</code>有点误导人…应该不是<code>kv head num * max seq len = q head num * max k len</code>，只是单纯的扩展了</p>
<h1 id="Lesson11-Fused-mask-amp-softmax"><a href="#Lesson11-Fused-mask-amp-softmax" class="headerlink" title="Lesson11  Fused mask&amp;softmax"></a>Lesson11  Fused mask&amp;softmax</h1><p>讲解了：<br><code>src/kernels/attn_softmax_kernel.cu</code><br><code>src/kernels/attn_softmax_kernel.h</code></p>
<p><code>SumOp</code>和<code>MaxOp</code>的定义<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template &lt;typename T&gt;  
struct SumOp  
{  
    __device__ __forceinline__ T operator()(const T &amp;a, const T &amp;b) const { return a + b; }  
};  
  
template &lt;typename T&gt;  
struct MaxOp  
{  
    __device__ __forceinline__ T operator()(const T &amp;a, const T &amp;b) const { return max(a, b); }  
};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>👆这样写的目的是模板化<p></p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template &lt;template &lt;typename&gt; class ReductionOp, typename T&gt;  
__inline__ __device__ T warpReduce(T val)  
{  
    for (int mask = 32 / 2; mask &gt; 0; mask /= 2)  
    {        
	    val = ReductionOp&lt;T&gt;()(val, __shfl_xor_sync(0xffffffff, val, mask));  
    }    
    return val;  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>👆使用模板模板参数<code>ReductionOp</code>，在调用<code>warpReduce</code>时传入不同的操作类型<code>SumOp</code>和<code>MaxOp</code></p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">LLM-CHECK_WITH_INFO(k_length % 2 == 0, "K_len should be divided by 2 under half type!");<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>fp32类型下以float4力度读写（还未实现），fp16类型下以half2读写，这里是只对fp16做向量化使其<code>vec_size=2</code>，而fp32向量化后<code>vec_size=1</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">#define LAUNCH_SOFTMAX(dtype, vec_size)                                \ 
    if (block.x &gt; 2048 &amp;&amp; block.x &lt;= 4096)                             \ 
    {                                                                  \ 
        constexpr int NUMS_PER_THREAD_PER_ROW = 4;                     \ 
        block.x /= 4 * vec_size;                                       \ 
        block.x = (block.x + 32 - 1) / 32 * 32;                        \ 
        assert(block.x &lt; 1024);                                        \ 
        ScaleMaskAndSoftmax_##dtype&lt;dtype, NUMS_PER_THREAD_PER_ROW&gt;    \&lt;&lt;&lt;grid, block&gt;&gt;&gt;((dtype *)attn_score-&gt;data, \                                             (dtype *)qk-&gt;data,         \  
	              (dtype *)mask-&gt;data,       \                                             batch_size,                \                                             head_nums,                 \                                             q_length,                  \                                             k_length,                  \               
	              scale);                    \  
    }<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li><code>NUMS_PER_THREAD_PER_ROW</code>作为编译器常量</li>
<li>如果当前输入的shape比较大，每个线程只访问4个vec，即<code>.x</code>、<code>.y</code>、<code>.z</code>、<code>.w</code>这种，所以<code>block.x</code>被分为<code>4*vec_size</code>份<ul>
<li>其中，<code>vec_size</code>对于<code>half</code>来说取2，对于<code>float</code>来说取1</li>
</ul>
</li>
<li>同时block个数仍需对齐32，向上取整</li>
<li>整体看来就是用较少的线程处理数据，如果输入shape太大就采用输入向量化（目前只实现了fp16）并且减少线程使用</li>
</ul>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template &lt;typename T, int NUMS_PER_THREAD_PER_ROW&gt;  
__global__ void ScaleMaskAndSoftmax_float(T *attn_score,  
                                          T *qk,  
                                          T *mask,  
                                          int batch_size,  
                                          int head_nums,  
                                          int q_len,  
                                          int k_len,  
                                          float scale){  
    int batch_id = blockIdx.y;
    int head_id = blockIdx.z; 
    if(threadIdx.x &gt;= k_len){  
        return;  
    }    
    __shared__ float s_max, inv_sum;  
    for(int row_start = 0; row_start &lt; q_len; row_start++){  
        int qk_offset = 0;  
        T qk_data = static_cast&lt;T&gt;(0);  
        T mask_data = static_cast&lt;T&gt;(0);  
        T data[NUMS_PER_THREAD_PER_ROW];  
        T thread_max = FIL_MIN;  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>在launch中<ul>
<li>grid=<code>[q_length, batch_size, head_nums]</code></li>
<li>block=<code>[k_length(以32的倍数向上取整)]</code></li>
</ul>
</li>
<li>开始处理所有行</li>
</ul>
<p>以下全都在上一层的<code>for</code>的内部，为便于看代码因此忽略部分缩进<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">for (int col_start = 0; col_start &lt; NUMS_PER_THREAD_PER_ROW; col_start++){ 
	// 每个线程只需要处理NUMS_PER_THREAD_PER_ROW个数据  
	qk_offset = batch_id * head_nums * q_len * k_len + 
			    head_id * q_len * k_len + row_start * k_len + 
			    col_start * blockDim.x + threadIdx.x;  
	qk_data = qk[qk_offset];  
	mask_offset = batch_id * q_len * k_len + head_id * q_len * k_len 
				  + row_start * k_len + col_start * blockDim.x 
				  + threadIdx.x;  
    mask_data = mask[mask_offset];  
  
    data[col_start] = scale * qk_data + (1 - mask_data) * -1e4f;  
    thread_max = fmax(data[col_start], thread_max); // 一个线程对多个元素做处理之后，多个元素的最大值  
}  
T max_val = blockReduce&lt;MaxOp, T&gt;(thread_max); // 一行的最大值  
// block的最大值存在id为0的线程中  
if(threadIdx.x == 0){  
    s_max = max_val;  
}        
__syncthreads();  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li>列被分为<code>NUMS_PER_THREAD_PER_ROW</code>个数据由同一个线程处理</li>
<li>每遍历一次<code>col_start</code>就会有相应的线程并行，之后再用<code>blockReduce</code>进行最后的规约</li>
<li><code>mask_data</code>和<code>qk_data</code>不同的地方是没有<code>head_nums</code>，其他都一致<ul>
<li>如果<code>mask_data=1</code>，说明不需要被mask，反之需要被mask(加上$-10^4$，这使得在softmax时得到的值非常的小)<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/8616eaf5c62443e35305f7cfd27a345.jpg" alt=""><br>考虑到数值范围的溢出问题，一般会在指数部分减去<code>D=max(zi)</code><br>softmax的公式为：$D=max(z_i),softmax(z_i)=\dfrac{e^{z_i-D}}{\sum^C_{c=1}e^{z_c-D}}$<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">T thread_sum = 0.0f;  
for(int col_start = 0; col_start &lt; NUMS_PER_THREAD_PER_ROW; col_start++){  
    data[col_start] = expf(data[col_start] - s_max);  
    thread_sum += data[col_start];  
}        
T sum_val = blockReduce&lt;SumOp, T&gt;(thread_sum);  
if(threadIdx.x == 0){  
    inv_sum = 1 / (sum_val + 1e-6);  
}       
__syncthreads();  
for(int col_start = 0; col_start &lt; NUMS_PER_THREAD_PER_ROWl;col_start++) {  
	qk_offset = batch_id * head_nums * q_len * k_len + head_id * q_len * 
				k_len + row_start * k_len + col_start * blockDim.x + 
				threadIdx.x;  
attn_score[qk_offset] = (data[col_start] * inv_sum);  
}  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ul>
</li>
</ul>
<p>对于fp16，不同的地方在于向量化处理<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">//scalar_cast_vec: 将常量转换为2个或4个向量  
Vec_t ONE = scalar_cast_vec&lt;Vec_t&gt;(__float2half(1.0f));  
Vec_t NEG_INF = scalar_cast_vec&lt;Vec_t&gt;(__float2half(-10000.0f));  
Vec_t scale_vec = scalar_cast_vec&lt;Vec_t&gt;(__float2half(scale));<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><br>根据<code>src/utils/vectorze_utils.h</code>：half-&gt;half2 ，float-&gt;float4<p></p>
<p>在<code>src/utils/vectorize_utils.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T_OUT, typename T_IN&gt;  
inline __decvice__ T_OUT scalar_cast_vec(T_IN val){  
    return val;  
}  
// half转为half2  
template&lt;&gt;  
inline __device__ half2 scaler_cast_vec&lt;half2, half&gt;(half val){  
    return __half2half2(val);  
}  
// float转为float2  
template&lt;&gt;  
inline __device__ float2 scaler_cast_vec&lt;float2, float&gt;(float val){  
	return __make_float2(val, val);  
}  
// float转为float4  
template&lt;&gt;  
inline __device__ float4 scaler_cast_vec&lt;float4, float&gt;(float val){  
    return __make_float4(val, val, val, val);  
}  
// float转为half2  
template&lt;&gt;  
inline __device__ float2 scaler_cast_vec&lt;half2, float&gt;(float val){  
    return __float2half2_rn(val);  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>还有一部分直接用库中half2函数进行计算处理<p></p>
<h1 id="Lesson12-Fused-transpose-amp-remove-padding"><a href="#Lesson12-Fused-transpose-amp-remove-padding" class="headerlink" title="Lesson12 Fused transpose&amp;remove padding"></a>Lesson12 Fused transpose&amp;remove padding</h1><p>讲解了：<br><code>src/kernels/fused_transpose_and_remv_pad.cu</code><br><code>src/kernels/fused_transpose_and_remv_pad.h</code></p>
<p><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/1ebaf4bf0c8c8e05cdc563c83f71a77.jpg" alt=""><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template &lt;typename T&gt;  
__global__ void fused_transpose_reshape_remv_pad(T *src,  
                                                 T *dst,  
                                                 const int num_tokens,  
                                                 const int batch_size,  
                                                 const int seq_len,  
                                                 const int head_num,  
                                                 const int head_size,  
                                                 const int *padding_offset /*for remove padding*/)  
{  
    int token_id = blockIdx.x; // 这里的token_id是指padding之前的每个token的id  
    int batch_id = token_id + padding_offset[token_id] / seq_len; // 这里的batch_id是指padding之后每个token对应的batch的id  
    int seq_id = token_id + padding_offset[token_id] % seq_len;   // 每个token在句子中的编号，范围是0~seq_len-1  
    // transpose前后的offset  
    int src_offset = batch_id * head_num * seq_len * head_size + seq_id * head_size; // transpose前的偏移位置，具体到head_size的偏移，这里把head_id * seq_len * head_size去掉了，会在for循环补上  
    int dst_offset = token_id * head_num * head_size; // 这里的偏移只具体到token  
  
    for(int i = threadIdx.x; i &lt; head_num * head_size; i+=blockDim.x){ // 因为每个block处理一个token，所以i+=blockDim.x  
        int head_id = i / head_size;  
        int head_size_id = i % head_size;  
        dst[dst_offset + i] = src[src_offset + i * seq_len * head_size + head_size_id];  
    }
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>代码比较容易理解，不懂的看注释即可<p></p>
<h1 id="Lesson13-Fused-addResidualNorm"><a href="#Lesson13-Fused-addResidualNorm" class="headerlink" title="Lesson13 Fused addResidualNorm"></a>Lesson13 Fused addResidualNorm</h1><p>讲解了：<br><code>src/fused_addresidual_norm.cu</code><br><code>src/fused_addresidual_norm.h</code></p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
__global__ void FusedAddBiasResidualRMSNorm( // residual.shape = [num tokens, hidden_units]  
                    T* residual,    // [num tokens, hidden_units]  
                    T* decoder_in,  // [num tokens, hidden_units]  
                    /*optional*/const T* bias,  // [hidden_units]  
                    const T* scale, // [hidden_units], RMSNorm weights  
                    float eps,      // RMSNorm eps  
                    int num_tokens,   
                    int hidden_units){  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>rmsnorm(decoder_in + residual + bias)</p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">// grid:[num_tokens] block:[num_threads]    int vec_size = Vec&lt;T&gt;::size;  
    using Vec_t = typename Vec&lt;T&gt;::Type;  
    int batch_id = blockIdx.x; // 一个block表示一个batch  
    int tid = threadIdx.x;  
    Vec_t *de_out = reinterpret_cast&lt;Vec_t*&gt;(decoder_in + batch_id * hidden_units); 
    Vec_t *rsd = reinterpret_cast&lt;Vec_t*&gt;(residual + batch_id * hidden_units);  
    Vec_t *bia;  
    if(bias != nullptr){  
        bia = reinterpret_cast&lt;Vec_t*&gt;(bias);  
    }    Vec_t tmp;  
    T thread_sum = static_cast&lt;T&gt;(0.0f);  
    for (int i = threadIdx.x; i &lt; hidden_units / vec_size; i += blockDim.x) {  
        if(residual != nullptr){  
            // 下面对应HF中的hidden_states = residual + hidden_states  
            de_out[i].x += rsd[i].x;  
            de_out[i].y += rsd[i].y;  
            de_out[i].z += rsd[i].z;  
            de_out[i].w += rsd[i].w;  
            // 下面对应residul = hidden_states            
            rsd[i].x = de_out[i].x;  
            rsd[i].y = de_out[i].y;  
            rsd[i].z = de_out[i].z;  
            rsd[i].w = de_out[i].w;  

        }        
        if(bias != nullptr){  
            de_out[i].x += bia[i].x;  
            de_out[i].y += bia[i].y;  
            de_out[i].z += bia[i].z;  
            de_out[i].w += bia[i].w;  
        }  
        thread_sum += de_out[i].x * de_out[i].x;  
        thread_sum += de_out[i].y * de_out[i].y;  
        thread_sum += de_out[i].z * de_out[i].z;  
        thread_sum += de_out[i].w * de_out[i].w;  
    }  
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li><code>Vec_t *de_out = reinterpret_cast&lt;Vec_t*&gt;(decoder_in + batch_id * hidden_units)</code>：每个block表示一个token，每个token的大小为hidden_units，这里表示了当前token的偏移量 </li>
<li>在HF中的顺序<br><code>hidden_states = residual + hidden_states</code>对应<code>de_out[i].x += rsd[i].x;</code><br><code>residul = hidden_states</code>对应<code>rsd[i].x = de_out[i].x;</code><br><code>hidden_states = self.post_attention_layernorm(hidden_states)</code>对应<code>de_out[idx].x = de_out[idx].x * inv_mean * s[idx].x;</code></li>
<li>根据公式$\dfrac{x_i×g_i}{\sqrt{\sum^iE(x_i^2)+eps}}$<ul>
<li>$x_i$对应加了<code>residual</code>的<code>de_out[i]</code></li>
<li>$g_i$对应<code>s[idx]</code></li>
</ul>
</li>
</ul>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">    // 求分母，以1/xxx表示
    T block_sum = blockReduceSum&lt;float&gt;(thread_sum);  
    __shared__ float inv_mean;  
    if (threadIdx.x == 0) {  
        inv_mean = rsqrtf(block_sum / hidden_units + eps);  
    }    __syncthreads(); 
  
    // rmsnorm  
    Vec_t *s;  
    if(scale != nullptr) {  
        s = reinterpret_cast&lt;Vec_t *&gt;(scale);  
    }    
    for (int idx = threadIdx.x; idx &lt; hidden_units / vec_size; idx += blockDim.x) {  
        de_out[idx].x = de_out[idx].x * inv_mean * s[idx].x; 
        de_out[idx].y = de_out[idx].y * inv_mean * s[idx].y; 
        de_out[idx].z = de_out[idx].z * inv_mean * s[idx].z;  
        de_out[idx].w = de_out[idx].w * inv_mean * s[idx].w;  
    }
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h1 id="Lesson-14-Gate-Linear-amp-Up-Linear"><a href="#Lesson-14-Gate-Linear-amp-Up-Linear" class="headerlink" title="Lesson 14 Gate Linear&amp;Up Linear"></a>Lesson 14 Gate Linear&amp;Up Linear</h1><p>讲解了：<br><code>src/kernels/linear</code></p>
<p>输入：<br>    为context decoder时，<code>[batch_size, q hidden units]</code>；<br>    为self decoder时，<code>[token nums, q hidden units]</code><br>Gate&amp;Up权重：<code>[q hidden units, 2 * inter size]</code><br>输出：<code>[batch_size(或token nums), 2 * inter size] = [bs/tn, 2, inter size]</code>，实际上输出是三维</p>
<h1 id="Lesson-15-SwiGLU"><a href="#Lesson-15-SwiGLU" class="headerlink" title="Lesson 15 SwiGLU"></a>Lesson 15 SwiGLU</h1><p>讲解了：<br><code>src/kernels/act_kernel.h</code><br><code>src/kernels/act_kernel.cu</code></p>
<p><code>SiLU(Sigmoid Linear Unit)</code>，相对于ReLU，SiLU在函数接近0时具有更平滑的曲线<br>$y=x*sigmoid(\beta x)=\dfrac{1}{1+e^{-\beta x}}$，当$\beta=1$时就是SiLU<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/Pasted%20image%2020241105112111.png" alt=""><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;
__device__ __forceinline__ T silu(const T&amp; in){
	return in / (1.0f * expf(-in));
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<p><code>grid:[batch_size=input-&gt;shape[0]]</code><br><code>block:[256]</code></p>
<p><code>Gate Linear</code>和<code>Up Linear</code>的输出(对于context decoder而言)<code>[bs, 2, inter size]</code>可以视为两个大小为<code>[bs, inter size]</code>的部分，第一部分做<code>SiLU</code>，得到的结果与第二部分做<code>mul</code>最终得到最后的结果<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
__global__ void silu_and_mul_kernel(
					T* out, // shape: [bs, intermedia size]  
					const T* input,  // shape: [bs, 2, intermedia size]  
	                const int intermedia_size) {  
    const int batch_idx = blockIdx.x;  
    for(int idx = threadIdx.x; idx &lt; intermedia_size; idx +=blockDim.x){ 
        const T x = input[batch_idx * 2 * intermedia_size + idx];// 第一个 
        const T y = input[batch_idx * 2 * intermedia_size + intermedia_size + idx]; // 第二个  
        out[batch_idx * intermedia_size + idx] = silu&lt;T&gt;(x) * y;  
    }
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<h1 id="Lesson16-Fused-SelfDecoderAttention-kernel"><a href="#Lesson16-Fused-SelfDecoderAttention-kernel" class="headerlink" title="Lesson16 Fused SelfDecoderAttention kernel"></a>Lesson16 Fused SelfDecoderAttention kernel</h1><p>讲解了：<br><code>src/fused_decoder_self_attention.cu</code></p>
<p>融合部分：<code>concat kv</code>+<code>repeat kv</code>+<code>qk gemv</code>+<code>softmax</code>+<code>qk*v gemv</code></p>
<ul>
<li>如何fuse：数据在寄存器(如<code>q</code>、<code>k</code>和<code>v</code>)和显存(如<code>q_buf</code>、<code>k_buf</code>和<code>v_buf</code>)都出现，因此需要复用在寄存器和共享内存中的数据，因为访问显存会耗时，并且带宽很低</li>
<li>使用动态共享内存</li>
<li><code>Q*k Gemv</code>：<ul>
<li><code>q.shape=[batch size, head num, 1, head size]</code><ul>
<li>这里的<code>1</code>表示每次针对一个特定位置(当前token)计算attention</li>
</ul>
</li>
<li><code>k.shape=[batch size, head num, step, head size]</code>，<ul>
<li>这里不是<code>kv head num</code>，是因为在<code>repeat kv</code>这一步中已经把q和k的头对齐了</li>
<li>这里的<code>step</code>表示每个句子包含<code>step</code>个<code>token</code>，每个<code>token</code>的key都与当前查询向量<code>q</code>做点积</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>重温：</p>
<ul>
<li>qkv矩阵的shape<ul>
<li>q<code>[batch size, q head num, 1, head size]</code></li>
<li>k<code>[batch size, kv head num, step(/seqlen), head size]</code></li>
<li>v<code>[batch size, kv head num, step(/seqlen), head size]</code></li>
</ul>
</li>
</ul>
<p><code>launchDecoderMaskedMHA()</code></p>
<ul>
<li><code>qkv_buf</code>：<code>[batch size, qkv head num, head size]</code>，默认<code>head_num</code>是q的head，qkv、kv的head会加上相应的前缀</li>
<li>用<code>getVal</code>的前提是数据必须在CPU上(<code>LLM_CHECK(location == CPU)</code>)</li>
<li>grid：<code>[head_num, batch_size]</code></li>
<li>block：<code>[head_size]</code></li>
</ul>
<p>入参：<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
void launchDecoderMaskedMHA(TensorWrapper&lt;T&gt;* qkv_buf,
                            BaseWeight&lt;T&gt;&amp; qkv, 
                            TensorWrapper&lt;int&gt;* layer_id,  
                            TensorWrapper&lt;T&gt;* k_cache,  
                            TensorWrapper&lt;T&gt;* v_cache,  
                            TensorWrapper&lt;bool&gt;* finished, 
                            TensorWrapper&lt;int&gt;* step, 
                            TensorWrapper&lt;T&gt;* mha_output,  
                            LLaMAAttentionStaticParams&amp; static_params){ <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li><code>qkv_buf=qkv_linear=[bs, q_hidden_units] * [qhiddenunits, hiddenunits] = [bs, qkv_head_num, head_size]</code><ul>
<li><code>qhiddenunits</code>：将输入的嵌入向量(embedding vector)的向量长度，</li>
<li><code>hiddenunits</code>：<code>=[qkv_head_num,qiddenunist]=[qkv_head_num,head_size]</code><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/22bbf7d7781de208731d147b46c3b5e%201.jpg" alt="|550"></li>
</ul>
</li>
<li>kv的cache<ul>
<li>k_cache<code>[num layers, bs, kv head num, max seq len or step, head size]</code></li>
<li>v_cache<code>[num layers, bs, kv head num, max seq len or step, head size]</code><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">    const int qkv_head_num = qkv_buf-&gt;shape[1];  
    const int kv_head_num = k_cache-&gt;shape[2];  
    const int max_seq_len = k_cache-&gt;shape[3];   
int head_num = qkv_head_num - 2 * kv_head_num;  
    const int head_size = qkv_buf-&gt;shape[2];  
    const int cur_step = step-&gt;getVal();
    const int layer = layer_id-&gt;getVal();  
    const int layer_offset = layer * max_seq_len * batch_size * kv_head_num * head_size;  
    size_t smem_size_bytes = head_size * sizeof(T) + cur_step * sizeof(float);  
    T* qkv_data = qkv_buf-&gt;data;  
    T* q = qkv_data;
    T* k = qkv_data + head_num * head_size;  
    T* v = qkv_data + (head_num + kv_head_num) * head_size;  
  
    int   rotary_embedding_dim = static_params.rotary_embedding_dim;  
    float rotary_embedding_base = static_params.rotary_embedding_base;  
    int   max_position_embeddings = static_params.max_position_embeddings;  
    bool  use_dynamic_ntk = static_params.use_dynamic_ntk;  
    dim3 grid(head_num, batch_size);  
    dim3 block(head_size); //vec size = 4 for fp32  
    masked_MHA_kernel&lt;T&gt;&lt;&lt;&lt;grid, block, smem_size_bytes&gt;&gt;&gt;(  
										q,  
										k,  
										v,  
										// /*(T*)*/qkv.bias,  
										k_cache-&gt;data + layer_offset,  
										v_cache-&gt;data + layer_offset,  
										mha_output-&gt;data,  
										batch_size,  
										head_num,  
										kv_head_num,  
										max_seq_len,  
										head_size,  
										cur_step,  
										rotary_embedding_dim,  
										rotary_embedding_base);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
</ul>
</li>
<li><code>q、k、v</code>：<code>qkv_buf=[bs, qkv_head_num, head_size]</code>，<code>q、k、v</code>分别加上相应偏移量</li>
<li><code>k_cache、v_cache</code>：定位到某一个<code>layer</code>上，不考虑<code>layer</code>时的<code>shape</code>为<code>[bs, kv head num, max seq len or step, head size]</code></li>
<li><code>mha_output-&gt;data</code>：作为输出地址</li>
<li><code>cur_step</code>：当前时间步，当前生成到第几个token</li>
<li><code>rotary_embedding_dim、rotary_embedding_base</code>：RoPE用</li>
</ul>
<p><code>masked_MHA_kernel()</code><br>入参：<br></p><pre class="line-numbers language-c" data-language="c"><code class="language-c">template<span class="token operator">&lt;</span>typename T<span class="token operator">&gt;</span>  
__global__ <span class="token keyword">void</span> <span class="token function">masked_MHA_kernel</span><span class="token punctuation">(</span><span class="token keyword">const</span> T<span class="token operator">*</span> q<span class="token punctuation">,</span>  
                    <span class="token keyword">const</span> T<span class="token operator">*</span> k<span class="token punctuation">,</span>  
                    <span class="token keyword">const</span> T<span class="token operator">*</span> v<span class="token punctuation">,</span>  
                    T<span class="token operator">*</span> qkv_bias<span class="token punctuation">,</span>  
                    T<span class="token operator">*</span> k_cache<span class="token punctuation">,</span>  
                    T<span class="token operator">*</span> v_cache<span class="token punctuation">,</span>  
                    T<span class="token operator">*</span> mha_output<span class="token punctuation">,</span>  
                    <span class="token keyword">const</span> <span class="token keyword">int</span> batch_size<span class="token punctuation">,</span>  
                    <span class="token keyword">const</span> <span class="token keyword">int</span> head_num<span class="token punctuation">,</span>  
                    <span class="token keyword">const</span> <span class="token keyword">int</span> kv_head_num<span class="token punctuation">,</span>  
                    <span class="token keyword">const</span> <span class="token keyword">int</span> max_seq_len<span class="token punctuation">,</span>  
                    <span class="token keyword">const</span> <span class="token keyword">int</span> head_size<span class="token punctuation">,</span>  
                    <span class="token keyword">const</span> <span class="token keyword">int</span> step<span class="token punctuation">,</span>  
                    <span class="token keyword">int</span>   rotary_embedding_dim<span class="token punctuation">,</span>  
                    <span class="token keyword">float</span> rotary_embedding_base<span class="token punctuation">)</span><span class="token punctuation">{</span><span class="token comment">// rsqrt(dh)  </span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li><code>k_offset</code>和<code>cache_offset</code>区别：<ul>
<li><code>k_offset</code>是qkv linear提供给k的，(因为是self_attention所以)一个batch只有一个token</li>
<li><code>cache_offset</code>是kv cache提供给k的，有<code>max seq len</code>，一个batch最多有max seq len个token(有这么多是因为新生成的token的k、v也加上去了)</li>
</ul>
</li>
<li>以<code>tid * vec_size &lt; head_size</code>作为是否超出边界的判断<ul>
<li><code>head_size</code>一般是4、8、16的倍数，所以当<code>vec_size</code>为2或4时也能正常判断</li>
<li>(抛开倍数问题会觉得不能正常判断的原因是：<code>head_size=7</code>，当<code>tid(=1)*vec_size(=4)</code>时，<code>4&lt;7</code>此时判断未超出边界，但是一共有<code>2×4=8</code>已经超出边界了)</li>
</ul>
</li>
<li>输出：`mha_output.shape=[batch_size, q_head_num, 1, head_size]</li>
</ul>
<p>①ConcatPastKVCache<br><code>input=[bs, kv head num, seqlen, head size]</code><br><code>output=[bs, kv head num, max_seq_len, head size]</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">int tid = threadIdx.x;  
   int q_head_id = blockIdx.x;  
   int q_batch_id = blockIdx.y;  
   int kv_head_id = q_head_id / (head_num / kv_head_num);  
   int kv_batch_id = q_batch_id;  
   
   int batch_stride = head_num * head_size;  
   int kv_batch_stride = kv_head_num * head_size;  
   int head_stride = head_size;  
 
   int q_offset = q_batch_id * batch_size + q_head_id * head_stride + tid;  
   // k_offset是qkv linear提供给k的  
   int k_offset = kv_batch_id * kv_batch_stride + kv_head_id * head_stride + tid;
   // cache_offset是kv cache提供给k的  
   int cache_offset = kv_batch_id*kv_head_num*max_seq_len*head_size 
				 + kv_head_id * max_seq_len * head_size 
				 + tid * vec_size;//没有seq len的维度是因为seq len始终为1  
   int step_stride = head_size;  
 
   float scale = rsqrt((float)head_size);  
 
   int vec_size = Vec&lt;T&gt;::size;  
   int q_offset_vec = q_batch_id * batch_size + q_head_id * head_stride + tid * vec_size;  
   int k_offset_vec = kv_batch_id * kv_batch_stride + kv_head_id * head_stride + tid * vec_size;  
   using Vec_t = typename Vec&lt;T&gt;::Type; <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<p>②声明动态共享内存变量<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">const T* q_mem = q;  
const T* k_mem = k;  
const T* v_mem = v;  
if(tid * vec_size &lt; head_size){  
    qvec = *reinterpret_cast&lt;Vec_t*&gt;(const_cast&lt;T*&gt;(&amp;q_mem[q_offset_vec]));  
    kvec = *reinterpret_cast&lt;Vec_t*&gt;(const_cast&lt;T*&gt;(&amp;k_mem[k_offset_vec]));  
    vvec = *reinterpret_cast&lt;Vec_t*&gt;(const_cast&lt;T*&gt;(&amp;v_mem[v_offset_vec]));  
}  
extern __shared__ char sqk[]; // 声明动态共享内存变量  
// shared memory的分配  
// 存到shared memory中的数据的特点是低延迟、高复用  
// 在这里对q用shared memory进行存储是因为之后有个优化，使用一个block取多行k进行qk gemm，此时q的复用频率变高，不需要重复加载q  
T* sq_scalar = reinterpret_cast&lt;T*&gt;(sqk);  
float* logits = reinterpret_cast&lt;float*&gt;(sq_scalar + head_size);  
Vec_t *sq = reinterpret_cast&lt;Vec_t*&gt;(sq_scalar);  
  
if(tid * vec_size &lt; head_size){  
    sq[tid] = qvec;  
}    __syncthreads();  
float zero = 0.0f;  
Vec_t zero_f4 = scalar_cast_vec&lt;Vec_t, T&gt;(zero); // 将float转为float4  
float4 scale_f4 = scalar_cast_vec&lt;float4, float&gt;(scale);  
  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">// q*k gemv  
for(int iter = 0; iter &lt; step; iter++){ //一个block循环计算step行  
    Vec_t kvec_qk = tid * vec_size &lt; head_size ? *reinterpret_cast&lt;Vec_t*&gt;(&amp;k_cache[iter * step_stride + cache_offset]) : zero_f4; // 这里乘iter相当于乘max seq len。我的理解是cache_offset是对于token而言的，iter*cache_offset的偏移使定位到当前step(当前token)  
  
    if(iter == step - 1 &amp;&amp; tid * vec_size &lt; head_size){ // step的最后一个位置存储RoPE输出的k  
        *reinterpret_cast&lt;Vec_t*&gt;(&amp;k_cache[iter * step_stride + cache_offset]) = kvec;  
        kvec_qk = kvec; // 这里的kvec_qk是用来做计算的，下面的vvec_qkc同理  
    }  
  
    Vec_t qk = zero_f4;  
    qk.x = tid * vec_size &lt; head_size ? sq[tid].x * kvec_qk.x * scale_f4.x : zero;  
    qk.y = tid * vec_size &lt; head_size ? sq[tid].y * kvec_qk.y * scale_f4.y : zero;  
    qk.z = tid * vec_size &lt; head_size ? sq[tid].z * kvec_qk.z * scale_f4.z : zero;  
    qk.w = tid * vec_size &lt; head_size ? sq[tid].w * kvec_qk.w * scale_f4.w : zero;  
  
    T qk_acc = qk.x + qk.y + qk.z + qk.w; // 一个线程有4个值，先在线程局部把这四个值加起来，再用blockReduceSum  
    T attn_score = blockReduceSum&lt;T&gt;(qk_acc);  
    if(tid == 0){  
        logits[iter] = attn_score; // logits是step×1大小的数组  
    }  
    __syncthreads();  
}  
// softmax    T local_logits = tid &lt; step ? (T)logits[tid] : 0;  
__shared__ float row_max, fenmu;  
T block_max = blockReduceMax&lt;T&gt;(local_logits);  
if(tid == 0){  
    row_max = block_max;  
}    __syncthreads();  
T fenzi = tid &lt; step ? expf(logits[tid] - row_max) : 0; // e(x_i - x-max) / sigma(e(x_i, x_max));  
T block_fenmu = blockReduceSum&lt;T&gt;(fenzi);  
if(tid == 0){  
    fenmu = block_fenmu + 1e-6;  
}    __syncthreads();  
if(tid &lt; step){  
    logits[tid] = (T)(fenzi / fenmu);  
}    __syncthreads();  
  
// 隐式的repeat kv，都是向量化类型  
if(tid * vec_size &lt; head_size){  
    Vec_t O = scalar_cast_vec&lt;Vec_t, T&gt;(0.0f); // 中间寄存器  
    for(int iter = 0; iter &lt; step; iter++){  
        Vec_t vvec_qkv = *reinterpret_cast&lt;Vec_t*&gt;(&amp;v_cache[iter * step_stride + cache_offset]);  
  
        if(iter == step - 1){ // step的最后一个位置存储RoPE输出的k  
            *reinterpret_cast&lt;Vec_t*&gt;(&amp;v_cache[iter * step_stride + cache_offset]) = vvec;  
            vvec_qkv = vvec;  
        }            __syncthreads();  
        O.x += vvec_qkv.x * logits[iter]; // v的一整行×qk的一个  
        O.y += vvec_qkv.y * logits[iter]; // v的一整行×qk的一个  
        O.z += vvec_qkv.z * logits[iter]; // v的一整行×qk的一个  
        O.w += vvec_qkv.w * logits[iter]; // v的一整行×qk的一个  
    }  
    *reinterpret_cast&lt;Vec_t*&gt;(&amp;mha_output[q_offset]) = O; // [batch size, q head num, 1, head size]  
}}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<h1 id="Lesson17-topK"><a href="#Lesson17-topK" class="headerlink" title="Lesson17 topK"></a>Lesson17 topK</h1><p>讲解了：<br><code>src/kernels/topK.cu</code><br><code>src/kernels/topK.h</code></p>
<p>输入：<code>[bs, beam_width, vocab size]</code><br>输出：<code>[bs, beam_width, K]</code></p>
<p>topK中的K是从一组候选中选取得分最高的前K个值<br>beam_width是指保留的候选路径数<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/Pasted%20image%2020241117225342.png" alt="|450"></p>
<p>目的：每个vocab需要选择K个值作为topK<br>做法：由于<code>vocab_size</code>比较大，因此分成两次topK</p>
<ul>
<li>第一次：<code>[bs, beamwidth, vocab size] =&gt; [bs, beamwidth, BlockPerBeam, K]</code><ul>
<li>将vocab分为<code>BlockPerBeam</code>段，每段做topK选出前<code>K</code>个最大的值</li>
<li>第一次topK后每个vocab还有<code>BlockPerBeam * K</code>个值</li>
<li>grid：<code>[min(batch_size * BlockPerBeam, maxBlockNums)]</code></li>
<li>block：<code>[256]</code></li>
</ul>
</li>
<li>第二次：<code>[bs, beamwidth, BlockPerBeam, K] =&gt; [bs, beamwidth, K]</code><ul>
<li>将vocab剩下的<code>BlockPerBeam * K</code>个值直接做topK得到K个值</li>
<li>grid：<code>[min(batch_size, maxBlockNums)]</code></li>
<li>block：<code>[256]</code></li>
</ul>
</li>
</ul>
<p>①topK的做法<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T, int K&gt;  
struct topK{  
    // 下面这两行的访问权限是public，因为默认就是public所以不用显式地写出来  
    T val[K];  
    int id[L];  
	// 初始化topK中id全为-1，val全为最小值
    __device__ void init(){  
        for(int i = 0; i &lt; K; i++){  
            id[i] = -1; 
            val[i] = FLT_MIN;  
        }    
    }    
    // 如果当前输入的数字比最后一个数字大，则摒弃最后一个数字，将输入的数字排进来
    void insertHeap(T data, int data_id){  
		if(id[K-1] == -1 || val[K-1] &lt; data){  
			id[K-1] = data_id;  
			val[K-1] = data;  
		}        
        // 只需要对当前输入进来的做冒泡排序，因为每进来一个都做一次冒泡排序
        for(int i = K-2; i &gt;= 0; i--){  
            if(val[i + 1] &gt; val[i]){  
                T tmp = val[i];  
                val[i] = val[i + 1];  
                val[i + 1] = tmp;  
                int tmp_id = id[i];  
                id[i] = id[i + 1];  
                id[i + 1] = tmp_id;  
            }        
        }    
    }
};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<p>②将两个topK做一次reduce输出为一个topK<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T, int K&gt;  
__device__ topK&lt;T, K&gt; reduce_functor(const topK&lt;T, K&gt;&amp; a, const topK&lt;T, K&gt;&amp; b) {  
    topK&lt;T, K&gt; res = a;  
    for(int i = 0; i &lt; K; i++){  
        res.insertHeap(b.val[i], b.id[i]);  
    }    
    return res;  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<p>③第一次topK<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T, int K, int blockSize, int BlockPerBeam&gt;  
__global__ void topK_kernel_round1(const T* probs, 
								   const int vocab_size,   
								   int* topK_ids, 
								   T* topK_vals){  
    int tid = threadIdx.x;  
    int bid = blockIdx.x;  
    int row_id = bid / BlockPerBeam;     // 哪一批vocab/哪一个batch中  
    int block_lane = bid % BlockPerBeam; // 同一批vocab中的哪一个段  
    topK&lt;T, K&gt; thread_topK; // 为每一个线程分配一个topK寄存器  
    thread_topK.init();  
    // 下面做thread层次的reduce  
    for(int data_id = tid + block_lane * blockSize; data_id &lt; vocab_size; data_id += BlockPerBeam * blockSize){  
        int data_offset = data_id + row_id * vocab_size;  
        T data = probs[data_offset];  
        thread_topK.insertHeap(data, data_offset);  
    }    
    
    typedef cub::BlockReduce&lt;topK&lt;T, K&gt;, blockSize&gt; blockreduce; 
    __shared__ typename blockreduce::TempStorage tmp_storage;    
    topK&lt;T, K&gt; block_topk = blockreduce(tmp_storage).Reduce(thread_topK, reduce_functor&lt;T, K&gt;);  
  
    if(tid == 0){  
        for(int k_offset = 0; k_offset &lt; K; k_offset++){  
            int dst_offset = row_id * BlockPerBeam * K + 
				             block_lane * K + 
				             k_offset;  
            topK_vals[dst_offset] = block_topk.val[k_offset];  
            topK_ids[dst_offset] = block_topk.id;  
        }    
    }
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>入参：<p></p>
<ul>
<li><code>probs</code>：输入的概率值<code>[bs, beamwidth, vocab size]</code></li>
<li><code>topK_ids</code>和<code>topK_vals</code>：作为输出</li>
</ul>
<p>在未需要<code>data+=BlockPerBeam*blockSize</code>时，</p>
<ul>
<li>每个batch中，<code>block_lane=0~7</code>，<code>tid=0~255</code></li>
<li>在不同batch中，<code>row_id</code>不同</li>
<li><code>data_id+=BlockPerBeam*blockSize</code>可以理解为当<code>data_id</code>是0~2047并且<code>data_id</code>仍未超出<code>vocab_size</code>时，在不变动<code>tid</code>和<code>bid</code>前提下，线程并行执行<code>data+_id</code>加上步长为<code>BlockPerBeam*blockSize</code>得到的新的<code>data_id</code>的行为。直到<code>data_id</code>超过<code>vocab_size</code>为止</li>
<li><code>data_id</code>可以理解为在某一vocab中的偏移量，加上<code>row_id</code>关于batch的偏移得到最终的偏移量<code>data_offset</code></li>
<li><code>thread_topK</code>：是每个线程都有自己的topK<ul>
<li><code>bid=0, tid=0</code>：负责<code>data_id</code>为0、2048、4096的topK</li>
<li><code>bid=7, tid=1</code>：负责<code>data_ia</code>为1793、2561、4609的topK</li>
</ul>
</li>
</ul>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">typedef cub::BlockReduce&lt;topK&lt;T, K&gt;, blockSize&gt; blockreduce; 
   __shared__ typename blockreduce::TempStorage tmp_storage;    
   topK&lt;T, K&gt; block_topk = blockreduce(tmp_storage).Reduce(thread_topK, reduce_functor&lt;T, K&gt;);  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<ul>
<li><code>cub::BlockReduce</code>是NVIDIA提供的CUB(CUDA UnBound)库中的一个模板类，目的是将线程块中的数据(由每个线程负责一部分)规约为单一结果<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template &lt;typename T, int BLOCK_DIM&gt; 
class cub::BlockReduce { 
public: 
	using TempStorage = typename ImplementationDefined; 
	BlockReduce(TempStorage&amp; temp_storage); 
	T Reduce(T input, ReduceOp reduce_op);
};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><code>tmp_storage</code>：供线程块中的线程通信和归约使用</li>
<li><code>block_topk</code>：合并每个线程块中的线程的topK，得到每个线程块的topK</li>
</ul>
<p>最后每个block只使用第一个线程做转移，将block_topk个数据转移到topK_vals和topK_ids中。<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/7745326675b006f165b0b23a6397ba1.jpg" alt=""><br>④第二次topK<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T, int beam_width, int K, int blockSize, int BlockPerBeam&gt;  
__global__ void topK_kernel_round2(const int* topK_ids, 
								   const T* topK_vals,  
								   int* final_topK_ids, 
								   T* final_topK_vals){  
    int tid = threadIdx.x;  
    int bid = blockIdx.x;  
    int row_id = bid; // 改动1：每个batch只用一个block表示，同时没有block_lane
    topK&lt;T, K&gt; thread_topK;  
    thread_topK.init();  
    // 下面做thread层次的reduce  
    for(int data_id = tid; data_id &lt; beam_width * BlockPerBeam * K; data_id += blockSize){ // 改动2：data_id的初始不用考虑该batch的第几个block，步长为blockSize
        int data_offset = data_id + bid * beam_width * BlockPerBeam * K; // 改动3：batch内的偏移确定后，data_offset在每个batch之间的偏移就是beam_width*BlockPerBeam*K 
		thread_topK.insertHeap(topK_vals[data_offset], 
							   topK_ids[data_offset]);  
    }    
    
    typedef cub::BlockReduce&lt;topK&lt;T, K&gt;, blockSize&gt; blockreduce;  
    __shared__ typename blockreduce::TempStorage tmp_storage;  
    topK&lt;T, K&gt; block_topk = blockreduce(tmp_storage).Reduce(thread_topK, reduce_functor&lt;T, K&gt;);  
  
    if(tid == 0){  
        int beam_id = (blockDim.x * blockIdx.x + tid) / BlockPerBeam/ K; // 改动4：写入时需要考虑beam_id，感觉这条公式有点奇怪？
        for(int k_offset = 0; k_offset &lt; K; k_offset++){  
            int dst_offset = bid * beam_width * K + 
				             beam_id * K + 
				             k_offset; // 改动5
            final_topK_vals[dst_offset] = block_topk.val[k_offset];  
            final_topK_ids[dst_offset] = block_topk.id[k_offset];  
        }    
    }
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/b25acc5c3bf310a0925e8b3119ca82d.jpg" alt=""><p></p>
<h1 id="Lesson18-FusedSoftmax-and-Sampling"><a href="#Lesson18-FusedSoftmax-and-Sampling" class="headerlink" title="Lesson18 FusedSoftmax and Sampling"></a>Lesson18 FusedSoftmax and Sampling</h1><p>讲解了：<br><code>src/kernels/sampling.cu</code><br><code>src/kernels/sampling.h</code><br><code>src/utils/params.h</code><br><code>tests/unittests/test_sampling.cu</code></p>
<p>在GPU上生成随机数，<strong>主机仅传给设备一个信号，是的多个随机数在device端被生成</strong>：<code>curand_kernel</code></p>
<p><code>params.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">using IntDict = std::unordered_map&lt;std::string, int&gt;;
using floatDict = std::unordered_map&lt;std::string, float&gt;;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><br>键为字符串，值为int或float<p></p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">__device__ void curand_init(unsigned long long seed, unsigned long long subsequence, unsigned long long offset, curandState_t* state)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p><code>seed</code>：时间种子。<br><code>subsequence</code>：序列号，区分不同线程块的随机数生成器，确保每个块有自己的随机数生成器。<br><code>offset</code>：在指定序列中的偏移量，用于跳过序列的前几个值以获得不同的随机数，这里表示从序列的起点开始生成随机数。<br><code>state</code>：指向<code>curandState_t</code>的指针，保存生成器的内部状态。</p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">__device__ float curand_uniform(curandState_t* state)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<p>返回在<code>0.0f</code>和<code>1.0f</code>之间均匀分布的浮动值</p>
<p><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/eeb944a0bce156627540b5e069888b6.jpg" alt="|425"><br>在上图的例子中，<code>thredhold-topk_val[0]&gt;0，thredhold-topk_val[0]-topk_val[1]&lt;0</code>，因此采样值落在<code>topk_val[1]</code>上</p>
<ul>
<li>grid：<code>[batch_size]</code></li>
<li>block：<code>[K]</code></li>
</ul>
<h1 id="Lesson19-allocator"><a href="#Lesson19-allocator" class="headerlink" title="Lesson19 allocator"></a>Lesson19 allocator</h1><p>讲解了：<br><code>src/memory/allocator/base_allocator.h</code><br><code>src/memory/allocator/cuda_allocator.h</code><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/2f3403abcd9639c30a2c174db5c3798.jpg" alt=""><br><code>base_allocator.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">class BaseAllocator // 公共的父类  
{  
public:  
    virtual ~BaseAllocator(){};
  
    template&lt;class T&gt;  
    T* Malloc(T* ptr, size_t size, bool is_host){  
        return(T*)UnifyMalloc((void*)ptr, size, is_host); 
    }    
    virtual void* UnifyMalloc(void* ptr, size_t size, bool is_host = false) = 0; 
  
    template&lt;typename T&gt;  
    void Free(T* ptr, bool is_host = false){  
        UnifyFree((void*)ptr, is_host);  
    }    
    virtual void UnifyFree(void* ptr, bool is_host = false) = 0;  
};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li>父类的析构函数要声明为虚函数：确保当使用基类指针指向派生类对象时，销毁对象时会<strong>正确调用派生类的析构函数</strong>。</li>
<li><code>(void*)ptr</code>：CPU的分配函数malloc返回的是一个void类型的，所以把传进去的指针强转为void</li>
<li>定义<code>UnifyMalloc</code>和<code>UnifyFree</code>为虚函数，在子类里一定要实现这个函数 </li>
</ul>
<p><code>cuda_allocator.h</code><br>①定义两种块<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">struct CudaBigBlock {  
    void *data;  
    size_t size;  
    bool is_allocated;  
    CudaBigBlock() = default; // 构造函数  
    CudaBigBlock(void* data_, size_t size_, bool is_allocated_): // 构造函数  
        data(data_), size(size_), is_allocated(is_allocated_){}  
};  
  
struct CudaSmallBlock {  
    void* data;  
    size_t size;  
    bool is_allocated;  
    CudaSmallBlock() = default; // 构造函数  
    CudaSmallBlock(void* data_, size_t size_, bool is_allocated_): // 构造函数  
            data(data_), size(size_), is_allocated(is_allocated_){}  
};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>大小块的定义相同<p></p>
<ul>
<li>大内存块：不易造成内存碎片</li>
<li>小内存块：碎片化较严重，构建小块的内存池主要为了收集碎片大小归还OS(有时不是内存不够，而是碎片太多可能会报out of memory的错</li>
</ul>
<p>②定义分配器<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">class CudaAllocator: public BaseAllocator {  
private:  
    //{device id: block}    // 每个设备都有内存池  
    std::map&lt;int, std::vector&lt;CudaSmallBlock&gt; &gt; cudaSmallBlockMap;  
    std::map&lt;int, std::vector&lt;CudaBigBlock&gt; &gt; cudaBigBlockMap;  
    std::map&lt;int, size_t&gt; FreeSize;  
    int dev_id;  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>定义了<p></p>
<ul>
<li>设备ID与以<code>CudaSmallBlock</code>为对象的数组的映射(每个设备都有一个大、小内存池)</li>
<li>设备ID与以<code>CudaBigBlock</code>为对象的数组的映射</li>
<li>设备ID与该设备空闲内存大小的映射</li>
</ul>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">public:  
    CudaAllocator() {  
        cudaGetDevice(&amp;dev_id);  
    }   
     ~CudaAllocator() {  
    }    <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<p>为<code>CudaAllocator</code>实现<code>UnifyMalloc</code></p>
<p>0）对齐32bytes以实现<code>float4</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void* UnifyMalloc(void* ptr, size_t size, bool is_host) { 
	size = ((size + 31) / 32 ) * 32;  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><br>1）如果是主机上申请buffer，用<code>malloc</code>申请<br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">if(is_host){  
    ptr = malloc(size); 
    memset(ptr, 0, size);
    return ptr;  
}  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li><code>memset</code>：初始化从<code>ptr</code>指向开始的size个值，初始化的数值为0<br>2）在bigblocks中找空闲的块，即被free出来但是还未归还到OS的<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">if(size &gt; 1024 * 1024){
    auto BigBlocks = cudaBigBlockMap[dev_id];  
    int blockID = -1;  
    for(int i = 0; i &lt; BigBlocks.size(); i++){ 
        if(BigBlocks[i].size &gt;= size&amp;&amp;!BigBlocks[i].is_allocated &amp;&amp; BigBlocks[i].size - size &lt; 1024 * 1024){  
            if(blockID == -1 || BigBlocks[blockID].size &gt; BigBlocks[i].size){ 
                blockID = i;  
            }        
        }    
    }    
    if(blockID != -1){  
        BigBlocks[blockID].is_allocated = true;  
        return BigBlocks[blockID].data;  
    }    
    void* new_buffer;  
    cudaMalloc(&amp;new_buffer, size);  
    BigBlocks.push_back(CudaBigBlock(new_buffer, size, false));  
    return new_buffer;  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li>如果<code>size</code>大于1024k就用bigblock</li>
<li><code>if(BigBlocks[i].size &gt;= size &amp;&amp; !BigBlocks[i].is_allocated &amp;&amp; BigBlocks[i].size - size &lt; 1024 * 1024)</code> <ul>
<li><code>BigBlocks[i].size &gt;= size</code>：该内存块的大小要大于申请的内存</li>
<li><code>!BigBlocks[i].is_allocated</code>：该内存块没有被分配出去</li>
<li><code>BigBlocks[i].size - size &lt; 1024 * 1024</code>：该内存块分配之后剩余的内存不会超过1024k(碎片化？)</li>
</ul>
</li>
<li><code>if(blockID == -1 || BigBlocks[blockID].size &gt; BigBlocks[i].size)</code><ul>
<li><code>blockID == -1</code>：如果当前还没分配内存块</li>
<li>或者<code>BigBlocks[blockID].size &gt; BigBlocks[i].size</code>：已经分配给该内存的内存块比当前的内存块要大，则替换当前内存块来存储</li>
</ul>
</li>
<li>分配内存块之后，返回一个void类型的指针</li>
<li>如果未能找到合适的，直接<code>cudaMalloc</code><br>3）在smallblocks中找空闲的块，即被free出来但是还未归还到OS的<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">auto SmallBlocks = cudaSmallBlocksMap[dev_id];  
for(int i = 0; i &lt; SmallBlocks.size(); i++){  
    if(SmallBlocks[i].size &gt;= size&amp;&amp;!SmallBlocks[i].is_allocated &amp;&amp;SmallBlocks[i].size - size &lt; 1024 * 1024){  
        SmallBlocks[i].is_allocated = true;  
        FreeSize[dev_id] += SmallBlocks[i].size; // 这里去掉
        return SmallBlocks[i].data;  
    }        
}  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li>匹配策略：简单首次匹配，使用第一个符合要求的内存块而不再比较</li>
<li><code>FreeSize[dev_id] += SmallBlocks[i].size;</code>：将分配出来的内存块大小加到对应设备的<code>FreeSize</code>中，以便之后释放内存<br>4）没有找到合适内存的<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">    void* newBuffer = (void*)ptr;  
    CHECK(cudaMalloc(&amp;newBuffer, size));  
    CHECK(cudaMemset(newBuffer, 0, size)); // size是初始化的字节数  
    SmallBlocks.push_back(CudaSmallBlock(newBuffer, size, false));  
    return new_buffer;  
}  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li><code>__host__ cudaError_t cudaMemset(void* devPtr, int value, size_t count)</code><ul>
<li>Initializes or sets device memory to a value.</li>
<li>devPtr：Pointer to device memory</li>
<li>value：Value to set for each byte of specified memory</li>
<li>count： Size in bytes to set<br>0）如果指针指向主机端的内存，直接释放<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void UnifyFree(void* ptr, bool is_host) {  
	if (ptr == nullptr) {  
		return;  
	}
      if(is_host){  
           cudaFree(ptr);  
       } <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
1）当累积的小内存块超过1G时，清理未分配出去的smallblocks，已分配的保留在smallmap中<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">for(auto&amp; it : cudaSmallBlocksMap){  
    if(FreeSize[it.first]) &gt; 1024 * 1024 * 1024{  
        auto&amp; cudaBlocks = it.second;  
        std::vector&lt;CudaSmallBlock&gt; tmp;  
        for(int i = 0; i &lt; cudaBlocks.size(); ++i){  
            if(!cudaBlocks[i].is_allocated){  
                cudaSetDevice(it.first);  
                cudaFree(cudaBlocks[i].data); // 未分配，归还OS
            } else{  
                tmp.push_back(cudaBlocks[i]); // 已分配，存回map中
            }  
        }                
        cudaBlocks.clear(); 
        it.second = tmp;  
        FreeSize[it.first] = 0; 
    }        
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
</ul>
</li>
<li><code>for(auto&amp; it : cudaSmallBlocksMap)</code>：<ul>
<li><code>&amp;it</code>：对容器元素的引用，<code>&amp;</code>表示对<code>it</code>的修改会直接作用于容器中的元素而不会创建副本</li>
<li><code>it.first</code>和<code>it.second</code>：分别是设备ID和内存块向量</li>
</ul>
</li>
<li><code>__host__ cudaError_t cudaSetDevice(int device)</code>：Set device to be used for GPU executions.</li>
<li><code>cudaBlocks.clear()</code>：在更新cudaBlocks之前先清空</li>
<li><code>FreeSize[it.first] = 0</code>：对当前设备的FreeSize归零<br>3）找到待free的内存块的位置，设<code>is_allocated = false</code>，大小block都不归还到OS，除非没有在大小block里面找到待free的指针<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">        for(auto&amp; it : cudaSmallBlocksMap){  
            auto&amp; cudaBlocks = it.second;  
            for(int i = 0; i &lt; cudaBlocks.size(); i++){  
                if(cudaBlocks[i].data == ptr){  
                    cudaBlocks[i].is_allocated = false;  
                    FreeSize[it.first] += cudaBlocks[i].size;
                    return;  
                }            
            }            
            auto&amp; bigBlocks = cudaBigBlocksMap[it.first];  
            for(int i = 0; i &lt; bigBlocks.size(); i++){  
                if(bigBlocks[i].data == ptr){  
                    bigBlocks[i].is_allocated = false;  
                    return;  
                }            
            }        
        }        
	    cudaFree(ptr);  
    }
};<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ul>
<p><code>a.size</code>和<code>a.size()</code></p>
<ul>
<li>当<code>a</code>是标准容器(<code>std::vecotr</code>，<code>std::map</code>等等)时，<code>size</code>是一个成员函数，用于获取容器的大小，写法为<code>a.size()</code>，调用成员函数</li>
<li>当<code>a</code>是用户自定义的类，<code>public: size_t size;</code>时，<code>size</code>是一个成员变量，写法为<code>a.size</code>；<code>public: size(){};</code>时，<code>size</code>是成员函数，写法为<code>a.size()</code></li>
</ul>
<h1 id="Lesson-20-Context-attention"><a href="#Lesson-20-Context-attention" class="headerlink" title="Lesson 20 Context attention"></a>Lesson 20 Context attention</h1><p><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/5731d9562b034964b53c0e7d24bd858.jpg" alt=""></p>
<h2 id="20-1src-layers-attention-context-attention-cpp"><a href="#20-1src-layers-attention-context-attention-cpp" class="headerlink" title="20.1src/layers/attention/context_attention.cpp"></a>20.1<code>src/layers/attention/context_attention.cpp</code></h2><h3 id="20-1-1-构造函数"><a href="#20-1-1-构造函数" class="headerlink" title="20.1.1 构造函数"></a>20.1.1 构造函数</h3><p><code>LLaMAContextAttentionLayer&lt;T&gt;::LLaMAContextAttentionLayer</code>：构造函数<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">head_num(head_num),  
kv_head_num(kv_head_num),  
head_size(head_size),  
stream(stream),  
cublas_wrapper(cublas_wrapper),  
allocator(allocator), 
hidden_units(head_num * head_size),  
attn_static_params(attn_params),   
q_head_per_kv(head_num / kv_head_num),  
scale(float(1 / sqrt(head_size)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<h3 id="20-1-2-分配内存"><a href="#20-1-2-分配内存" class="headerlink" title="20.1.2 分配内存"></a>20.1.2 分配内存</h3><p><code>LLaMAContextAttentionLayer&lt;T&gt;::allocForForward(LLaMAAttentionDynParams&amp; params)</code>：分配forward所需要的buffer</p>
<ul>
<li><p><code>LLaMAAttentionDynParams</code>定义来源：<code>src/models/llama_llama_params.h</code></p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">struct LLaMAAttentionDynParams {  
    int batch_size;  
    int num_tokens;  
    int max_q_len;  
    int max_k_len;  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
</li>
<li><p>先定义指针</p>
<ul>
<li><code>new</code>：它从堆上分配指定类型的内存，并返回一个指向该内存块的指针。使用 <code>new</code> 分配的内存不会像栈上分配的变量那样在函数结束时自动释放，需要手动释放。</li>
<li>和<code>malloc</code>区别：<ul>
<li><code>new</code>：不仅分配内存，还会调用对象的构造函数（如果是类对象的话）</li>
<li><code>malloc</code>：只负责分配内存，不会调用构造函数</li>
</ul>
</li>
</ul>
</li>
<li>再分配内存<ul>
<li><code>allocator-&gt;Malloc</code></li>
<li>对<code>k_cache_buf</code>和<code>v_cache_buf</code>分配内存时，在<code>k_cache_buf</code>分配两倍的内存，再令<code>v_cache_buf</code>的数据指针指向<code>k_cache_buf</code>偏移<code>batch_size * head_num * max_k_len * head_size</code>的地方。这样可以减少一次内存分配<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">k_cache_buf-&gt;data = allocator-&gt;Malloc(k_cache_buf-&gt;data, 2 * sizeof(T) * batch_size * head_num * max_k_len * head_size);

v_cache_buf-&gt;data = (T*)k_cache_buf-&gt;data + batch_size * head_num * max_k_len * head_size;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
①<code>fusedQkvGemm</code><br><code>input</code></li>
</ul>
</li>
<li><code>input tensor</code><br><code>output</code></li>
<li><code>qkv_buf_wo_pad</code>: <code>[num_tokens, qkv_head_num, head_size]</code><br>作用：做linear将输入的tensor乘上qkv权重，得到qkv<br>②<code>AddbiasAndPaddingAndRope</code><br><code>output</code></li>
<li><code>q_buf_w_pad</code>: <code>[bs, head_num, max_q_len, head_size]</code></li>
<li><code>k_buf_w_pad</code>: <code>[bs, kv_head_num, max_q_len, head_size]</code></li>
<li><code>v_buf_w_pad</code>: <code>[bs, kv_head_num, max_q_len, head_size]</code><br>作用：添加偏置，进行padding使同一批次的句子长度相同，进行位置旋转编码<br>③<code>ConcatPastKVcache</code><br><code>output</code></li>
<li><code>k_cache_buf</code>: <code>[bs, head_num, max_q_len, head_size]</code></li>
<li><code>v_cache_buf</code>: <code>[bs, head_num, max_q_len, head_size]</code><br>作用：将新得到的KV存储到cache中<br>④<code>qk gemm</code><br><code>output</code></li>
<li><code>qk_buf</code>: <code>[bs, head_num, max_q_len, max_k_len]</code><br>作用：进行qk相乘，得到$QK^T$<br>⑤<code>FusedMaskAndScaleSoftmax</code><br><code>output</code></li>
<li><code>qk buf</code><br>作用：加上mask并进行scale和softmax，得到$Softmax(\dfrac{QK^T}{\sqrt{d_k}})$<br>⑥<code>qk*v gemm</code><br><code>output</code></li>
<li><code>qkv_buf_w_pad</code>: <code>[bs, head_num, max_q_len, head_size]</code><br>作用：得到$Softmax(\dfrac{QK^T}{\sqrt{d_k}})V$<br>⑦<code>RemovingPadding</code><br><code>output</code></li>
<li><code>qkv_buf_wo_pad_1</code>: <code>[num_tokens, head_num, head_size]</code><br>作用：将padding去掉</li>
</ul>
<h3 id="20-1-3-释放内存"><a href="#20-1-3-释放内存" class="headerlink" title="20.1.3 释放内存"></a>20.1.3 释放内存</h3><p><code>src/utils/macro.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">inline void syncAndCheck(const char* const file, int const line){  
    cudaDeviceSynchronize();  
    cudaError_t result = cudaGetLastError();  
    if (result) {  
        throw std::runtime_error(std::string("[TM][ERROR] CUDA runtime error: ") + (_cudaGetErrorEnum(result)) + " " + file + ":" + std::to_string(line) + " \n");  
    }
}  
  
#define DeviceSyncAndCheckCudaError() syncAndCheck(__FILE__, __LINE__)  <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li><code>syncAndCheck</code><ul>
<li><code>cudaDeviceSynchronize()</code>：确保当前所有 CUDA 操作完成</li>
<li><code>cudaGetLastError()</code>：检查CUDA运行时的最后一个错误</li>
<li>参数：<ul>
<li><code>file</code>：记录发生错误的源文件名称</li>
<li><code>line</code>：记录发生错误的行号</li>
</ul>
</li>
</ul>
</li>
<li><code>#define DeviceSyncAndCheckCudaError() syncAndCheck(__FILE__, __LINE__)</code><ul>
<li>调用<code>syncAndCheck</code>函数，并自动捕获当前的文件名和行号，在调用时不需要显式传递 <code>__FILE__</code> 和 <code>__LINE__</code></li>
</ul>
</li>
</ul>
<p>释放<code>qkv_buf_wo_pad</code>、<code>q_buf_w_pad</code>、<code>k_cache_buf</code>、<code>qk_buf</code>、<code>qkv_buf_w_pad</code>、<code>qkv_buf_wo_pad_1</code>，在每个<code>Free</code>的后面加上<code>DeviceSyncAndCheckCudaError()</code>，检查是否发生错误</p>
<h3 id="20-1-4-前向传播"><a href="#20-1-4-前向传播" class="headerlink" title="20.1.4 前向传播"></a>20.1.4 前向传播</h3><p><code>src/utils/tensor.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">struct TensorMap{  
    std::unordered_map&lt;std::string, Tensor*&gt; tensor_map_;  
  
    TensorMap() = default;  
    
    TensorMap(std::initializer_list&lt;std::pair&lt;std::string, Tensor*&gt;&gt; tensor_map){  
        for (auto&amp; pair : tensor_map){  
            if (isValid(pair.second)){  
                tensor_map_.insert(pair.first, pair.second); 
            }  
            else{  
                LLM_CHECK_WITH_INFO(isValid(pair.second),fmtstr("%s is not a valid tensor, skipping insert into TensorMap", pair.first.c_str()));  
            }        
        }    
    }  
    
    TensorMap(const std::unordered_map&lt;std::string, Tensor*&gt;&amp; tensor_map) {  
		for(auto it = tensor_map.begin(); it != tensor_map.end(); it++){
            if (isValid(it-&gt;second)) {  
                tensor_map_.insert(it-&gt;first, it-&gt;second);  
            }            
            else {  
                // TODO: add a reminder info  
            }  
        }    
    };
    // ...
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li><code>TensorMap(std::initializer_list&lt;std::pair&lt;std::string, Tensor*&gt;&gt; tensor_map)</code><ul>
<li>接受一个 <code>std::initializer_list</code> 类型的参数，其元素是键值对 <code>std::pair&lt;std::string, Tensor*&gt;</code>，适用于初始化容器</li>
<li>例子👇<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">Tensor* tensor1 = new Tensor(); 
Tensor* tensor2 = nullptr; // 无效指针
TensorMap tmap = {{"key1", tensor1}, 
				 {"key2", tensor2}} // 无效，会被跳过<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li>
</ul>
</li>
<li><code>TensorMap(const std::unordered_map&lt;std::string, Tensor*&gt;&amp; tensor_map)</code><ul>
<li>接受一个 <code>std::unordered_map&lt;std::string, Tensor*&gt;</code> 类型的参数，使用现有哈希表初始化</li>
<li>例子👇<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">std::unordered_map&lt;std::string, 
Tensor*&gt; umap = {{"key1", tensor1}, 
				{"key2", tensor2}}; 
TensorMap tmap(umap);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre>
</li>
</ul>
</li>
</ul>
<p>①入参<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
void LLaMAContextAttentionLayer&lt;T&gt;::forward
(TensorMap&amp; inputs, 
TensorMap&amp; outputs, 
LLaMAattentionWeights&lt;T&gt;&amp; weights,
LLaMAAttentionDynParams&amp; params, 
LLaMAAttentionStaticParams&amp; static_params)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p></p>
<ul>
<li><code>inputs</code>：元素大概是<code>{"attention_input",tensor1},{"padding_offset",tensor2}</code><ul>
<li>因为很多函数需要TensorWrapper，而传进去的是Tensor，对于需要Tensor强转为TensorWrapper的情况，用到👇(<code>src/utils/tensor.h</code>)<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">template&lt;typename T&gt;  
TensorWrapper&lt;T&gt;* as(){  
    return static_cast&lt;TensorWrapper&lt;T&gt;*&gt;(this); // Tensor转子类TensorWrapper的下行转换  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre></li>
</ul>
</li>
<li><code>outputs</code>：同上</li>
<li><code>weights</code>：<code>src/weights/llama/attention_weights.h</code>中，内置属性有<code>BaseWeight&lt;T&gt; qkv;  BaseWeight&lt;T&gt; output;</code></li>
<li><code>params</code>：<code>src/models/llama/llama_params.h</code>，内置属性有<code>int batch_size; int num_tokens; int max_q_len; int max_k_len;</code></li>
<li><code>static_params</code>：<code>src/models/llama/llama_params.h</code>，是关于旋转编码的属性<code>int rotary_embedding_dim; float rotary_embedding_base;  int max_position_embeddings; bool use_dynamic_ntk;</code></li>
</ul>
<p>②准备内存<br>使用20.1.2中的分配内存</p>
<p>③qkv linear<br><code>src/kernels/linear.h</code><br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchLinearGemm(attention_input-&gt;as&lt;T&gt;(), weights.qkv, qkv_buf_wo_pad, cublas_wrapper)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br>对应<code>input</code>、<code>weight</code>、<code>output</code>、<code>cublas_wrapper</code>、<code>trans_a</code>、<code>trans_b</code><br>完成<code>fusedQkvGemm</code><p></p>
<p>④qkv bias and rope and padding<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchAddFusedQKVBiasTransposeAndRoPE(qkv_buf_w_pad, 
									  k_buf_w_pad, 
									  v_buf_w_pad, 
									  qkv_buf_wo_pad,
									  weights.qkv, 
									  padding_offset-&gt;as&lt;int&gt;(), 
									  history_length-&gt;as&lt;int&gt;(), 
									  input_length-&gt;as&lt;int&gt;(),
									  static_params);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>最后在<code>k_buf_w_pad</code>和<code>v_buf_w_pad</code>得到rope和padding的版本<p></p>
<p>⑤concat past kv cache<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchConcatKVCache(k_buf_w_pad, v_buf_w_pad, 
					layer_id-&gt;as&lt;int&gt;(), 
					input_length-&gt;as&lt;int&gt;(), 
					history_length-&gt;as&lt;T&gt;(),  
                    all_k_cache-&gt;as&lt;T&gt;(), 
                    all_v_cache-&gt;as&lt;T&gt;());<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>最后在<code>all_k_cache</code>和<code>all_v_cache</code>得到kvcache<br>因为<code>layer_id</code>是在CPU上分配的<code>int layer = layer_id-&gt;getVal();</code>因此需要转为TensorWrapper<p></p>
<p>⑥repeat kv<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchRepeatKVCache(all_k_cache-&gt;as&lt;T&gt;(), 
					all_v_cache-&gt;as&lt;T&gt;(), 
					context_length-&gt;as&lt;int&gt;(),  
                    layer_id-&gt;as&lt;int&gt;(),
                    k_cache_buf, 
                    v_cache_buf);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br><code>input</code>:<br>    <code>all_k_cache</code>&amp;<code>all_v_cache</code>: <code>[num_layers, batch_size, kv_head_num, max_seq_len, head_size]</code><br><code>output</code>:<br>    <code>k_cache_buf</code>&amp;<code>v_cache_buf</code>: <code>[bs, head_num, max_k_len, head_size]</code><br>作用是将kvcache的<code>kv_head_num</code>补成<code>head_num</code><p></p>
<p>⑦qk<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchLinearStridedBatchGemm(q_buf_w_pad, k_cache_buf, qk_buf, cublas_wrapper, false, true);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br><code>input</code>:<br>    <code>q_buf_w_pad</code>: <code>[bs, head_num, max_q_len, head_size]</code><br>    <code>k_cache_buf</code>: <code>[bs, head_num, max_k_len, head_size]</code>(trans_b = true)<br><code>output</code>:<br>    <code>qk_buf</code>: <code>[bs, head_num, max_q_len, max_k_len]</code><p></p>
<p>⑧scale + mask + softmax<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchScaleMaskAndSoftmax(qk_buf, attention_mask-&gt;as&lt;T&gt;(), qk_buf, scale);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br>给<code>qk_buf</code>加scale、mask、softmax<p></p>
<p>⑨qk*v<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchLinearStridedBatchGemm(qk_buf, v_cache_buf, qkv_buf_w_pad, cublas_wrapper, false, false);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br><code>input</code>:<br>    <code>qk_buf</code>: <code>[bs, head_num, max_q_len, max_k_len]</code><br>    <code>v_cache_buf</code>: <code>[bs, head_num, max_k_len, head_size]</code><br><code>output</code>:<br>    <code>qkv_buf_w_pad</code>: <code>[bs, head_num, max_q_len, head_size]</code><p></p>
<p>⑩transpose + removepadding<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchTransposeOutRemovePadding(qkv_buf_w_pad, padding_offset-&gt;as&lt;T&gt;(), qkv_buf_wo_pad_1);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br><code>input</code>:<br>    <code>qkv_buf_w_pad</code>: <code>[bs, head_num, max_q_len, head_size]</code><br>    先transpose变成<code>[bs, max_q_len, head_num, head_size]</code><br><code>output</code>:<br>    <code>qkv_buf_wo_pad_1</code>: <code>[numtokens, hiddenunits]</code><p></p>
<p>①output linear<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchLinearGemm(qkv_buf_wo_pad_1, weights.output, attention_output, cublas_wrapper, false, true);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br>乘上输出的权重<p></p>
<p>②freebuf<br>释放所有的缓存</p>
<h2 id="20-2examples-cpp-attention-context-attn-example-cpp"><a href="#20-2examples-cpp-attention-context-attn-example-cpp" class="headerlink" title="20.2examples/cpp/attention/context_attn_example.cpp"></a>20.2examples/cpp/attention/context_attn_example.cpp</h2><p>变量：</p>
<ul>
<li>基本参数：<ul>
<li><code>head_num</code>&amp;<code>kv_head_num</code>：前者是q的，后者是k和v的</li>
<li><code>head_size</code></li>
<li><code>num_layers</code></li>
<li><code>max_seq_len</code>：kv cache最大的上下文长度</li>
<li><code>hidden_units</code>&amp;<code>q_hidden_units</code>：前者是qkv总和的，后者是q的</li>
<li>作为初始化每个kernel里大小的参数</li>
</ul>
</li>
<li>静态参数：(多数是位置编码的)<ul>
<li><code>rotary_embedding_dim</code></li>
<li><code>rotary_embedding_base</code></li>
<li><code>max_position_embeddings</code></li>
<li><code>use_dynamic_ntk</code></li>
</ul>
</li>
<li>动态参数：<ul>
<li><code>batch_size</code></li>
<li><code>num_tokens</code></li>
<li><code>max_q_len</code>&amp;<code>max_k_len</code><ul>
<li><code>max_q_len</code>：padding之前同一batch下的最长的句子长度</li>
<li><code>max_k_len</code>：同一个batch中上下文的最大值</li>
</ul>
</li>
</ul>
</li>
<li>输入输出值(从主机上获取数据，复制到设备上)<ul>
<li><code>attention_input</code>：<code>[num_tokens，q_hidden_units]</code>，是最初的输入</li>
<li><code>qkv_weights</code>：<code>[q_hidden_units, hidden_units]</code>，做<code>qkvgemm</code>时用到</li>
<li><code>mask</code>：<code>[batch_size, max_q_len, max_k_len]</code>，当前的toekn不能访问到其后面的token</li>
<li><code>qkv_bias</code>：<code>[hidden_units]</code>，qkv的偏置</li>
<li><code>all_k_cache</code>：<code>[num_layers, batch_size, kv_head_num, max_seq_len, head_size]</code></li>
<li><code>all_v_cache</code>：<code>[num_layers, batch_size, kv_head_num, max_seq_len, head_size]</code></li>
<li><code>padding_offset</code>：<code>[num_tokens]</code>，每个token都有一个”在该token之前的padding个数的数值“</li>
<li><code>history_length</code>：<code>[batch_size]</code></li>
<li><code>layer_id</code>：</li>
<li><code>ctx_len</code>：<code>[batch_size]</code>，每句话的上下文长度？</li>
<li><code>attention_output</code>：<code>[num_tokens, q_hidden_units]</code></li>
<li><code>output_weights</code>：<code>[q_hidden_units, q_hidden_units]</code></li>
</ul>
</li>
</ul>
<h1 id="Lesson21-mask-self-attention-layer"><a href="#Lesson21-mask-self-attention-layer" class="headerlink" title="Lesson21 mask self attention layer"></a>Lesson21 mask self attention layer</h1><p>说是写的GQA部分<br>区别与context decoder: </p>
<ul>
<li>自回归生成模式，因此不需要mask和padding(和remove padding)</li>
</ul>
<p>layer搭建顺序和context attention类似</p>
<h2 id="21-1src-layers-attention-masked-self-attentioon-cpp"><a href="#21-1src-layers-attention-masked-self-attentioon-cpp" class="headerlink" title="21.1src/layers/attention/masked_self_attentioon.cpp"></a>21.1src/layers/attention/masked_self_attentioon.cpp</h2><p>①分配内存<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">allocForForward(params);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br>②qkv linear<br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchLinearGemm(attention_input-&gt;as&lt;T&gt;(), weights.qkv, qkv_buf, cublas_wrapper);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><br>同时，这里不需要在后面加上<code>DeviceSyncAndCheckCudaError();</code>因为<code>cublasWrapper</code>自带了<code>CHECK_CUBLAS</code>(因此涉及到cublas的都不需要再进行检查)<br>③fused decoder self attention<br><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchDecoderMaskedMHA(qkv_buf, weights.qkv, 
					   layer_id-&gt;as&lt;int&gt;(),  
                       k_cache-&gt;as&lt;T&gt;(), 
                       v_cache-&gt;as&lt;T&gt;(), 
                       finished-&gt;as&lt;bool&gt;(),  
                       step-&gt;as&lt;int&gt;(), 
                       mha_output-&gt;as&lt;T&gt;(), 
                       static_params);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>最后一个入参是<code>LLaMAAttentionStaticParams&amp; static_param</code>，是一个含有位置编码属性的结构体<p></p>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">struct LLaMAAttentionStaticParams {  
    int   rotary_embedding_dim;  
    float rotary_embedding_base;  
    int   max_position_embeddings;  
    bool  use_dynamic_ntk; // for dyn scaling rope  
};
template&lt;typename T&gt;  
class LLaMASelfAttentionLayer {
private:
	LLaMAAttentionStaticParams attn_static_params;
public:
	LLaMAAttentionStaticParams&amp; GetAttnStaticParams(){  
    return attn_static_params;  // 这里的返回值是引用，函数的调用不会复制attn_static_params，而是直接返回它的内存地址
}
template&lt;typename T&gt;  
void LLaMASelfAttentionLayer&lt;T&gt;::forward(TensorMap&amp; inputs, TensorMap&amp; outputs, LLaMAattentionWeights&lt;T&gt;&amp; weights, LLaMAAttentionDynParams&amp; params){
	LLaMAAttentionStaticParams static_params = GetAttnStaticParams();
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>为什么可以直接使用<code>LLaMAAttentionStaticParams static_params = GetAttnStaticParams();</code>：<ul>
<li>编译器对<strong>引用</strong>指向的<code>attn_static_params</code>执行拷贝构造，生成一个新的<code>LLaMAAttentionStaticParams</code>实例<ul>
<li>局部变量<code>static_params</code>是一个<strong>值类型</strong></li>
<li><code>GetAttnStaticParams()</code>返回一个指向类中成员变量<code>attn_static_params</code>的<strong>引用</strong></li>
</ul>
</li>
<li>如果修改<code>static_params</code>，不会影响<code>attn_static_params</code></li>
</ul>
</li>
<li>如果是另一种情况<code>LLaMAAttentionStaticParams&amp; static_params = GetAttnStaticParams();</code><ul>
<li><code>static_params</code>只是<code>attn_static_params</code>的一个别名，编译器不会为<code>static_params</code>分配新的内存空间，他和<code>attn_static_params</code>共用一块内存<ul>
<li>局部变量<code>static_params</code>是一个<strong>引用类型</strong></li>
<li><code>GetAttnStaticParams()</code>返回一个指向类中成员变量<code>attn_static_params</code>的<strong>引用</strong></li>
</ul>
</li>
</ul>
</li>
<li>引用与指针的区别<ul>
<li>引用：一旦绑定到某个变量就不能再绑定到其他变量；本质上是变量的别名，不需要占用额外的内存</li>
<li>指针：可以重新指向其他变量；是一个独立的变量，需要占用内存来存储地址</li>
</ul>
</li>
</ul>
<p>④output<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchLinearGemm(mha_output, weights.output, attention_output-&gt;as&lt;T&gt;, cublas_wrapper);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p></p>
<h2 id="21-2src-examples-cpp-attention-self-attention-example-cpp"><a href="#21-2src-examples-cpp-attention-self-attention-example-cpp" class="headerlink" title="21.2src/examples/cpp/attention/self_attention_example.cpp"></a>21.2src/examples/cpp/attention/self_attention_example.cpp</h2><p>变量：</p>
<ul>
<li>基本参数：<ul>
<li><code>head_num</code>&amp;<code>kv_head_num</code>：前者是q的，后者是k和v的</li>
<li><code>head_size</code></li>
<li><code>num_layers</code></li>
<li><code>max_seq_len</code>：kv cache最大的上下文长度</li>
<li><code>hidden_units</code>&amp;<code>q_hidden_units</code>：前者是qkv总和的，后者是q的</li>
<li>作为初始化每个kernel里大小的参数</li>
</ul>
</li>
<li>静态参数：(多数是位置编码的)<ul>
<li><code>rotary_embedding_dim</code></li>
<li><code>rotary_embedding_base</code></li>
<li><code>max_position_embeddings</code></li>
<li><code>use_dynamic_ntk</code></li>
</ul>
</li>
<li>动态参数：<ul>
<li><code>batch_size</code></li>
</ul>
</li>
<li>输入输出值(从主机上获取数据，复制到设备上)<ul>
<li><code>attention_input</code>：<code>[num_tokens，q_hidden_units]</code>，是最初的输入</li>
<li><code>all_k_cache</code>：<code>[num_layers, batch_size, kv_head_num, max_seq_len, head_size]</code></li>
<li><code>all_v_cache</code>：`[num_layers, batch_size, kv_head_num, max_seq_len, </li>
<li><code>layer_id</code></li>
<li><code>finished</code>：<code>[batch_size]</code></li>
<li><code>qkv_weights</code>：<code>[q_hidden_units, hidden_units]</code>，做<code>qkvgemm</code>时用到</li>
<li><code>output_weights</code>：<code>[q_hidden_units, q_hidden_units]</code></li>
<li><code>qkv_bias</code>：<code>[hidden_units]</code>，qkv的偏置</li>
<li><code>attention_output</code>：<code>[num_tokens, q_hidden_units]</code></li>
</ul>
</li>
</ul>
<h1 id="Lesson22-FFN"><a href="#Lesson22-FFN" class="headerlink" title="Lesson22 FFN"></a>Lesson22 FFN</h1><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/peixu/p/16842247.html" title="发布于 2022-10-30 21:04">关于Transformer中feed forward layer理解</a><br><a target="_blank" rel="noopener" href="https://juejin.cn/post/7389923941492375579">Transformer 论文通俗解读：FFN 的作用</a></p>
<h2 id="22-1-src-layers-ffn-ffn-h"><a href="#22-1-src-layers-ffn-ffn-h" class="headerlink" title="22.1 src/layers/ffn/ffn.h"></a>22.1 <code>src/layers/ffn/ffn.h</code></h2><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">void allocForForward(LLaMAAttentionDynParams&amp; params);  
void allocForForward(int batch_size);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>重载函数</p>
<ul>
<li>context attention中在remove padding后数据的第一维是<code>num_tokens</code>(传入的是<code>params.num_tokens</code>)</li>
<li>self attention中数据的第一维一直是<code>batch_size</code>(<code>[batch_size, 1, ...]</code>)</li>
</ul>
<h2 id="22-2-src-layers-ffn-ffn-cpp"><a href="#22-2-src-layers-ffn-ffn-cpp" class="headerlink" title="22.2 src/layers/ffn/ffn.cpp"></a>22.2 <code>src/layers/ffn/ffn.cpp</code></h2><p><code>forward()</code><br>①确定使用哪种forward的内存分配<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">if (params.num_tokens &gt; 0) {  
    allocForForward(params);  
	} else {                  
    allocForForward(params.batch_size);  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><br>如果存在num_tokens则为context attention，对应<code>params</code>；<br>如果不存在则为self attention，对应<code>batch_size</code><p></p>
<p>②fusedGateUp projs<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchLinearGemm(ffn_input-&gt;as&lt;T&gt;(), weights.gateAndup, SwiGLU_input, cublas_wrapper, false, true);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p></p>
<ul>
<li>输入<code>ffn_input</code>：<code>[bs(/num_tokens), q_hidden_units]</code></li>
<li>权重<code>weights.gateAndup</code>：<code>[q_hidden_units, 2 * inter_size]</code></li>
<li>输出<code>SwiGLU_input</code>：<code>[bs(/num_tokens), 2 * inter_size]</code></li>
<li>经过Gate Linear和Up Linear的输入都是<code>[bs(/num_tokens), q_hidden_units]</code>，因此将他们像fusedQKVGemm一样进行fusedGateUpGemm，输出使用同一块buf</li>
<li>为啥这里trans_b=true？</li>
</ul>
<p>③swiGLU<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchAct(SwiGLU_input, down_proj_input);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p></p>
<ul>
<li>输入<code>SwiGLU_input</code>：<code>[bs(/num_tokens), 2 * inter_size]</code><ul>
<li>两个大小为<code>[bs(/num_tokens), inter_size]</code>的Gate数组和Up数组的相同偏移量的数据一起计算，因此最后的输出的第二维大小为原来的一半</li>
</ul>
</li>
<li>输出<code>down_proj_input</code>：<code>[bs(/num_tokens), inter_size]</code></li>
</ul>
<p>④down proj<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">launchLinearGemm(down_proj_input, weights.down, ffn_output-&gt;as&lt;T&gt;(), cublas_wrapper, false, true);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p></p>
<ul>
<li>输入<code>down_proj_input</code>：<code>[bs(/num_tokens), inter_size]</code></li>
<li>权重<code>weights.gateAndup</code>：<code>[q_hidden_units, inter_size]</code> trans_b=true</li>
<li>输出<code>SwiGLU_input</code>：<code>[bs(/num_tokens), q_hidden_units]</code></li>
</ul>
<h2 id="22-3examples-cpp-ffn-ffn-example-cpp"><a href="#22-3examples-cpp-ffn-ffn-example-cpp" class="headerlink" title="22.3examples/cpp/ffn/ffn_example.cpp"></a>22.3<code>examples/cpp/ffn/ffn_example.cpp</code></h2><p>变量：</p>
<ul>
<li>基本参数：<ul>
<li>`head_num</li>
<li><code>head_size</code></li>
<li><code>inter_size</code></li>
<li>`hidden_units</li>
<li>作为初始化每个kernel里大小的参数</li>
</ul>
</li>
<li>动态参数：<ul>
<li><code>num_tokens</code></li>
</ul>
</li>
<li>输入输出值(从主机上获取数据，复制到设备上)<ul>
<li><code>ffn_input</code>：<code>[hidden_units, num_tokens]</code></li>
<li><code>gate_up</code>：<code>[hidden_units, 2 * inter_size]</code></li>
<li><code>down</code>：<code>[hidden_units, inter_size]</code></li>
</ul>
</li>
<li>设置为设备参数<ul>
<li><code>ffn_output</code></li>
</ul>
</li>
</ul>
<h2 id="22-4-关于CMakeList-txt"><a href="#22-4-关于CMakeList-txt" class="headerlink" title="22.4 关于CMakeList.txt"></a>22.4 关于CMakeList.txt</h2><p><code>src/layers/ffn/CMakList.txt</code></p>
<ul>
<li>将<code>ffn.cpp</code>编译到静态库中并命名为<code>Llamaffn</code><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">add_library(Llamaffn STATIC ffn.cpp)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li>链接<code>Llamaffn</code>所用到的函数(<code>launchLinearGemm</code>,<code>launchAct</code>)对应的静态库(<code>linear</code>,<code>act</code>)<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">target_link_libraries(Llamaffn PUBLIC  
                             -lcudart  
                             -lcudadevrt  
                             act  
                             linear)<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<code>examples/cpp/ffn/CMakeList.txt</code></li>
<li>将<code>ffn_example.cpp</code>编译到可执行目标文件中并命名为<code>ffnExample</code><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">add_executable(ffnExample ffn_example.cpp)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre></li>
<li>链接<code>ffnExample</code>所用到的函数(<code>ffn.cpp</code>)对应的静态库(<code>Llamaffn</code>)</li>
</ul>
<h1 id="Lesson23-llama-layer-weight"><a href="#Lesson23-llama-layer-weight" class="headerlink" title="Lesson23 llama layer weight"></a>Lesson23 llama layer weight</h1><p>讲解了：<br><code>src/weights/llama/layer_weights.h</code><br><code>src/weights/llama/layer_weights.cc</code><br><code>src/weights/llama/CMakelists.txt</code><br><code>src/utils/weights_utils.h</code><br><code>src/utils/weights_utils.cu</code><br><code>src/utils/CMakelists.txt</code></p>
<p>有weight的地方</p>
<ul>
<li>llama weights<ul>
<li>Embedding</li>
<li>LMhead(本质上也是一个linear)</li>
</ul>
</li>
<li>layer weights<ul>
<li>特点是有很多transformer堆叠起来</li>
<li><code>LayerNormWeight&lt;T&gt; attn_norm_weight;</code><ul>
<li>RMSNorm</li>
</ul>
</li>
<li><code>LLaMAattentionWeights&lt;T&gt; self_attn_weight;</code><ul>
<li>QKVgemm</li>
<li>output linear</li>
</ul>
</li>
<li><code>LayerNormWeight&lt;T&gt; ffn_norm_weight;</code><ul>
<li>FusedAddbiasResidualAndRMSNorm</li>
</ul>
</li>
<li><code>LLaMAFFNWeights&lt;T&gt; ffn_weight;</code><ul>
<li>Gate</li>
<li>Up</li>
<li>down</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><code>src/utils/weights_utils.cu</code><br>在源文件中的模板不是一个函数，只有在实例化后才是一个函数并且可以进行链接<br>实现了<code>GPUMalloc</code>和<code>GPUFree</code>，目的：在分配内存和释放内存时进行检查</p>
<p><code>src/weights/llama/layer_weights.h</code><br>定义了四个函数</p>
<p><code>src/weights/llama/layer_weights.cc</code><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/57edc5cf50e98d36315d8564f2c5111.png" alt=""><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/39931b88ed57cae22fab8e5d4700e67.jpg" alt=""><br>①<code>attn_norm_weight.gamma</code>：context decoder和self decoder共用</p>
<ul>
<li>类型为：<code>LayerNormWeight&lt;T&gt;</code></li>
<li>成员有：<code>T* gamma</code><br>④<code>ffn_norm_weight.gamma</code>：context decoder和self decoder共用<br>同上</li>
</ul>
<p>②<code>self_attn_weight.qkv</code>：context decoder和self decoder共用</p>
<ul>
<li>类型为：<code>BaseWeight&lt;T&gt;</code></li>
<li>成员有：<ul>
<li><code>std::vector&lt;int&gt; shape;</code></li>
<li><code>T* data;</code></li>
<li><code>WeightType type;</code></li>
<li><code>T* bias;</code>不一定每个weight都有<br>③⑤⑥⑦同上<br>③<code>self_attn_weight.output</code><br>⑤<code>ffn_weight.gate</code>：context decoder和self decoder共用<br>⑥<code>ffn_weight.up</code>：context decoder和self decoder共用<br>⑦<code>ffn_weight.down</code>：context decoder和self decoder共用</li>
</ul>
</li>
</ul>
<p>在<code>loadWeights</code>这一步中，可以加入假的数据，省去加载模型这一步，主要用于测试性能，不关注精度</p>
<ul>
<li>流程：<code>cudaMalloc</code>各种d_weights变量 -&gt; <code>malloc</code>各种h_weights变量 -&gt; h_weights载入假数据 -&gt; 通过<code>cudaMemcpy</code>将h_weights复制到d_weights -&gt; 再将d_weights赋值给</li>
</ul>
<p><code>freeWights(BaseWeight&lt;T&gt;&amp; weights)</code>：将<code>bias</code>也给释放<br>最后析构函数中释放所有的缓存</p>
<h1 id="Lesson24-AddBiasResidual"><a href="#Lesson24-AddBiasResidual" class="headerlink" title="Lesson24 AddBiasResidual"></a>Lesson24 AddBiasResidual</h1><p>讲解了：<br><code>src/kernels/add_residual.h</code><br><code>src/kernels/add_residual.cu</code><br><code>tests/unittest/test_residual.cu</code></p>
<p><code>residual</code>来源<code>FusedAddbiasResidualAndRMSNorm</code>中<code>FusedAddbiasResidual</code>的输出(同时是<code>RMSNorm</code>的输入)<br><code>decoder_out</code>来源<code>Down Linear</code></p>
<p><code>decoder_out</code> += <code>residual</code></p>
<ul>
<li><code>decoder_out</code>：<code>[num_tokens, hidden_units]</code></li>
<li><code>redisual</code>：<code>[num_tokens, hidden_units]</code>←在context decoder中，在self decoder中第一维是batch_size</li>
<li>作用：代表了初步融合后的特征，是一种包含原始信息与新特征的信息流，随后会传递到归一化操作（如 RMSNorm）中，以便为下游模块提供稳定的分布。</li>
</ul>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">Vec_t* dout = reinterpret_cast&lt;Vec_t*&gt;(decoder_out + batch_id * hidden_units);
Vec_t* rsd = reinterpret_cast&lt;Vec_t*&gt;(residual + batch_id * hidden_units);<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre>
<p>将<code>decoder_out</code>和<code>residual</code>转化为向量化类型，并且每个<code>dout</code>/<code>rsd</code>表示每一个token或batch每一行的数据都能被转换为Vec_t类型的指针</p>
<p>一般实现(fp32)和特化实现(专给fp16使用)的区别<br></p><pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">for(int i = tid; i &lt; hidden_units/vec_size; i+=blockDim.x){
    dout[i].x += rsd[i].x; // dout既是输入也是输出  
    dout[i].y += rsd[i].y;  
    dout[i].z += rsd[i].z;  
    dout[i].w += rsd[i].w;  
}

for(int i = tid; i &lt; hidden_units/vec_size; i+=blockDim.x){
    dout[i] = __hadd2(dout[i], rsd[i]); // 两个half2做加法  
}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><br><code>for</code>循环保证一个block的线程能够遍历完<code>hidden_units</code>的元素<p></p>
<p><code>tests/uinttests/test_residual.cu</code><br>在<code>CPUresidual</code>中，可以通过AVX-512 kernel + openMP进行性能改善</p>
<ul>
<li><code>AVX-512</code>(Advanced Vector Extensions 512)<ul>
<li>SIMD</li>
<li>支持512位宽的寄存器和矢量操作<ul>
<li>每次处理可以加载16个浮点数</li>
</ul>
</li>
</ul>
</li>
<li><code>OpenMP</code>(Open Multi-Processing)<ul>
<li>多线程并行编程接口，用于在共享内存环境中通过任务划分和线程控制实现并行加速</li>
</ul>
</li>
</ul>
<h1 id="Lesson25-Context-Decoder"><a href="#Lesson25-Context-Decoder" class="headerlink" title="Lesson25 Context Decoder"></a>Lesson25 Context Decoder</h1><p><code>src/layers/decoder</code></p>
<p><code>Input embedding</code> -&gt; <code>RMSNorm</code> -&gt; <code>Context Attention</code> -&gt; <code>FusedAddbiasResidualAndRMSNorm</code>(凡是残差加，残差来自上一个<code>RMSNorm</code>的输入) -&gt; <code>FFN</code> -&gt; <code>AddbiasResidual</code></p>
<p>四个中间buffer：</p>
<ul>
<li><code>decoder_residual</code>：(不同时)存储(两个地方的)残差</li>
<li><code>attention_mask</code>：保存生成的<code>CausalMask</code></li>
<li><code>padding_offset</code></li>
<li><code>cum_seqlens</code>：累积句子长度，和<code>padding_offset</code>的生命周期一样</li>
</ul>
<p><code>src/layers/decoder/context_decoder.cpp</code><br><code>LlamaContextDecoder&lt;T&gt;::forward</code></p>
<ul>
<li>入参：<ul>
<li><code>TensorMap&amp; input_tensors</code></li>
<li>`const std::vector<llamalayerweight<t>*&gt;&amp; layerWeights</llamalayerweight<t></li>
<li><code>TensorMap&amp; output_tensors</code></li>
<li><code>LLaMAAttentionDynParams&amp; dyn_params</code><br>①内存分配：导入动态变量并分配四个中间buffer的内存<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">allocForForward(dyn_params);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
②获得偏移<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">Tensor* seq_lens = input_tensors["input_length"];  
launchCalPaddingoffset(padding_offset, cum_seqlens, seq_lens-&gt;as&lt;int&gt;());<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li>
</ul>
</li>
<li>入参：<ul>
<li><code>TensorWrapper&lt;int&gt;* padding_offset</code></li>
<li><code>TensorWrapper&lt;int&gt;* cum_seqlens</code>，累积的句子长度，是所有batch的累积</li>
<li><code>TensorWrapper&lt;int&gt;* input_lengths</code>，每个句子的输入长度<br>③获取掩码长度<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">Tensor* context_length = input_tensors["context_length"];  
launchBuildCausalMasks(attention_mask, seq_lens-&gt;as&lt;int&gt;(), context_length-&gt;as&lt;int&gt;());<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li>
</ul>
</li>
<li>入参：<ul>
<li><code>TensorWrapper&lt;T&gt;* mask</code></li>
<li><code>TensorWrapper&lt;int&gt;* q_lens</code>，每个句子的输入长度</li>
<li><code>TensorWrapper&lt;int&gt;* k_lens</code>，每个句子的上下文长度<br>④搭建32层context decoder layer并进行context attention<br>为啥：搜“疑问”<br>1)从函数输入的<code>input_tensors</code>和<code>output_tensors</code>获得相应键值对并取地址，检查是否为空<br>2)初始化<code>ctx_attn_inputs</code>和<code>ctx_attn_outpus</code>键值对，这里指的是每一层的输入和输出<br>3)进行32层Layer的context attention</li>
</ul>
</li>
<li>从for循环的变量获取<code>layer_id</code>并更新到<code>ctx_attn_inputs[layer_id]</code>中</li>
<li>获取context attntion的输入</li>
<li>进行context attention，输出为<code>ctx_attn_outputs</code></li>
<li>进行FusedAddBiasResidualRMSNorm，输出为<code>decoder_output</code>(ctx_attn_outputs[“attention_output”]的指针)</li>
<li>进行ffn，输入为<code>decoder_output</code>，输出直接复用输入的内存区</li>
<li>进行AddResidual，该kernel在实现时，残差加的结果放在decoder_out上</li>
<li>把当前Layer的输出作为下一层Layer的输入，直接用当前Layer的输出<code>decoder_output</code>更新到key为”attention_input”的value中</li>
</ul>
<h1 id="Lesson26-Self-Decoder"><a href="#Lesson26-Self-Decoder" class="headerlink" title="Lesson26 Self Decoder"></a>Lesson26 Self Decoder</h1><p>待解决：llama2是不包含第一个addbias的，所以只剩下RoPE，因为旋转编码的旋转大小是64，所以不能融合到Fused Masked self Attetion里，如果是2就可以</p>
<ul>
<li>RoPE没有出现！<ul>
<li>有的，是在Fused Masked self Attention里</li>
</ul>
</li>
</ul>
<p>除了一些不需要的变量，步骤方法和context decoder的搭建差不多</p>
<h1 id="Lesson27-llama-weights"><a href="#Lesson27-llama-weights" class="headerlink" title="Lesson27 llama weights"></a>Lesson27 llama weights</h1><p><code>tools/convert_downloaded_llama_weights.py</code><br><code>tools/weights_conver.py</code></p>
<ul>
<li>在python中<ul>
<li>下载模型</li>
<li>转换<ul>
<li>将原本的.pth形式的权重，对应输出到每一层的每一个类型的(qkvgemm, gate, down, up等等的)权重</li>
<li>合并qkv的权重，合并gate和up的权重，把所有权重文件转为.bin格式的，即转为二进制</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><code>src/weights/llama/llama_weights.cc</code><br><code>src/weights/llama/llama_weights.h</code></p>
<ul>
<li>读取从python文件中得到的权重，并赋值给已分配好显存的指针</li>
<li>四个public成员(llama weights)<ul>
<li><code>llama_layer_weight</code>，有<code>num_layer</code>层</li>
<li><code>out_rmsnorm_weight</code></li>
<li><code>post_decoder_embedding_weight</code> (sampling的LMhead)</li>
<li><code>pre_decoder_embedding_weight</code></li>
</ul>
</li>
</ul>
<p><code>src/utils/weights_utils.cc</code><br><code>src/utils/weights_utils.h</code></p>
<ul>
<li>当pyhton转换完的权重格式与目标格式不一致时，如half和float，则进行转换再加载二进制文件</li>
</ul>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">llama_layer_weight.reserve(num_layer);<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>
<ul>
<li>vector.push_back和vector.reserve的区别<ul>
<li>push_back，2-&gt;4-&gt;6-&gt;8-&gt;16-&gt;32-&gt;…，当向量中有2个元素，push_back到3个元素时，vector地址自动重新分配并且容量变为4，后续的增加同理</li>
<li>reserve，指定容量，不会自动重新分配</li>
</ul>
</li>
</ul>
<h1 id="Lesson28-llama类"><a href="#Lesson28-llama类" class="headerlink" title="Lesson28 llama类"></a>Lesson28 llama类</h1><p>更高层次的抽象</p>
<p><code>std::function&lt;返回值类型(参数列表)&gt;</code>定义了一个可调用对象的签名</p>
<ul>
<li>可调用对象的签名包括返回值类型和参数列表<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/Pasted%20image%2020250115113416.png" alt=""></li>
</ul>
<p><code>src/models/llama/llama.cpp</code><br><code>src/models/llama/llama.h</code></p>
<p><code>std::string Llama&lt;T&gt;::Response(const std::vector&lt;std::string&gt; &amp;input, CallBack PrintRes)</code></p>
<ul>
<li>入参<ul>
<li><code>input</code>：用户输入的类型为字符串向量的句子</li>
<li><code>PrintRes</code>：打印结果</li>
</ul>
</li>
<li><code>Encode</code><ul>
<li><code>input</code>、<code>history_str</code>、<code>total_str</code></li>
<li>上面这三个和<code>MakeInput</code>函数有关</li>
<li><code>MakeInput</code>中的<code>ret</code>就是<code>Response</code>中的<code>input</code><ul>
<li><code>total_str</code>是所有轮次的input，包括现在和之前的</li>
<li><code>history_str</code>是之前轮次的input</li>
<li><code>input</code>是现在轮次的input</li>
</ul>
</li>
<li>得到三者的token indexs</li>
<li>这三者的长度是<code>int_params_first_token</code>字典中的值</li>
</ul>
</li>
<li><code>attn_dyn_params</code> llama类模型里动态改变的变量<ul>
<li><code>batch_size</code>：硬写为1</li>
<li><code>num_tokens</code>：当前输入的长度</li>
<li><code>max_q_len</code>：batch中q的最大长度，因为一个batch只有一个句子，所以等于num_tokens</li>
<li><code>max_k_len</code>：动态最大上下文，<code>step</code>的值与其相同(在self decoder中用到)</li>
</ul>
</li>
<li>获得所有轮次的token string<ul>
<li>自定义<code>self_token_limit</code></li>
<li><code>firstTokenGen</code><ul>
<li>入参：<code>attn_dyn_params</code>、<code>int_params_first_token</code></li>
<li><code>InitializeForContextDecoder</code><ul>
<li>传入所有轮次、之前轮次和现在轮次到CPU中，再复制到GPU中</li>
</ul>
</li>
<li><code>inputEmbedding</code><ul>
<li>得到输入的句子对应的token在embed table里的词向量</li>
</ul>
</li>
<li>包装<code>decoder_inputs</code>和<code>decoder_outputs</code>这两个TensorMap</li>
<li>进行推理</li>
<li>进行RMSNorm</li>
<li>进行LMHead和topkSample<ul>
<li>LMHead<ul>
<li>如果是context decoder<ul>
<li>取输出里的最后一个token作为LMHead的输入，经过矩阵相乘得到<code>probs</code>维度为：<code>[1, vocab_size]</code>，就知道这几个vocab的可能性</li>
</ul>
</li>
<li>如果是self decoder<ul>
<li><code>decoder_output</code>就是唯一的token，直接作为LMHead的输入</li>
</ul>
</li>
</ul>
</li>
<li>topkSample<br>  * </li>
</ul>
</li>
</ul>
</li>
<li><code>continueTokenGen</code></li>
</ul>
</li>
</ul>
<h1 id="Lesson29"><a href="#Lesson29" class="headerlink" title="Lesson29"></a>Lesson29</h1><p>提供接受用户输入或者promt的接口<br>实现大封装的API，创建C++类</p>
<p><code>std::unique_ptr</code></p>
<ul>
<li>独占所有权，不能被共享</li>
<li>自动释放内存</li>
<li>不可以被复制，所有权可以转让，转让后原来的变为空指针</li>
</ul>
<p><code>LLMengine-learn/user_entry.cpp</code></p>
<ul>
<li>传入模型和tokenizer的地址，直接调用Response</li>
</ul>
<h1 id="debug思路"><a href="#debug思路" class="headerlink" title="debug思路"></a>debug思路</h1><ol>
<li>打印/保存中间数据：将输出存为一个.bin文件，再使用<code>std::ifstream</code>读取，可以逐一比较(与huggingface的)结果</li>
</ol>
<ul>
<li>fp32两个结果的误差大于$10^{-6}$就说明有误差</li>
</ul>
<ol>
<li><code>DeviceSynAndCheckCudaError</code></li>
</ol>
<ul>
<li>检查有没有运行时错误</li>
<li>有<code>cudaDeviceSynchronize()</code>：CPU等待GPU上所有任务完成，因此最好在确保项目没有问题时关掉</li>
</ul>
<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">// CMakeList.txt中
option(PERF
	  "measure the model inference performance"
	  OFF)
if(PERF)
	add_compile_option(-DPRINT_DATA)
endif()
// context_attention.cpp中
#ifndef PERF
	DeviceSynAndCheckCudaError();
#else
#endif<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>
<ul>
<li>CMake选项定义：<code>option(选项名称 “选项描述” 默认值)</code></li>
<li>如果<code>PERF</code>是ON，那么在编译选项中添加<code>-DPRINT_DATA</code>，即在编译时添加一个宏<code>PRINT_DATA</code></li>
<li><code>cmake .. -DPERF=ON</code>时打开不进行Check(即不打开“检查运行时错误“)</li>
</ul>
<ol>
<li>PRINT_DATA</li>
</ol>
<ul>
<li>通常在首先输入的kernel里需要，检查放在lanuch里</li>
</ul>
<h1 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h1><ol>
<li>emm配完annaconda之后内存快炸了，查看任务管理器发现是vmmem的问题，是虚拟机资源分配的问题，这个问题上网查解决方法，不复杂</li>
</ol>
<ul>
<li><code>win+R</code>，输入<code>%UserProfile%</code></li>
<li>如果没有<code>.wslconfig</code>结尾的文件，可以新建一个，可以叫<code>Vmmem.wslconfig</code></li>
<li>添加以下内容<pre class="line-numbers language-none"><code class="language-none">#.wslconfig
[wsl2]
memory=3GB //分配给WSL内存，可以是内存的1/3或1/4
swap=0     //设置交换分区
localhostForwarding=true<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre></li>
<li>重启WSL：<code>win+R</code>，输入<code>services.msc</code>，找到<code>LxssManager</code>，重新启动</li>
</ul>
<ol>
<li>这个vmmem好像是个硬骨头啊！<br><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/166102340">wsl导致vmmem占用高解决办法 - 知乎</a><br>(👆文中借鉴<a target="_blank" rel="noopener" href="https://github.com/microsoft/WSL/issues/4166">WSL 2 consumes massive amounts of RAM and doesn’t return it - github.com</a>)<br>按照这个方法做到最后一步发现👇<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/Pasted%20image%2020241011223651.png" alt="|500"><br>然后就找到了这个方法👇<br><a target="_blank" rel="noopener" href="https://bbs.csdn.net/topics/396240104">drop_cashes无法操作 no such file or directory-CSDN社区</a></li>
</ol>
<ul>
<li>先<code>sudo su</code>进入root</li>
<li>输入<code>echo 3 &gt; /proc/sys/vm/drop_caches</code></li>
<li>然后再回去<code>sudo stat -c '%y' /root/drop_caches_last_run</code>就能看到清除缓存的记录了</li>
</ul>
<ol>
<li>vscode疯狂爆红，转clion去了</li>
</ol>
<h2 id="软件抽象资源和硬件资源的对应关系"><a href="#软件抽象资源和硬件资源的对应关系" class="headerlink" title="软件抽象资源和硬件资源的对应关系"></a>软件抽象资源和硬件资源的对应关系</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_41554005/article/details/119765334">【GPU结构与CUDA系列4】GPU存储资源：寄存器，本地内存，共享内存，缓存，显存等存储器细节_gpu内寄存器 - CSDN</a></p>
<h2 id="如何高效访问gpu全局内存"><a href="#如何高效访问gpu全局内存" class="headerlink" title="如何高效访问gpu全局内存"></a>如何高效访问gpu全局内存</h2><p>(解答<em>为什么v是按行连续分布和为什么要那样计算qkvgemm</em>)</p>
<ul>
<li><p>越靠近CPU排序：寄存器Register &gt; 缓存Cache &gt; 内存Memory &gt; 硬盘</p>
</li>
<li><p>“C和CUDA中的多维数组元素是根据<strong>行优先</strong>约定放置在线性寻址的内存空间中的”</p>
<ul>
<li>非按行排列：<strong>cublas API</strong>接受的输入以及输出的内存排布全部都默认为<strong>列主序</strong></li>
</ul>
</li>
<li>“当线程访问矩阵数据时，如果<strong>数据排列的顺序与线程访问顺序匹配</strong>，内存带宽的利用率会更高”。这里应该和warp有关<ul>
<li>Fused SelfDecoder Attention kernel中，<code>block</code>的大小是<code>head_size</code>，<code>grid</code>的大小是<code>head_num*batch_size</code><br>👇根据线程访问顺序匹配(从blockIdx到threadIdx，threadIdx的跨度是顺序的)<pre class="line-numbers language-c++" data-language="c++"><code class="language-c++">cache_offset = blockIdx.y * kv_head_num * max_seq_len * head_size +
			blockIdx.x/(head_num/kv_head_num)*max_seq_len*head_size+
			threadIdx.x * vec_size;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre>
<img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/c22b854c31b08ae192c8e310cb4a8fa%201.jpg" alt=""><br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/8ea8ba5aa4aa7e6763e069de3b29f14.jpg" alt=""></li>
</ul>
</li>
<li>一次数据传输的数据量默认情况下是32个字节</li>
<li>$合并度=\dfrac{线程束请求的字节数}{由该请求导致的所有数据传输处理的字节数}$</li>
<li>数据传输对数据地址的要求：在一次数据传输中，从全局内存转移到L2缓存的一片内存首地址一定是一个最小颗粒度(该例子是32)的整数倍。<ul>
<li>一次传输只取0~31、32~63、64~95……<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/808198b24f332fb853b67f74add48cd.jpg" alt=""><br>👇左行右列导致跨越式的非合并访问<br><img src="https://raw.githubusercontent.com/Wabbybabb0/Blog_picture/refs/heads/main/LLM2%E6%8E%A8%E7%90%86%E5%BC%95%E6%93%8E/7b1158679c6058fdd34cdd461ad7d17.jpg" alt=""></li>
</ul>
</li>
</ul>

                
            </div>
            <hr/>

            

    <div class="reprint" id="reprint-statement">
        
            <div class="reprint__author">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-user">
                        文章作者:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="/about" rel="external nofollow noreferrer">WB</a>
                </span>
            </div>
            <div class="reprint__type">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-link">
                        文章链接:
                    </i>
                </span>
                <span class="reprint-info">
                    <a href="https://wabbybabb0.github.io/2024/10/08/llm2-tui-li-yin-qing/">https://wabbybabb0.github.io/2024/10/08/llm2-tui-li-yin-qing/</a>
                </span>
            </div>
            <div class="reprint__notice">
                <span class="reprint-meta" style="font-weight: bold;">
                    <i class="fas fa-copyright">
                        版权声明:
                    </i>
                </span>
                <span class="reprint-info">
                    本博客所有文章除特別声明外，均采用
                    <a href="https://creativecommons.org/licenses/by/4.0/deed.zh" rel="external nofollow noreferrer" target="_blank">CC BY 4.0</a>
                    许可协议。转载请注明来源
                    <a href="/about" target="_blank">WB</a>
                    !
                </span>
            </div>
        
    </div>

    <script async defer>
      document.addEventListener("copy", function (e) {
        let toastHTML = '<span>复制成功，请遵循本文的转载规则</span><button class="btn-flat toast-action" onclick="navToReprintStatement()" style="font-size: smaller">查看</a>';
        M.toast({html: toastHTML})
      });

      function navToReprintStatement() {
        $("html, body").animate({scrollTop: $("#reprint-statement").offset().top - 80}, 800);
      }
    </script>



            <div class="tag_share" style="display: block;">
                <div class="post-meta__tag-list" style="display: inline-block;">
                    
                        <div class="article-tag">
                            
                                <a href="/tags/LLM/">
                                    <span class="chip bg-color">LLM</span>
                                </a>
                            
                                <a href="/tags/CUDA/">
                                    <span class="chip bg-color">CUDA</span>
                                </a>
                            
                        </div>
                    
                </div>
                <div class="post_share" style="zoom: 80%; width: fit-content; display: inline-block; float: right; margin: -0.15rem 0;">
                    <link rel="stylesheet" type="text/css" href="/libs/share/css/share.min.css">
<div id="article-share">

    
    <div class="social-share" data-sites="twitter,facebook,google,qq,qzone,wechat,weibo,douban,linkedin" data-wechat-qrcode-helper="<p>微信扫一扫即可分享！</p>"></div>
    <script src="/libs/share/js/social-share.min.js"></script>
    

    

</div>

                </div>
            </div>
            
                <style>
    #reward {
        margin: 40px 0;
        text-align: center;
    }

    #reward .reward-link {
        font-size: 1.4rem;
        line-height: 38px;
    }

    #reward .btn-floating:hover {
        box-shadow: 0 6px 12px rgba(0, 0, 0, 0.2), 0 5px 15px rgba(0, 0, 0, 0.2);
    }

    #rewardModal {
        width: 320px;
        height: 350px;
    }

    #rewardModal .reward-title {
        margin: 15px auto;
        padding-bottom: 5px;
    }

    #rewardModal .modal-content {
        padding: 10px;
    }

    #rewardModal .close {
        position: absolute;
        right: 15px;
        top: 15px;
        color: rgba(0, 0, 0, 0.5);
        font-size: 1.3rem;
        line-height: 20px;
        cursor: pointer;
    }

    #rewardModal .close:hover {
        color: #ef5350;
        transform: scale(1.3);
        -moz-transform:scale(1.3);
        -webkit-transform:scale(1.3);
        -o-transform:scale(1.3);
    }

    #rewardModal .reward-tabs {
        margin: 0 auto;
        width: 210px;
    }

    .reward-tabs .tabs {
        height: 38px;
        margin: 10px auto;
        padding-left: 0;
    }

    .reward-content ul {
        padding-left: 0 !important;
    }

    .reward-tabs .tabs .tab {
        height: 38px;
        line-height: 38px;
    }

    .reward-tabs .tab a {
        color: #fff;
        background-color: #ccc;
    }

    .reward-tabs .tab a:hover {
        background-color: #ccc;
        color: #fff;
    }

    .reward-tabs .wechat-tab .active {
        color: #fff !important;
        background-color: #22AB38 !important;
    }

    .reward-tabs .alipay-tab .active {
        color: #fff !important;
        background-color: #019FE8 !important;
    }

    .reward-tabs .reward-img {
        width: 210px;
        height: 210px;
    }
</style>

<div id="reward">
    <a href="#rewardModal" class="reward-link modal-trigger btn-floating btn-medium waves-effect waves-light red">赏</a>

    <!-- Modal Structure -->
    <div id="rewardModal" class="modal">
        <div class="modal-content">
            <a class="close modal-close"><i class="fas fa-times"></i></a>
            <h4 class="reward-title">给WB来包辣条</h4>
            <div class="reward-content">
                <div class="reward-tabs">
                    <ul class="tabs row">
                        <li class="tab col s6 alipay-tab waves-effect waves-light"><a href="#alipay">支付宝</a></li>
                        <li class="tab col s6 wechat-tab waves-effect waves-light"><a href="#wechat">微 信</a></li>
                    </ul>
                    <div id="alipay">
                        <img src="/medias/reward/alipay.jpg" class="reward-img" alt="支付宝打赏二维码">
                    </div>
                    <div id="wechat">
                        <img src="/medias/reward/wechat.png" class="reward-img" alt="微信打赏二维码">
                    </div>
                </div>
            </div>
        </div>
    </div>
</div>

<script>
    $(function () {
        $('.tabs').tabs();
    });
</script>

            
        </div>
    </div>

    
        <div class="card" data-aos="fade-up">
    <div id="utteranc-container" class="card-content">
        <script src="https://utteranc.es/client.js"
                repo="Wabbybabb0/commit-utterance"
                issue-term="pathname"
                theme="github-light"
                crossorigin="anonymous"
                async>
        </script>
    </div>
</div>
    

    

    

    

    

    

    

    

    

<article id="prenext-posts" class="prev-next articles">
    <div class="row article-row">
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge left-badge text-color">
                <i class="fas fa-chevron-left"></i>&nbsp;上一篇</div>
            <div class="card">
                <a href="/2025/03/19/mla/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/22.jpg" class="responsive-img" alt="Multi-head Latent Attention模型理解">
                        
                        <span class="card-title">Multi-head Latent Attention模型理解</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            作为FlashMLA的核心之一，MLA设计了在保持性能的同时着重减少KV Cache的attention机制，十分值得细究！
                        
                    </div>
                    <div class="publish-info">
                        <span class="publish-date">
                            <i class="far fa-clock fa-fw icon-date"></i>2025-03-19
                        </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/" class="post-category">
                                    高性能计算
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/GPU/">
                        <span class="chip bg-color">GPU</span>
                    </a>
                    
                    <a href="/tags/DeepSeek/">
                        <span class="chip bg-color">DeepSeek</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
        
        <div class="article col s12 m6" data-aos="fade-up">
            <div class="article-badge right-badge text-color">
                下一篇&nbsp;<i class="fas fa-chevron-right"></i>
            </div>
            <div class="card">
                <a href="/2024/08/02/cudac/">
                    <div class="card-image">
                        
                        
                        <img src="/medias/featureimages/10.png" class="responsive-img" alt="CUDA编程：基础与实践学习笔记">
                        
                        <span class="card-title">CUDA编程：基础与实践学习笔记</span>
                    </div>
                </a>
                <div class="card-content article-content">
                    <div class="summary block-with-text">
                        
                            樊哲勇老师的CUDA编程教程包括CUDA编程的语法知识、优化策略及程序开发实践，对新手很友好！
                        
                    </div>
                    <div class="publish-info">
                            <span class="publish-date">
                                <i class="far fa-clock fa-fw icon-date"></i>2024-08-02
                            </span>
                        <span class="publish-author">
                            
                            <i class="fas fa-bookmark fa-fw icon-category"></i>
                            
                            <a href="/categories/%E9%AB%98%E6%80%A7%E8%83%BD%E8%AE%A1%E7%AE%97/" class="post-category">
                                    高性能计算
                                </a>
                            
                            
                        </span>
                    </div>
                </div>
                
                <div class="card-action article-tags">
                    
                    <a href="/tags/CUDA/">
                        <span class="chip bg-color">CUDA</span>
                    </a>
                    
                    <a href="/tags/GPU/">
                        <span class="chip bg-color">GPU</span>
                    </a>
                    
                </div>
                
            </div>
        </div>
        
    </div>
</article>

</div>


<script>
    $('#articleContent').on('copy', function (e) {
        // IE8 or earlier browser is 'undefined'
        if (typeof window.getSelection === 'undefined') return;

        var selection = window.getSelection();
        // if the selection is short let's not annoy our users.
        if (('' + selection).length < Number.parseInt('120')) {
            return;
        }

        // create a div outside of the visible area and fill it with the selected text.
        var bodyElement = document.getElementsByTagName('body')[0];
        var newdiv = document.createElement('div');
        newdiv.style.position = 'absolute';
        newdiv.style.left = '-99999px';
        bodyElement.appendChild(newdiv);
        newdiv.appendChild(selection.getRangeAt(0).cloneContents());

        // we need a <pre> tag workaround.
        // otherwise the text inside "pre" loses all the line breaks!
        if (selection.getRangeAt(0).commonAncestorContainer.nodeName === 'PRE') {
            newdiv.innerHTML = "<pre>" + newdiv.innerHTML + "</pre>";
        }

        var url = document.location.href;
        newdiv.innerHTML += '<br />'
            + '来源: Wabbybabbo的摸鱼圣地<br />'
            + '文章作者: Wabbybabbo<br />'
            + '文章链接: <a href="' + url + '">' + url + '</a><br />'
            + '本文章著作权归作者所有，任何形式的转载都请注明出处。';

        selection.selectAllChildren(newdiv);
        window.setTimeout(function () {bodyElement.removeChild(newdiv);}, 200);
    });
</script>


<!-- 代码块功能依赖 -->
<script type="text/javascript" src="/libs/codeBlock/codeBlockFuction.js"></script>

<!-- 代码语言 -->

<script type="text/javascript" src="/libs/codeBlock/codeLang.js"></script>


<!-- 代码块复制 -->

<script type="text/javascript" src="/libs/codeBlock/codeCopy.js"></script>


<!-- 代码块收缩 -->

<script type="text/javascript" src="/libs/codeBlock/codeShrink.js"></script>


    </div>
    <div id="toc-aside" class="expanded col l3 hide-on-med-and-down">
        <div class="toc-widget card" style="background-color: white;">
            <div class="toc-title"><i class="far fa-list-alt"></i>&nbsp;&nbsp;目录</div>
            <div id="toc-content"></div>
        </div>
    </div>
</div>

<!-- TOC 悬浮按钮. -->

<div id="floating-toc-btn" class="hide-on-med-and-down">
    <a class="btn-floating btn-large bg-color">
        <i class="fas fa-list-ul"></i>
    </a>
</div>


<script src="/libs/tocbot/tocbot.min.js"></script>
<script>
    $(function () {
        tocbot.init({
            tocSelector: '#toc-content',
            contentSelector: '#articleContent',
            headingsOffset: -($(window).height() * 0.4 - 45),
            collapseDepth: Number('0'),
            headingSelector: 'h2, h3, h4'
        });

        // modify the toc link href to support Chinese.
        let i = 0;
        let tocHeading = 'toc-heading-';
        $('#toc-content a').each(function () {
            $(this).attr('href', '#' + tocHeading + (++i));
        });

        // modify the heading title id to support Chinese.
        i = 0;
        $('#articleContent').children('h2, h3, h4').each(function () {
            $(this).attr('id', tocHeading + (++i));
        });

        // Set scroll toc fixed.
        let tocHeight = parseInt($(window).height() * 0.4 - 64);
        let $tocWidget = $('.toc-widget');
        $(window).scroll(function () {
            let scroll = $(window).scrollTop();
            /* add post toc fixed. */
            if (scroll > tocHeight) {
                $tocWidget.addClass('toc-fixed');
            } else {
                $tocWidget.removeClass('toc-fixed');
            }
        });

        
        /* 修复文章卡片 div 的宽度. */
        let fixPostCardWidth = function (srcId, targetId) {
            let srcDiv = $('#' + srcId);
            if (srcDiv.length === 0) {
                return;
            }

            let w = srcDiv.width();
            if (w >= 450) {
                w = w + 21;
            } else if (w >= 350 && w < 450) {
                w = w + 18;
            } else if (w >= 300 && w < 350) {
                w = w + 16;
            } else {
                w = w + 14;
            }
            $('#' + targetId).width(w);
        };

        // 切换TOC目录展开收缩的相关操作.
        const expandedClass = 'expanded';
        let $tocAside = $('#toc-aside');
        let $mainContent = $('#main-content');
        $('#floating-toc-btn .btn-floating').click(function () {
            if ($tocAside.hasClass(expandedClass)) {
                $tocAside.removeClass(expandedClass).hide();
                $mainContent.removeClass('l9');
            } else {
                $tocAside.addClass(expandedClass).show();
                $mainContent.addClass('l9');
            }
            fixPostCardWidth('artDetail', 'prenext-posts');
        });
        
    });
</script>

    

</main>


    
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"] ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for(i=0; i < all.length; i += 1) {
            all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    




<script src="https://cdn.jsdelivr.net/npm/mathjax@2.7.8/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script>
    MathJax.Hub.Config({
        tex2jax: {inlineMath: [['$', '$'], ['\(', '\)']]}
    });
</script>



    <footer class="page-footer bg-color">
    
        <link rel="stylesheet" href="/libs/aplayer/APlayer.min.css">
<style>
    .aplayer .aplayer-lrc p {
        
        display: none;
        
        font-size: 12px;
        font-weight: 700;
        line-height: 16px !important;
    }

    .aplayer .aplayer-lrc p.aplayer-lrc-current {
        
        display: none;
        
        font-size: 15px;
        color: #42b983;
    }

    
    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body {
        left: -66px !important;
    }

    .aplayer.aplayer-fixed.aplayer-narrow .aplayer-body:hover {
        left: 0px !important;
    }

    
</style>
<div class="">
    
    <div class="row">
        <meting-js class="col l8 offset-l2 m10 offset-m1 s12"
                   server="netease"
                   type="playlist"
                   id="8590828427"
                   fixed='true'
                   autoplay='false'
                   theme='#42b983'
                   loop='all'
                   order='random'
                   preload='auto'
                   volume='0.7'
                   list-folded='true'
        >
        </meting-js>
    </div>
</div>

<script src="/libs/aplayer/APlayer.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/meting@2/dist/Meting.min.js"></script>

    
    <div class="container row center-align" style="margin-bottom: 0px !important;">
        <div class="col s12 m8 l8 copy-right">
            Copyright&nbsp;&copy;
            
                <span id="year">2023-2025</span>
            
            <span id="year">2023</span>
            <a href="/about" target="_blank">Wabbybabbo</a>
            |&nbsp;Powered by&nbsp;<a href="https://hexo.io/" target="_blank">Hexo</a>
            |&nbsp;Theme&nbsp;<a href="https://github.com/blinkfox/hexo-theme-matery" target="_blank">Matery</a>
            <br>
            
            
            
            
            
            
            <span id="busuanzi_container_site_pv">
                |&nbsp;<i class="far fa-eye"></i>&nbsp;总访问量:&nbsp;<span id="busuanzi_value_site_pv"
                    class="white-color"></span>&nbsp;次
            </span>
            
            
            <span id="busuanzi_container_site_uv">
                |&nbsp;<i class="fas fa-users"></i>&nbsp;总访问人数:&nbsp;<span id="busuanzi_value_site_uv"
                    class="white-color"></span>&nbsp;人
            </span>
            
            <br>
            
            <br>
            
        </div>
        <div class="col s12 m4 l4 social-link social-statis">
    <a href="https://github.com/Wabbybabb0" class="tooltipped" target="_blank" data-tooltip="访问我的GitHub" data-position="top" data-delay="50">
        <i class="fab fa-github"></i>
    </a>



    <a href="mailto:Wabbybabb0@outlook.com" class="tooltipped" target="_blank" data-tooltip="邮件联系我" data-position="top" data-delay="50">
        <i class="fas fa-envelope-open"></i>
    </a>







    <a href="tencent://AddContact/?fromId=50&fromSubId=1&subcmd=all&uin=497056512" class="tooltipped" target="_blank" data-tooltip="QQ联系我: 497056512" data-position="top" data-delay="50">
        <i class="fab fa-qq"></i>
    </a>







    <a href="/atom.xml" class="tooltipped" target="_blank" data-tooltip="RSS 订阅" data-position="top" data-delay="50">
        <i class="fas fa-rss"></i>
    </a>

</div>
    </div>
</footer>

<div class="progress-bar"></div>


    <!-- 搜索遮罩框 -->
<div id="searchModal" class="modal">
    <div class="modal-content">
        <div class="search-header">
            <span class="title"><i class="fas fa-search"></i>&nbsp;&nbsp;搜索</span>
            <input type="search" id="searchInput" name="s" placeholder="请输入搜索的关键字"
                   class="search-input">
        </div>
        <div id="searchResult"></div>
    </div>
</div>

<script type="text/javascript">
$(function () {
    var searchFunc = function (path, search_id, content_id) {
        'use strict';
        $.ajax({
            url: path,
            dataType: "xml",
            success: function (xmlResponse) {
                // get the contents from search data
                var datas = $("entry", xmlResponse).map(function () {
                    return {
                        title: $("title", this).text(),
                        content: $("content", this).text(),
                        url: $("url", this).text()
                    };
                }).get();
                var $input = document.getElementById(search_id);
                var $resultContent = document.getElementById(content_id);
                $input.addEventListener('input', function () {
                    var str = '<ul class=\"search-result-list\">';
                    var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
                    $resultContent.innerHTML = "";
                    if (this.value.trim().length <= 0) {
                        return;
                    }
                    // perform local searching
                    datas.forEach(function (data) {
                        var isMatch = true;
                        var data_title = data.title.trim().toLowerCase();
                        var data_content = data.content.trim().replace(/<[^>]+>/g, "").toLowerCase();
                        var data_url = data.url;
                        data_url = data_url.indexOf('/') === 0 ? data.url : '/' + data_url;
                        var index_title = -1;
                        var index_content = -1;
                        var first_occur = -1;
                        // only match artiles with not empty titles and contents
                        if (data_title !== '' && data_content !== '') {
                            keywords.forEach(function (keyword, i) {
                                index_title = data_title.indexOf(keyword);
                                index_content = data_content.indexOf(keyword);
                                if (index_title < 0 && index_content < 0) {
                                    isMatch = false;
                                } else {
                                    if (index_content < 0) {
                                        index_content = 0;
                                    }
                                    if (i === 0) {
                                        first_occur = index_content;
                                    }
                                }
                            });
                        }
                        // show search results
                        if (isMatch) {
                            str += "<li><a href='" + data_url + "' class='search-result-title'>" + data_title + "</a>";
                            var content = data.content.trim().replace(/<[^>]+>/g, "");
                            if (first_occur >= 0) {
                                // cut out 100 characters
                                var start = first_occur - 20;
                                var end = first_occur + 80;
                                if (start < 0) {
                                    start = 0;
                                }
                                if (start === 0) {
                                    end = 100;
                                }
                                if (end > content.length) {
                                    end = content.length;
                                }
                                var match_content = content.substr(start, end);
                                // highlight all keywords
                                keywords.forEach(function (keyword) {
                                    var regS = new RegExp(keyword, "gi");
                                    match_content = match_content.replace(regS, "<em class=\"search-keyword\">" + keyword + "</em>");
                                });

                                str += "<p class=\"search-result\">" + match_content + "...</p>"
                            }
                            str += "</li>";
                        }
                    });
                    str += "</ul>";
                    $resultContent.innerHTML = str;
                });
            }
        });
    };

    searchFunc('/search.xml', 'searchInput', 'searchResult');
});
</script>

    <!-- 回到顶部按钮 -->
<div id="backTop" class="top-scroll">
    <a class="btn-floating btn-large waves-effect waves-light" href="#!">
        <i class="fas fa-arrow-up"></i>
    </a>
</div>


    <script src="/libs/materialize/materialize.min.js"></script>
    <script src="/libs/masonry/masonry.pkgd.min.js"></script>
    <script src="/libs/aos/aos.js"></script>
    <script src="/libs/scrollprogress/scrollProgress.min.js"></script>
    <script src="/libs/lightGallery/js/lightgallery-all.min.js"></script>
    <script src="/js/matery.js"></script>

    <!-- Baidu Analytics -->

    <!-- Baidu Push -->

<script>
    (function () {
        var bp = document.createElement('script');
        var curProtocol = window.location.protocol.split(':')[0];
        if (curProtocol === 'https') {
            bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
        } else {
            bp.src = 'http://push.zhanzhang.baidu.com/push.js';
        }
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(bp, s);
    })();
</script>

    
    <script src="/libs/others/clicklove.js" async="async"></script>
    
    
    <script async src="/libs/others/busuanzi.pure.mini.js"></script>
    

    

    

	
    

    

    

    
    <script src="/libs/instantpage/instantpage.js" type="module"></script>
    

</body>

</html>
